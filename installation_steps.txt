# WSL Development Environment Setup Steps for Deep-Wallwars Project

## Initial Setup & Environment Check
1. **Verified WSL2 Installation**: 
   - Ubuntu 24.04.2 LTS already installed and running
   - Project directory accessible at `/mnt/c/Users/Nilo/repos/Deep-Wallwars`

2. **Resolved Git Configuration Issues**:
   - Identified line ending differences between Windows and WSL git configs
   - Windows: `core.autocrlf = true`
   - WSL: `core.autocrlf = (not set)`
   - **Fixed**: Set `git config core.autocrlf true` in WSL
   - **Result**: Both environments now show identical git status

## Development Tools Installation
3. **Updated Package Lists**:
   ```bash
   sudo apt update
   ```

4. **Installed Essential Development Tools**:
   ```bash
   sudo apt install -y build-essential cmake python3-pip
   ```
   - Installed: gcc, g++, make, cmake, python3-pip
   - Verified with: `which gcc g++ cmake python3 pip3`

## CUDA Setup for GPU Support
5. **Verified GPU Access**: 
   - `nvidia-smi` works in WSL2 (RTX 4090 visible)
   - Windows driver 572.61 supports WSL2 passthrough
   - CUDA Version 12.8 supported

6. **Installed CUDA Toolkit 12.8**:
   ```bash
   wget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-keyring_1.1-1_all.deb
   sudo dpkg -i cuda-keyring_1.1-1_all.deb
   sudo apt update
   sudo apt install -y cuda-toolkit-12-8
   ```

7. **Configured CUDA Environment**:
   ```bash
   echo 'export PATH=/usr/local/cuda-12.8/bin:$PATH' >> ~/.bashrc
   echo 'export LD_LIBRARY_PATH=/usr/local/cuda-12.8/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc
   source ~/.bashrc
   ```
   - **Verified**: `nvcc --version` shows CUDA 12.8.93

## C++ Dependencies
8. **Installed Google Libraries**:
   ```bash
   sudo apt install -y libgflags-dev libgoogle-glog-dev
   ```

9. **Installed Folly**:
     ```bash
     mkdir -p ~/deepwallwars_folly_deps/folly_src
     cd ~/deepwallwars_folly_deps/folly_src
     git clone https://github.com/facebook/folly.git
     cd folly
     # Install system dependencies for folly (if not already done)
     # sudo ./build/fbcode_builder/getdeps.py install-system-deps --recursive
     # Build and install folly and its dependencies to a persistent location
     FOLLY_INSTALL_PATH=~/deepwallwars_folly_deps/folly_build_scratch
     python3 ./build/fbcode_builder/getdeps.py --scratch-path "$FOLLY_INSTALL_PATH" --allow-system-packages build --no-tests folly
     ```
   - **Installed at**: `~/deepwallwars_folly_deps/folly_build_scratch/installed/folly`
     - (and dependencies like fmt, glog in subdirectories of `~/deepwallwars_folly_deps/folly_build_scratch/installed/`)
   - **Verified**: `libfolly.a` and CMake config files present

10. **Installed TensorRT**:
    ```bash
    wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.0-1_all.deb
    sudo dpkg -i cuda-keyring_1.0-1_all.deb
    sudo apt update
    sudo apt install tensorrt-dev
    ```

## Current Status
- âœ… WSL2 Ubuntu 24.04.2 LTS ready
- âœ… Git configuration synchronized between Windows/WSL
- âœ… Basic development tools installed (gcc, g++, cmake, pip3)
- âœ… CUDA 12.8 toolkit installed and configured
- âœ… gflags and glog installed
- âœ… Folly installed and built successfully
- âœ… TensorRT development libraries installed
- âœ… **PROJECT BUILD SUCCESSFUL!** ðŸŽ‰
- âœ… **ALL UNIT TESTS PASSING!** ðŸŽ‰

## Project Build
11. **Built Deep-Wallwars Project**:
    ```bash
    cd /mnt/c/Users/Nilo/repos/Deep-Wallwars
    mkdir -p build && cd build
    cmake -DCMAKE_PREFIX_PATH="~/deepwallwars_folly_deps/folly_build_scratch/installed/folly;~/deepwallwars_folly_deps/folly_build_scratch/installed/fmt-ChsBN0c2hHnpsw2DxW2xN41t8N72XBRNf6P5tBwGPyA;~/deepwallwars_folly_deps/folly_build_scratch/installed/glog-Kx5Zk7U5w6J8_u5_yY1L4Y6tX8Z9vP1Q_wJ4_kL3_nI" ..
    ```
    **Note**: Fixed compatibility issue in `src/cached_policy.cpp` line 52:
    - Changed: `folly::in_place_t()` â†’ `std::in_place`
    ```bash
    make -j$(nproc)
    ```
    - **Result**: Successfully built `deep_ww` executable

12. **Installed and Ran Unit Tests**:
    ```bash
    sudo apt install catch2
    cmake -DCMAKE_PREFIX_PATH="..." ..  # Same as above
    make -j$(nproc)
    ./unit_tests
    ```
    - **Result**: All tests passed (63 assertions in 16 test cases) âœ…

## Python Training Environment Setup
13. **Verified Python and Pip Versions**:
    - In WSL, without an active virtual environment:
      ```bash
      python3 --version && pip3 --version
      ```
    - Result: Python 3.12.3, pip 24.0

14. **Installed `python3-venv` Package**:
    - Needed for creating Python virtual environments.
    - Attempt to create venv failed initially due to missing `ensurepip`.
      ```bash
      sudo apt install python3-venv
      ```

15. **Created Python Virtual Environment**:
    - Navigated to project root: `cd /mnt/c/Users/Nilo/repos/Deep-Wallwars`
    - Created venv:
      ```bash
      python3 -m venv .venv
      ```

16. **Activated Virtual Environment**:
    ```bash
    source .venv/bin/activate
    ```
    - Prompt changed to indicate `(.venv)`

17. **Upgraded Pip in Virtual Environment**:
    ```bash
    pip install --upgrade pip
    ```
    - Result: Successfully installed pip-25.1.1

18. **Installed PyTorch with CUDA 12.8 Support**:
    - Command (in activated .venv):
      ```bash
      pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128
      ```
    - Verification (in activated .venv):
      ```python
      python3 -c "import torch; print(f'PyTorch version: {torch.__version__}'); print(f'CUDA available: {torch.cuda.is_available()}'); print(f'CUDA version: {torch.version.cuda}'); print(f'cuDNN version: {torch.backends.cudnn.version()}'); print(f'Device count: {torch.cuda.device_count()}'); print(f'Current device: {torch.cuda.current_device()}'); print(f'Device name: {torch.cuda.get_device_name(torch.cuda.current_device())}')"
      ```
    - Result:
      - PyTorch version: 2.7.0+cu128
      - CUDA available: True
      - CUDA version: 12.8
      - cuDNN version: 90701 (9.7.0.1)
      - Device count: 1
      - Device name: NVIDIA GeForce RTX 4090

19. **Installed Fastai**:
    - Command (in activated .venv):
      ```bash
      pip install fastai
      ```
    - Verification (in activated .venv):
      ```python
      python3 -c "import fastai; import torch; print(f'Fastai version: {fastai.__version__}'); print(f'Fastai using CUDA: {torch.cuda.is_available()}')"
      ```
    - Result:
      - Fastai version: 2.8.1
      - Fastai using CUDA: True

20. **Installed `onnx` Python Package**:
    - **Reason**: Required by `torch.onnx.export()` for ONNX model saving. Missed in initial dependency setup.
    - Command (in activated .venv):
      ```bash
      pip install onnx
      ```

21. **Installed Native TensorRT SDK (for trtexec)**:
    - **Reason**: `tensorrt-dev` apt package did not include `trtexec`, which is called by the training script.
    - **Downloaded**: `TensorRT-10.11.0.33.Linux.x86_64-gnu.cuda-12.9.tar.gz` from NVIDIA Developer site.
    - **Installation Steps**:
      ```bash
      mkdir -p ~/downloads ~/opt
      mv /mnt/c/Users/Nilo/Downloads/TensorRT-10.11.0.33.Linux.x86_64-gnu.cuda-12.9.tar.gz ~/downloads/
      cd ~/downloads
      tar -xzvf TensorRT-10.11.0.33.Linux.x86_64-gnu.cuda-12.9.tar.gz -C ~/opt/
      # Add to .bashrc and source it
      echo '' >> ~/.bashrc
      echo '# Add TensorRT SDK to PATH and LD_LIBRARY_PATH' >> ~/.bashrc
      echo 'export TENSORRT_HOME=~/opt/TensorRT-10.11.0.33' >> ~/.bashrc
      echo 'export PATH="$TENSORRT_HOME/bin:$PATH"' >> ~/.bashrc
      echo 'export LD_LIBRARY_PATH="$TENSORRT_HOME/lib:$LD_LIBRARY_PATH"' >> ~/.bashrc
      source ~/.bashrc
      ```
    - **Verification**:
      ```bash
      # Ensure .venv is active BEFORE checking, or open a new terminal and activate .venv
      # then source ~/.bashrc if PATH issues persist.
      echo $PATH # Should include $TENSORRT_HOME/bin
      which trtexec # Should be ~/opt/TensorRT-10.11.0.33/bin/trtexec
      trtexec       # Should show version 10.11.0.0 build 33
      ```

22. **Created Directories for Training Script Outputs**:
    - Navigated to `scripts/` directory.
    - Ran:
      ```bash
      mkdir -p ../models
      mkdir -p ../data
      ```

## Environment Summary
**C++ Self-Play Environment: COMPLETE** ðŸŽ‰
- CUDA 12.8 + TensorRT 10.x for GPU inference
- Folly for coroutines, logging, and thread pools  
- Multi-threaded MCTS implementation
- Interactive play and training data generation
- Model ranking system

## Next Steps
- âœ… **C++ Environment Complete!**
- Install Python dependencies for training scripts (PyTorch, fastai)
- Set up training pipeline
- Generate initial models for self-play

Instructions:

To enter the Ubuntu WSL2 environment, run:
wsl -d Ubuntu

To activate the Python virtual environment, run:
source .venv/bin/activate

Make with this command:
cmake -DCMAKE_PREFIX_PATH="~/deepwallwars_folly_deps/folly_build_scratch/installed/folly;~/deepwallwars_folly_deps/folly_build_scratch/installed/fmt-ChsBN0c2hHnpsw2DxW2xN41t8N72XBRNf6P5tBwGPyA;~/deepwallwars_folly_deps/folly_build_scratch/installed/glog-Kx5Zk7U5w6J8_u5_yY1L4Y6tX8Z9vP1Q_wJ4_kL3_nI" ..


PyTorch version: 2.7.0+cu128
CUDA available: True
CUDA version: 12.8
cuDNN version: 90701
Device count: 1
Current device: 0
Device name: NVIDIA GeForce RTX 4090
Fastai version: 2.8.1
Fastai using CUDA: True
