{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f7de520-7e2d-46a8-9801-c8447f673484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import torch.onnx\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from model import ResNet\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "columns = 5\n",
    "rows = 5\n",
    "channels = 64\n",
    "layers = 15\n",
    "epochs = 100\n",
    "training_batch_size = 64\n",
    "inference_batch_size = 256\n",
    "kl_loss_scale = 0.1\n",
    "\n",
    "data_folder = \"../data\"\n",
    "models_folder = \"../models\"\n",
    "generation = 1\n",
    "\n",
    "class Snapshots(torch.utils.data.Dataset):\n",
    "    def __init__(self, file_name):\n",
    "        self.data = [[], [], [], []]\n",
    "        i = 0\n",
    "        with open(file_name) as f:\n",
    "            for line in f.readlines():\n",
    "                if line.strip() == \"\":\n",
    "                    i = 0\n",
    "                    continue\n",
    "\n",
    "                t = torch.tensor([float(x) for x in line.split(\", \")])\n",
    "\n",
    "                if i == 0:\n",
    "                    t = t.view(7, columns, rows)\n",
    "                self.data[i].append(t)\n",
    "                i += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data[0])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return [self.data[x][index] for x in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17deff99-8d43-42c5-998d-3a2a018cc1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(wp_out, sp_out, vs_out, wp_label, sp_label, vs_label):\n",
    "    kl_div = nn.KLDivLoss(reduction='sum')\n",
    "    mse = nn.MSELoss(reduction='sum')\n",
    "    \n",
    "    # Concatenate the outputs to form the complete action distribution\n",
    "    actions_out = torch.cat([wp_out, sp_out], dim=1)\n",
    "    # Apply log_softmax to convert to log probabilities\n",
    "    log_probs = F.log_softmax(actions_out, dim=1)\n",
    "    \n",
    "    # Concatenate the labels to form the complete target distribution\n",
    "    actions_label = torch.cat([wp_label, sp_label], dim=1)\n",
    "    \n",
    "    # Compute the KL divergence loss\n",
    "    kl_loss = kl_loss_scale * kl_div(log_probs, actions_label)\n",
    "    \n",
    "    # Compute the MSE loss for the scalar output\n",
    "    mse_loss = mse(vs_out, vs_label)\n",
    "\n",
    "    return (kl_loss, mse_loss)\n",
    "\n",
    "\n",
    "def save_model(model, folder):\n",
    "    torch.save(model, f\"{folder}/model_{generation}.pt\")\n",
    "    input_names = [\"States\"]\n",
    "    output_names = [\"WallPriors\", \"StepPriors\", \"Values\"]\n",
    "    dummy_input = torch.randn(inference_batch_size, 7, columns, rows).to(device)\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy_input,\n",
    "        f\"{folder}/model_{generation}.onnx\",\n",
    "        input_names=input_names,\n",
    "        output_names=output_names,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5aba22da-d7c8-4214-9b14-102331726bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet(columns, rows, channels, layers).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cae0251-928b-44eb-bfb2-4a400bba1adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_window = range((generation - 1) // 2, generation)\n",
    "snapshots = torch.utils.data.ConcatDataset(\n",
    "    [Snapshots(f\"{data_folder}/snapshots_{i}.csv\") for i in training_window]\n",
    ")\n",
    "training_data, eval_data = torch.utils.data.random_split(snapshots, [0.8, 0.2])\n",
    "training_loader = torch.utils.data.DataLoader(\n",
    "    training_data,\n",
    "    batch_size=training_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")\n",
    "eval_loader = torch.utils.data.DataLoader(\n",
    "    eval_data,\n",
    "    batch_size=training_batch_size,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    shuffle=False,\n",
    ")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5c56510-de5b-4897-bda8-d95fd99ad212",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_tensor(mat, cmap='viridis'):\n",
    "    \"\"\"\n",
    "    Plots a 2D PyTorch tensor as an n x n grid of squares with colors representing the tensor values.\n",
    "\n",
    "    Parameters:\n",
    "    - mat: 2D PyTorch tensor of floats.\n",
    "    - cmap: Colormap for visualizing the values in the tensor.\n",
    "    \"\"\"\n",
    "    # Ensure mat is a 2D tensor\n",
    "    if mat.dim() != 2:\n",
    "        raise ValueError(\"Input tensor must be 2D\")\n",
    "\n",
    "    # Convert the tensor to a NumPy array\n",
    "    matrix = mat.numpy()\n",
    "\n",
    "    # Plotting the matrix\n",
    "    plt.figure(figsize=(6,6))  # Adjust the figure size as needed\n",
    "    plt.imshow(matrix, cmap=cmap, interpolation='nearest')  # Use specified colormap\n",
    "    plt.colorbar()  # Show color scale\n",
    "    plt.xticks(range(matrix.shape[0]))  # Adjust ticks based on tensor size\n",
    "    plt.yticks(range(matrix.shape[1]))\n",
    "    plt.grid(False)  # Turn off the grid\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "812a3b40-e684-4e6c-a626-7aa278ac4bc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_tensor(snapshots[3][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9072c98-8c6b-48dc-b01c-e14007e34744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3246"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(snapshots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45e2a186-0104-4495-b63d-150a2074fca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss in epoch 0 of generation 1: 0.12602440669162943 + 1.742773794145551 = 1.8687982008371804.\n",
      "Evaluation loss in epoch 0 of generation 1: 0.11996891336925959 + 1.9207412178968244 = 2.040710131266084.\n",
      "Training loss in epoch 1 of generation 1: 0.12496459158191232 + 1.752132966603781 = 1.8770975581856935.\n",
      "Evaluation loss in epoch 1 of generation 1: 0.11996891336925959 + 1.9207412178968244 = 2.040710131266084.\n",
      "Training loss in epoch 2 of generation 1: 0.12496459240816077 + 1.7521329724793255 = 1.8770975648874864.\n",
      "Evaluation loss in epoch 2 of generation 1: 0.11996891336925959 + 1.9207412178968244 = 2.040710131266084.\n",
      "Training loss in epoch 3 of generation 1: 0.12496459286718768 + 1.7521329695415533 = 1.8770975624087411.\n",
      "Evaluation loss in epoch 3 of generation 1: 0.11996891336925959 + 1.9207412178968244 = 2.040710131266084.\n",
      "Training loss in epoch 4 of generation 1: 0.12496459093927464 + 1.7521329724793255 = 1.8770975634186002.\n",
      "Evaluation loss in epoch 4 of generation 1: 0.11996891336925959 + 1.9207412178968244 = 2.040710131266084.\n",
      "Training loss in epoch 5 of generation 1: 0.12496459048024773 + 1.752132966603781 = 1.877097557084029.\n",
      "Evaluation loss in epoch 5 of generation 1: 0.11996891336925959 + 1.9207412178968244 = 2.040710131266084.\n",
      "Training loss in epoch 6 of generation 1: 0.12496458901136161 + 1.75213297835487 = 1.8770975673662318.\n",
      "Evaluation loss in epoch 6 of generation 1: 0.11996891336925959 + 1.9207412178968244 = 2.040710131266084.\n",
      "Training loss in epoch 7 of generation 1: 0.12496459103108003 + 1.752132966603781 = 1.8770975576348612.\n",
      "Evaluation loss in epoch 7 of generation 1: 0.11996891336925959 + 1.9207412178968244 = 2.040710131266084.\n",
      "Training loss in epoch 8 of generation 1: 0.12496459213274462 + 1.752132966603781 = 1.8770975587365257.\n",
      "Evaluation loss in epoch 8 of generation 1: 0.11996891336925959 + 1.9207412178968244 = 2.040710131266084.\n",
      "Training loss in epoch 9 of generation 1: 0.1249645895621939 + 1.7521329724793255 = 1.8770975620415196.\n",
      "Evaluation loss in epoch 9 of generation 1: 0.11996891336925959 + 1.9207412178968244 = 2.040710131266084.\n",
      "Training loss in epoch 10 of generation 1: 0.1249645906638585 + 1.7521329695415533 = 1.8770975602054119.\n",
      "Evaluation loss in epoch 10 of generation 1: 0.11996891336925959 + 1.9207412178968244 = 2.040710131266084.\n",
      "Training loss in epoch 11 of generation 1: 0.1249645916737177 + 1.7521329636660088 = 1.8770975553397267.\n",
      "Evaluation loss in epoch 11 of generation 1: 0.11996891336925959 + 1.9207412178968244 = 2.040710131266084.\n",
      "Training loss in epoch 12 of generation 1: 0.12496459231635539 + 1.752132966603781 = 1.8770975589201364.\n",
      "Evaluation loss in epoch 12 of generation 1: 0.11996891336925959 + 1.9207412178968244 = 2.040710131266084.\n",
      "Training loss in epoch 13 of generation 1: 0.12496459048024773 + 1.752132966603781 = 1.877097557084029.\n",
      "Evaluation loss in epoch 13 of generation 1: 0.11996891336925959 + 1.9207412178968244 = 2.040710131266084.\n",
      "Training loss in epoch 14 of generation 1: 0.1249645901130262 + 1.7521329695415533 = 1.8770975596545796.\n",
      "Evaluation loss in epoch 14 of generation 1: 0.11996891336925959 + 1.9207412178968244 = 2.040710131266084.\n",
      "Training loss in epoch 15 of generation 1: 0.12496459286718768 + 1.75213297835487 = 1.877097571222058.\n",
      "Evaluation loss in epoch 15 of generation 1: 0.11996891336925959 + 1.9207412178968244 = 2.040710131266084.\n",
      "Training loss in epoch 16 of generation 1: 0.12496459149010694 + 1.7521329842304145 = 1.8770975757205215.\n",
      "Evaluation loss in epoch 16 of generation 1: 0.11996891336925959 + 1.9207412178968244 = 2.040710131266084.\n",
      "Training loss in epoch 17 of generation 1: 0.12496459121469079 + 1.752132966603781 = 1.8770975578184719.\n",
      "Evaluation loss in epoch 17 of generation 1: 0.11996891336925959 + 1.9207412178968244 = 2.040710131266084.\n",
      "Training loss in epoch 18 of generation 1: 0.1249645901130262 + 1.75213297835487 = 1.8770975684678963.\n",
      "Evaluation loss in epoch 18 of generation 1: 0.11996891336925959 + 1.9207412178968244 = 2.040710131266084.\n",
      "Training loss in epoch 19 of generation 1: 0.12496458974580467 + 1.7521329724793255 = 1.8770975622251302.\n",
      "Evaluation loss in epoch 19 of generation 1: 0.11996891336925959 + 1.9207412178968244 = 2.040710131266084.\n",
      "Training loss in epoch 20 of generation 1: 0.12496459213274462 + 1.7521329607282365 = 1.8770975528609812.\n",
      "Evaluation loss in epoch 20 of generation 1: 0.11996891336925959 + 1.9207412178968244 = 2.040710131266084.\n",
      "Training loss in epoch 21 of generation 1: 0.12496459048024773 + 1.752132966603781 = 1.877097557084029.\n",
      "Evaluation loss in epoch 21 of generation 1: 0.11996891336925959 + 1.9207412178968244 = 2.040710131266084.\n",
      "Training loss in epoch 22 of generation 1: 0.12496459103108003 + 1.7521329636660088 = 1.877097554697089.\n",
      "Evaluation loss in epoch 22 of generation 1: 0.11996891336925959 + 1.9207412178968244 = 2.040710131266084.\n",
      "Training loss in epoch 23 of generation 1: 0.12496459121469079 + 1.7521329754170978 = 1.8770975666317886.\n",
      "Evaluation loss in epoch 23 of generation 1: 0.11996891336925959 + 1.9207412178968244 = 2.040710131266084.\n",
      "Training loss in epoch 24 of generation 1: 0.1249645895621939 + 1.7521329798237562 = 1.8770975693859502.\n",
      "Evaluation loss in epoch 24 of generation 1: 0.11996891336925959 + 1.9207412178968244 = 2.040710131266084.\n",
      "Training loss in epoch 25 of generation 1: 0.12496459231635539 + 1.752132976885984 = 1.8770975692023393.\n",
      "Evaluation loss in epoch 25 of generation 1: 0.11996891336925959 + 1.9207412178968244 = 2.040710131266084.\n",
      "Training loss in epoch 26 of generation 1: 0.12496459204093924 + 1.7521329724793255 = 1.8770975645202648.\n",
      "Evaluation loss in epoch 26 of generation 1: 0.11996891336925959 + 1.9207412178968244 = 2.040710131266084.\n",
      "Training loss in epoch 27 of generation 1: 0.12496459176552309 + 1.7521329724793255 = 1.8770975642448486.\n",
      "Evaluation loss in epoch 27 of generation 1: 0.11996891336925959 + 1.9207412178968244 = 2.040710131266084.\n",
      "Training loss in epoch 28 of generation 1: 0.12496459158191232 + 1.7521329489771476 = 1.87709754055906.\n",
      "Evaluation loss in epoch 28 of generation 1: 0.11996891336925959 + 1.9207412178968244 = 2.040710131266084.\n",
      "Training loss in epoch 29 of generation 1: 0.12496459029663697 + 1.7521329636660088 = 1.8770975539626458.\n",
      "Evaluation loss in epoch 29 of generation 1: 0.11996891336925959 + 1.9207412178968244 = 2.040710131266084.\n",
      "Training loss in epoch 30 of generation 1: 0.12496459139830156 + 1.7521329695415533 = 1.877097560939855.\n",
      "Evaluation loss in epoch 30 of generation 1: 0.11996891336925959 + 1.9207412178968244 = 2.040710131266084.\n",
      "Training loss in epoch 31 of generation 1: 0.12496459213274462 + 1.7521329754170978 = 1.8770975675498425.\n",
      "Evaluation loss in epoch 31 of generation 1: 0.11996891557343752 + 1.9207412178968244 = 2.0407101334702618.\n",
      "Training loss in epoch 32 of generation 1: 0.12496459470329534 + 1.752132966603781 = 1.8770975613070764.\n",
      "Evaluation loss in epoch 32 of generation 1: 0.11996891336925959 + 1.9207412178968244 = 2.040710131266084.\n",
      "Training loss in epoch 33 of generation 1: 0.12496459360163074 + 1.7521329636660088 = 1.8770975572676396.\n",
      "Evaluation loss in epoch 33 of generation 1: 0.11996891336925959 + 1.9207412178968244 = 2.040710131266084.\n",
      "Training loss in epoch 34 of generation 1: 0.12496459103108003 + 1.7521329798237562 = 1.8770975708548363.\n",
      "Evaluation loss in epoch 34 of generation 1: 0.11996891336925959 + 1.9207412178968244 = 2.040710131266084.\n",
      "Training loss in epoch 35 of generation 1: 0.12496459158191232 + 1.7521329812926423 = 1.8770975728745547.\n",
      "Evaluation loss in epoch 35 of generation 1: 0.11996891336925959 + 1.9207412178968244 = 2.040710131266084.\n",
      "Training loss in epoch 36 of generation 1: 0.12496459029663697 + 1.7521329945126174 = 1.8770975848092544.\n",
      "Evaluation loss in epoch 36 of generation 1: 0.11996891336925959 + 1.9207412178968244 = 2.040710131266084.\n",
      "Training loss in epoch 37 of generation 1: 0.12496459029663697 + 1.7521329812926423 = 1.8770975715892793.\n",
      "Evaluation loss in epoch 37 of generation 1: 0.11996891336925959 + 1.9207412178968244 = 2.040710131266084.\n",
      "Training loss in epoch 38 of generation 1: 0.12496459048024773 + 1.7521329401638308 = 1.8770975306440787.\n",
      "Evaluation loss in epoch 38 of generation 1: 0.11996891483871154 + 1.9207412178968244 = 2.040710132735536.\n",
      "Training loss in epoch 39 of generation 1: 0.12496459396885227 + 1.752132990105959 = 1.8770975840748114.\n",
      "Evaluation loss in epoch 39 of generation 1: 0.11996891630816349 + 1.9207412178968244 = 2.040710134204988.\n",
      "Training loss in epoch 40 of generation 1: 0.12496459507051688 + 1.7521329724793255 = 1.8770975675498425.\n",
      "Evaluation loss in epoch 40 of generation 1: 0.11996891557343752 + 1.9207412178968244 = 2.0407101334702618.\n",
      "Training loss in epoch 41 of generation 1: 0.12496459286718768 + 1.75213297835487 = 1.877097571222058.\n",
      "Evaluation loss in epoch 41 of generation 1: 0.11996891557343752 + 1.9207412178968244 = 2.0407101334702618.\n",
      "Training loss in epoch 42 of generation 1: 0.12496460149689367 + 2.024407534770062 = 2.149372136266956.\n",
      "Evaluation loss in epoch 42 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 43 of generation 1: 0.12496459194913385 + 2.20327896570581 = 2.3282435576549436.\n",
      "Evaluation loss in epoch 43 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 44 of generation 1: 0.12496459038844235 + 2.2032789686435823 = 2.3282435590320243.\n",
      "Evaluation loss in epoch 44 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 45 of generation 1: 0.1249645895621939 + 2.2032789598302656 = 2.328243549392459.\n",
      "Evaluation loss in epoch 45 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 46 of generation 1: 0.12496459158191232 + 2.203278951016949 = 2.328243542598861.\n",
      "Evaluation loss in epoch 46 of generation 1: 0.11996891557343752 + 2.027161302845724 = 2.1471302184191616.\n",
      "Training loss in epoch 47 of generation 1: 0.12496459470329534 + 2.203278977456899 = 2.3282435721601944.\n",
      "Evaluation loss in epoch 47 of generation 1: 0.11996891557343752 + 2.027161302845724 = 2.1471302184191616.\n",
      "Training loss in epoch 48 of generation 1: 0.12496459213274462 + 2.203278953954721 = 2.3282435460874655.\n",
      "Evaluation loss in epoch 48 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 49 of generation 1: 0.12496459378524151 + 2.203278962768038 = 2.328243556553279.\n",
      "Evaluation loss in epoch 49 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 50 of generation 1: 0.12496459048024773 + 2.203278974519127 = 2.3282435649993745.\n",
      "Evaluation loss in epoch 50 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 51 of generation 1: 0.12496459048024773 + 2.2032789686435823 = 2.32824355912383.\n",
      "Evaluation loss in epoch 51 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 52 of generation 1: 0.12496459139830156 + 2.203278977456899 = 2.3282435688552003.\n",
      "Evaluation loss in epoch 52 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 53 of generation 1: 0.1249645901130262 + 2.203278974519127 = 2.3282435646321527.\n",
      "Evaluation loss in epoch 53 of generation 1: 0.11996891483871154 + 2.027161302845724 = 2.1471302176844356.\n",
      "Training loss in epoch 54 of generation 1: 0.12496459562134918 + 2.2032789803946713 = 2.3282435760160203.\n",
      "Evaluation loss in epoch 54 of generation 1: 0.11996889757265111 + 2.027161302845724 = 2.1471302004183754.\n",
      "Training loss in epoch 55 of generation 1: 0.12496459286718768 + 2.2032789715813546 = 2.328243564448542.\n",
      "Evaluation loss in epoch 55 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 56 of generation 1: 0.1249645901130262 + 2.203279000959077 = 2.328243591072103.\n",
      "Evaluation loss in epoch 56 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 57 of generation 1: 0.12496459305079845 + 2.2032789833324435 = 2.3282435763832416.\n",
      "Evaluation loss in epoch 57 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 58 of generation 1: 0.12496459231635539 + 2.203278962768038 = 2.328243555084393.\n",
      "Evaluation loss in epoch 58 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 59 of generation 1: 0.12496458974580467 + 2.203278962768038 = 2.3282435525138423.\n",
      "Evaluation loss in epoch 59 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 60 of generation 1: 0.12496458974580467 + 2.203278974519127 = 2.3282435642649313.\n",
      "Evaluation loss in epoch 60 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 61 of generation 1: 0.12496459048024773 + 2.203278977456899 = 2.3282435679371467.\n",
      "Evaluation loss in epoch 61 of generation 1: 0.11996891483871154 + 2.027161302845724 = 2.1471302176844356.\n",
      "Training loss in epoch 62 of generation 1: 0.12496459451968457 + 2.2032789715813546 = 2.328243566101039.\n",
      "Evaluation loss in epoch 62 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 63 of generation 1: 0.12496459139830156 + 2.2032789598302656 = 2.328243551228567.\n",
      "Evaluation loss in epoch 63 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 64 of generation 1: 0.12496459103108003 + 2.2032789480791766 = 2.3282435391102565.\n",
      "Evaluation loss in epoch 64 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 65 of generation 1: 0.1249645901130262 + 2.203278962768038 = 2.3282435528810637.\n",
      "Evaluation loss in epoch 65 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 66 of generation 1: 0.12496459213274462 + 2.20327896570581 = 2.3282435578385545.\n",
      "Evaluation loss in epoch 66 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 67 of generation 1: 0.12496459231635539 + 2.2032789950835325 = 2.328243587399888.\n",
      "Evaluation loss in epoch 67 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 68 of generation 1: 0.12496459103108003 + 2.203278974519127 = 2.3282435655502067.\n",
      "Evaluation loss in epoch 68 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 69 of generation 1: 0.12496459075566388 + 2.2032789715813546 = 2.3282435623370183.\n",
      "Evaluation loss in epoch 69 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 70 of generation 1: 0.12496459139830156 + 2.203278962768038 = 2.328243554166339.\n",
      "Evaluation loss in epoch 70 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 71 of generation 1: 0.12496459084746926 + 2.2032789833324435 = 2.3282435741799126.\n",
      "Evaluation loss in epoch 71 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 72 of generation 1: 0.12496458974580467 + 2.2032789803946713 = 2.328243570140476.\n",
      "Evaluation loss in epoch 72 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 73 of generation 1: 0.12496458919497237 + 2.203278974519127 = 2.328243563714099.\n",
      "Evaluation loss in epoch 73 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 74 of generation 1: 0.12496458974580467 + 2.203278962768038 = 2.3282435525138423.\n",
      "Evaluation loss in epoch 74 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 75 of generation 1: 0.12496459268357692 + 2.2032789833324435 = 2.3282435760160203.\n",
      "Evaluation loss in epoch 75 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 76 of generation 1: 0.1249645895621939 + 2.203278974519127 = 2.3282435640813204.\n",
      "Evaluation loss in epoch 76 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 77 of generation 1: 0.12496459213274462 + 2.203278986270216 = 2.3282435784029603.\n",
      "Evaluation loss in epoch 77 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 78 of generation 1: 0.12496459029663697 + 2.2032789803946713 = 2.328243570691308.\n",
      "Evaluation loss in epoch 78 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 79 of generation 1: 0.1249645906638585 + 2.2032789715813546 = 2.3282435622452127.\n",
      "Evaluation loss in epoch 79 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 80 of generation 1: 0.12496459121469079 + 2.203278981863557 = 2.328243573078248.\n",
      "Evaluation loss in epoch 80 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 81 of generation 1: 0.12496459204093924 + 2.2032789524858347 = 2.328243544526774.\n",
      "Evaluation loss in epoch 81 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 82 of generation 1: 0.12496459213274462 + 2.2032789921457603 = 2.3282435842785048.\n",
      "Evaluation loss in epoch 82 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 83 of generation 1: 0.12496459268357692 + 2.203278989207988 = 2.3282435818915648.\n",
      "Evaluation loss in epoch 83 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 84 of generation 1: 0.12496458937858314 + 2.203278951016949 = 2.328243540395532.\n",
      "Evaluation loss in epoch 84 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 85 of generation 1: 0.12496459020483158 + 2.203278998021305 = 2.3282435882261363.\n",
      "Evaluation loss in epoch 85 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 86 of generation 1: 0.12496459176552309 + 2.2032789803946713 = 2.3282435721601944.\n",
      "Evaluation loss in epoch 86 of generation 1: 0.11996891557343752 + 2.027161302845724 = 2.1471302184191616.\n",
      "Training loss in epoch 87 of generation 1: 0.12496459598857071 + 2.203278998021305 = 2.328243594009875.\n",
      "Evaluation loss in epoch 87 of generation 1: 0.11996890271573295 + 2.027161302845724 = 2.147130205561457.\n",
      "Training loss in epoch 88 of generation 1: 0.12496459470329534 + 2.203278986270216 = 2.328243580973511.\n",
      "Evaluation loss in epoch 88 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 89 of generation 1: 0.12496459213274462 + 2.2032789686435823 = 2.3282435607763268.\n",
      "Evaluation loss in epoch 89 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 90 of generation 1: 0.12496458983761005 + 2.2032789568924933 = 2.328243546730103.\n",
      "Evaluation loss in epoch 90 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 91 of generation 1: 0.12496459139830156 + 2.203278953954721 = 2.3282435453530224.\n",
      "Evaluation loss in epoch 91 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 92 of generation 1: 0.12496459130649618 + 2.203278993614646 = 2.3282435849211423.\n",
      "Evaluation loss in epoch 92 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 93 of generation 1: 0.1249645916737177 + 2.203278989207988 = 2.3282435808817055.\n",
      "Evaluation loss in epoch 93 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 94 of generation 1: 0.12496458827691855 + 2.203278962768038 = 2.3282435510449564.\n",
      "Evaluation loss in epoch 94 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 95 of generation 1: 0.12496459158191232 + 2.2032789568924933 = 2.3282435484744055.\n",
      "Evaluation loss in epoch 95 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 96 of generation 1: 0.12496459396885227 + 2.2032789686435823 = 2.3282435626124345.\n",
      "Evaluation loss in epoch 96 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 97 of generation 1: 0.1249645895621939 + 2.203278986270216 = 2.3282435758324094.\n",
      "Evaluation loss in epoch 97 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 98 of generation 1: 0.12496458992941543 + 2.203278977456899 = 2.3282435673863144.\n",
      "Evaluation loss in epoch 98 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 99 of generation 1: 0.12496459249996615 + 2.2032789715813546 = 2.3282435640813204.\n",
      "Evaluation loss in epoch 99 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 100 of generation 1: 0.12496459121469079 + 2.203278977456899 = 2.32824356867159.\n",
      "Evaluation loss in epoch 100 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 101 of generation 1: 0.1249645901130262 + 2.20327896570581 = 2.328243555818836.\n",
      "Evaluation loss in epoch 101 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 102 of generation 1: 0.12496458992941543 + 2.203278958361379 = 2.3282435482907946.\n",
      "Evaluation loss in epoch 102 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 103 of generation 1: 0.12496459213274462 + 2.2032789686435823 = 2.3282435607763268.\n",
      "Evaluation loss in epoch 103 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 104 of generation 1: 0.12496459213274462 + 2.20327896570581 = 2.3282435578385545.\n",
      "Evaluation loss in epoch 104 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 105 of generation 1: 0.12496459084746926 + 2.203278974519127 = 2.328243565366596.\n",
      "Evaluation loss in epoch 105 of generation 1: 0.11996891336925959 + 2.027161302845724 = 2.147130216214984.\n",
      "Training loss in epoch 106 of generation 1: 0.12496459424426842 + 2.203278854070464 = 2.328243448314733.\n",
      "Evaluation loss in epoch 106 of generation 1: 0.11996891336925959 + 2.0271609442994483 = 2.1471298576687077.\n",
      "Training loss in epoch 107 of generation 1: 0.12496459139830156 + 2.203277834663494 = 2.3282424260617955.\n",
      "Evaluation loss in epoch 107 of generation 1: 0.11996891336925959 + 2.0271576409714616 = 2.147126554340721.\n",
      "Training loss in epoch 108 of generation 1: 0.12496459194913385 + 1.5582858699993947 = 1.6832504619485285.\n",
      "Evaluation loss in epoch 108 of generation 1: 0.11996891557343752 + 1.0283423895461166 = 1.1483113051195541.\n",
      "Training loss in epoch 109 of generation 1: 0.1249645906638585 + 1.0138233274416506 = 1.1387879181055092.\n",
      "Evaluation loss in epoch 109 of generation 1: 0.11996891336925959 + 0.9807411893306786 = 1.1007101026999382.\n",
      "Training loss in epoch 110 of generation 1: 0.12496459121469079 + 0.9755579669704151 = 1.100522558185106.\n",
      "Evaluation loss in epoch 110 of generation 1: 0.11996891336925959 + 0.9856899096895991 = 1.1056588230588587.\n",
      "Training loss in epoch 111 of generation 1: 0.12496459121469079 + 0.9867463433196281 = 1.111710934534319.\n",
      "Evaluation loss in epoch 111 of generation 1: 0.11996891336925959 + 0.9955078912626246 = 1.1154768046318841.\n",
      "Training loss in epoch 112 of generation 1: 0.12496459139830156 + 0.9717534916833312 = 1.0967180830816328.\n",
      "Evaluation loss in epoch 112 of generation 1: 0.11996891336925959 + 0.9908369561005814 = 1.110805869469841.\n",
      "Training loss in epoch 113 of generation 1: 0.1249645895621939 + 0.9828373714736393 = 1.1078019610358332.\n",
      "Evaluation loss in epoch 113 of generation 1: 0.11996891336925959 + 0.9732588663674283 = 1.0932277797366878.\n",
      "Training loss in epoch 114 of generation 1: 0.12496459048024773 + 0.9731758097111376 = 1.0981404001913853.\n",
      "Evaluation loss in epoch 114 of generation 1: 0.11996891336925959 + 1.031037285074431 = 1.1510061984436906.\n",
      "Training loss in epoch 115 of generation 1: 0.12496459158191232 + 0.9739982375834233 = 1.0989628291653357.\n",
      "Evaluation loss in epoch 115 of generation 1: 0.11996891336925959 + 0.984898536341216 = 1.1048674497104756.\n",
      "Training loss in epoch 116 of generation 1: 0.12496458974580467 + 0.9707170228660681 = 1.0956816126118727.\n",
      "Evaluation loss in epoch 116 of generation 1: 0.11996891336925959 + 1.0010921701628182 = 1.1210610835320778.\n",
      "Training loss in epoch 117 of generation 1: 0.12496459075566388 + 0.9789635956080822 = 1.103928186363746.\n",
      "Evaluation loss in epoch 117 of generation 1: 0.11996891483871154 + 0.9881463572497727 = 1.1081152720884844.\n",
      "Training loss in epoch 118 of generation 1: 0.12496459286718768 + 0.9744460319251705 = 1.0994106247923583.\n",
      "Evaluation loss in epoch 118 of generation 1: 0.11996891336925959 + 0.9798413645504803 = 1.0998102779197398.\n",
      "Training loss in epoch 119 of generation 1: 0.12496459176552309 + 0.9768877774878654 = 1.1018523692533884.\n",
      "Evaluation loss in epoch 119 of generation 1: 0.11996891336925959 + 0.9734238285129353 = 1.093392741882195.\n",
      "Training loss in epoch 120 of generation 1: 0.12496458919497237 + 0.9740750221367015 = 1.0990396113316738.\n",
      "Evaluation loss in epoch 120 of generation 1: 0.11996891336925959 + 1.0041842967593246 = 1.1241532101285843.\n",
      "Training loss in epoch 121 of generation 1: 0.12496459176552309 + 0.9812055036201814 = 1.1061700953857045.\n",
      "Evaluation loss in epoch 121 of generation 1: 0.11996891336925959 + 0.9783574149862092 = 1.098326328355469.\n",
      "Training loss in epoch 122 of generation 1: 0.1249645901130262 + 0.9702253565679939 = 1.09518994668102.\n",
      "Evaluation loss in epoch 122 of generation 1: 0.11996891336925959 + 0.9999357438050727 = 1.1199046571743323.\n",
      "Training loss in epoch 123 of generation 1: 0.12496459121469079 + 0.9681151106947883 = 1.0930797019094791.\n",
      "Evaluation loss in epoch 123 of generation 1: 0.11996891483871154 + 0.9834228950582777 = 1.1033918098969893.\n",
      "Training loss in epoch 124 of generation 1: 0.12496466814760157 + 0.9715602068337377 = 1.0965248749813392.\n",
      "Evaluation loss in epoch 124 of generation 1: 0.11996891336925959 + 0.9798127807711232 = 1.0997816941403828.\n",
      "Training loss in epoch 125 of generation 1: 0.12496459103108003 + 0.9714389855374177 = 1.0964035765684979.\n",
      "Evaluation loss in epoch 125 of generation 1: 0.11996891336925959 + 0.9826450245038754 = 1.102613937873135.\n",
      "Training loss in epoch 126 of generation 1: 0.12496459158191232 + 0.9702887052198902 = 1.0952532968018025.\n",
      "Evaluation loss in epoch 126 of generation 1: 0.11996891336925959 + 0.9794250482403443 = 1.0993939616096038.\n",
      "Training loss in epoch 127 of generation 1: 0.12496459249996615 + 0.972281776578049 = 1.0972463690780152.\n",
      "Evaluation loss in epoch 127 of generation 1: 0.11996891336925959 + 0.9781933976983069 = 1.0981623110675665.\n",
      "Training loss in epoch 128 of generation 1: 0.12496459103108003 + 0.9707222293329367 = 1.0956868203640169.\n",
      "Evaluation loss in epoch 128 of generation 1: 0.11996891336925959 + 0.9806774268716069 = 1.1006463402408664.\n",
      "Training loss in epoch 129 of generation 1: 0.12496458992941543 + 0.9698505159245118 = 1.0948151058539273.\n",
      "Evaluation loss in epoch 129 of generation 1: 0.11996891336925959 + 1.008921458651362 = 1.1288903720206216.\n",
      "Training loss in epoch 130 of generation 1: 0.12496459213274462 + 0.9738163028169147 = 1.0987808949496594.\n",
      "Evaluation loss in epoch 130 of generation 1: 0.11996891336925959 + 0.9985827238791162 = 1.1185516372483757.\n",
      "Training loss in epoch 131 of generation 1: 0.1249645901130262 + 0.9686450113642432 = 1.0936096014772694.\n",
      "Evaluation loss in epoch 131 of generation 1: 0.11996891336925959 + 0.9894028738577303 = 1.10937178722699.\n",
      "Training loss in epoch 132 of generation 1: 0.12496459139830156 + 0.9763107637648496 = 1.1012753551631511.\n",
      "Evaluation loss in epoch 132 of generation 1: 0.11996891336925959 + 0.9837625383044979 = 1.1037314516737575.\n",
      "Training loss in epoch 133 of generation 1: 0.12496459185732847 + 0.9722427761825544 = 1.0972073680398828.\n",
      "Evaluation loss in epoch 133 of generation 1: 0.11996891336925959 + 0.9770400278006203 = 1.0970089411698798.\n",
      "Training loss in epoch 134 of generation 1: 0.1249645906638585 + 0.9665374241749232 = 1.0915020148387817.\n",
      "Evaluation loss in epoch 134 of generation 1: 0.11996891336925959 + 0.9987897285144024 = 1.118758641883662.\n",
      "Training loss in epoch 135 of generation 1: 0.12496459048024773 + 0.9696503008701822 = 1.0946148913504299.\n",
      "Evaluation loss in epoch 135 of generation 1: 0.11996891336925959 + 0.9763557099046987 = 1.0963246232739583.\n",
      "Training loss in epoch 136 of generation 1: 0.12496458992941543 + 0.98041490504133 = 1.1053794949707454.\n",
      "Evaluation loss in epoch 136 of generation 1: 0.11996891336925959 + 0.9851926897633793 = 1.1051616031326388.\n",
      "Training loss in epoch 137 of generation 1: 0.12496459103108003 + 0.9695780228595344 = 1.0945426138906145.\n",
      "Evaluation loss in epoch 137 of generation 1: 0.11996891336925959 + 0.985190854417892 = 1.1051597677871516.\n",
      "Training loss in epoch 138 of generation 1: 0.12496458992941543 + 0.9675844476734348 = 1.0925490376028502.\n",
      "Evaluation loss in epoch 138 of generation 1: 0.11996891336925959 + 0.9790901616100905 = 1.09905907497935.\n",
      "Training loss in epoch 139 of generation 1: 0.12496459176552309 + 0.9685428767742265 = 1.0935074685397494.\n",
      "Evaluation loss in epoch 139 of generation 1: 0.11996891336925959 + 0.9732594879456037 = 1.0932284013148632.\n",
      "Training loss in epoch 140 of generation 1: 0.12496459048024773 + 0.9714809428006812 = 1.0964455332809289.\n",
      "Evaluation loss in epoch 140 of generation 1: 0.11996891336925959 + 0.9740945143031046 = 1.0940634276723642.\n",
      "Training loss in epoch 141 of generation 1: 0.12496459084746926 + 0.9695910504105737 = 1.0945556412580428.\n",
      "Evaluation loss in epoch 141 of generation 1: 0.11996891336925959 + 0.9750257118824635 = 1.094994625251723.\n",
      "Training loss in epoch 142 of generation 1: 0.12496459158191232 + 0.9713221488661797 = 1.096286740448092.\n",
      "Evaluation loss in epoch 142 of generation 1: 0.11996891336925959 + 0.9738418420400752 = 1.0938107554093348.\n",
      "Training loss in epoch 143 of generation 1: 0.12496459084746926 + 0.9763005711640309 = 1.1012651620115002.\n",
      "Evaluation loss in epoch 143 of generation 1: 0.11996891336925959 + 0.9732748833936976 = 1.0932437967629571.\n",
      "Training loss in epoch 144 of generation 1: 0.12496459176552309 + 0.9758721719946731 = 1.1008367637601961.\n",
      "Evaluation loss in epoch 144 of generation 1: 0.11996891336925959 + 0.9745227214184307 = 1.0944916347876903.\n",
      "Training loss in epoch 145 of generation 1: 0.12496459249996615 + 0.9732074553938063 = 1.0981720478937724.\n",
      "Evaluation loss in epoch 145 of generation 1: 0.11996891336925959 + 0.9786917182440383 = 1.0986606316132979.\n",
      "Training loss in epoch 146 of generation 1: 0.12496459084746926 + 0.972164208401521 = 1.0971287992489902.\n",
      "Evaluation loss in epoch 146 of generation 1: 0.11996891336925959 + 0.9931170150568747 = 1.1130859284261343.\n",
      "Training loss in epoch 147 of generation 1: 0.12496459084746926 + 0.9730137519116672 = 1.0979783427591365.\n",
      "Evaluation loss in epoch 147 of generation 1: 0.11996891336925959 + 1.059533754004903 = 1.1795026673741626.\n",
      "Training loss in epoch 148 of generation 1: 0.12496458974580467 + 0.9777959192721808 = 1.1027605090179855.\n",
      "Evaluation loss in epoch 148 of generation 1: 0.11996891336925959 + 1.011809579764015 = 1.1317784931332746.\n",
      "Training loss in epoch 149 of generation 1: 0.12496458992941543 + 0.9735769948272647 = 1.0985415847566802.\n",
      "Evaluation loss in epoch 149 of generation 1: 0.11996891336925959 + 0.987736674045745 = 1.1077055874150046.\n",
      "Training loss in epoch 150 of generation 1: 0.12496459249996615 + 0.9719088249221232 = 1.0968734174220893.\n",
      "Evaluation loss in epoch 150 of generation 1: 0.11996891336925959 + 0.9899553187271847 = 1.1099242320964442.\n",
      "Training loss in epoch 151 of generation 1: 0.12496459286718768 + 0.9657506123845009 = 1.0907152052516886.\n",
      "Evaluation loss in epoch 151 of generation 1: 0.11996891336925959 + 0.9735302300592784 = 1.093499143428538.\n",
      "Training loss in epoch 152 of generation 1: 0.12496459305079845 + 0.9692829148237826 = 1.094247507874581.\n",
      "Evaluation loss in epoch 152 of generation 1: 0.11996891336925959 + 0.9774232520528127 = 1.0973921654220722.\n",
      "Training loss in epoch 153 of generation 1: 0.12496459121469079 + 0.9819059023087787 = 1.1068704935234697.\n",
      "Evaluation loss in epoch 153 of generation 1: 0.11996891336925959 + 0.9778926332118147 = 1.0978615465810744.\n",
      "Training loss in epoch 154 of generation 1: 0.12496459103108003 + 0.967727326226776 = 1.0926919172578562.\n",
      "Evaluation loss in epoch 154 of generation 1: 0.11996891336925959 + 0.9769365519331122 = 1.0969054653023718.\n",
      "Training loss in epoch 155 of generation 1: 0.12496459048024773 + 0.9693306169006795 = 1.0942952073809271.\n",
      "Evaluation loss in epoch 155 of generation 1: 0.11996891336925959 + 0.9767681292282598 = 1.0967370425975194.\n",
      "Training loss in epoch 156 of generation 1: 0.12496459084746926 + 0.9674560699639156 = 1.092420660811385.\n",
      "Evaluation loss in epoch 156 of generation 1: 0.11996891336925959 + 0.9799771551358498 = 1.0999460685051095.\n",
      "Training loss in epoch 157 of generation 1: 0.12496459048024773 + 0.9722999290727771 = 1.0972645195530248.\n",
      "Evaluation loss in epoch 157 of generation 1: 0.11996891336925959 + 0.9762707726430084 = 1.096239686012268.\n",
      "Training loss in epoch 158 of generation 1: 0.12496459084746926 + 0.969945702683159 = 1.0949102935306283.\n",
      "Evaluation loss in epoch 158 of generation 1: 0.11996891336925959 + 1.0292506666139387 = 1.1492195799831983.\n",
      "Training loss in epoch 159 of generation 1: 0.12496459084746926 + 0.9734228572617781 = 1.0983874481092475.\n",
      "Evaluation loss in epoch 159 of generation 1: 0.11996891336925959 + 0.9820193377408114 = 1.101988251110071.\n",
      "Training loss in epoch 160 of generation 1: 0.12496459176552309 + 0.9665206700597858 = 1.0914852618253088.\n",
      "Evaluation loss in epoch 160 of generation 1: 0.11996891336925959 + 0.9738781139920415 = 1.0938470273613012.\n",
      "Training loss in epoch 161 of generation 1: 0.12496459084746926 + 0.9697806131027275 = 1.0947452039501968.\n",
      "Evaluation loss in epoch 161 of generation 1: 0.11996891336925959 + 0.9909721339244872 = 1.1109410472937469.\n",
      "Training loss in epoch 162 of generation 1: 0.12496459103108003 + 0.9748306700391038 = 1.099795261070184.\n",
      "Evaluation loss in epoch 162 of generation 1: 0.11996891336925959 + 0.9832944928773195 = 1.1032634062465791.\n",
      "Training loss in epoch 163 of generation 1: 0.12496459029663697 + 0.9671775324325356 = 1.0921421227291725.\n",
      "Evaluation loss in epoch 163 of generation 1: 0.11996891336925959 + 0.9869811435693585 = 1.1069500569386181.\n",
      "Training loss in epoch 164 of generation 1: 0.1249645895621939 + 0.9712844014305494 = 1.0962489909927433.\n",
      "Evaluation loss in epoch 164 of generation 1: 0.11996891336925959 + 0.9864783794009263 = 1.1064472927701858.\n",
      "Training loss in epoch 165 of generation 1: 0.12496459194913385 + 0.9727407608942936 = 1.0977053528434275.\n",
      "Evaluation loss in epoch 165 of generation 1: 0.11996891336925959 + 0.973243631089597 = 1.0932125444588565.\n",
      "Training loss in epoch 166 of generation 1: 0.12496459139830156 + 0.9770008112129267 = 1.1019654026112282.\n",
      "Evaluation loss in epoch 166 of generation 1: 0.11996891336925959 + 0.98028552403619 = 1.1002544374054495.\n",
      "Training loss in epoch 167 of generation 1: 0.12496459084746926 + 0.9666700102431892 = 1.0916346010906586.\n",
      "Evaluation loss in epoch 167 of generation 1: 0.11996891336925959 + 0.9760916435112388 = 1.0960605568804984.\n",
      "Training loss in epoch 168 of generation 1: 0.12496459194913385 + 0.9691678187826023 = 1.094132410731736.\n",
      "Evaluation loss in epoch 168 of generation 1: 0.11996891336925959 + 1.0182849519609118 = 1.1382538653301715.\n",
      "Training loss in epoch 169 of generation 1: 0.12496459249996615 + 0.9758383024184117 = 1.100802894918378.\n",
      "Evaluation loss in epoch 169 of generation 1: 0.11996891336925959 + 0.977809794327511 = 1.0977787076967707.\n",
      "Training loss in epoch 170 of generation 1: 0.12496459231635539 + 0.9699500711504937 = 1.094914663466849.\n",
      "Evaluation loss in epoch 170 of generation 1: 0.11996891336925959 + 0.9756325220657607 = 1.0956014354350203.\n",
      "Training loss in epoch 171 of generation 1: 0.12496459158191232 + 0.9676048637216809 = 1.0925694553035932.\n",
      "Evaluation loss in epoch 171 of generation 1: 0.11996891336925959 + 0.9752372292046921 = 1.0952061425739517.\n",
      "Training loss in epoch 172 of generation 1: 0.12496459231635539 + 0.9710498335364235 = 1.0960144258527789.\n",
      "Evaluation loss in epoch 172 of generation 1: 0.11996891336925959 + 0.9732625282416909 = 1.0932314416109505.\n",
      "Training loss in epoch 173 of generation 1: 0.12496459020483158 + 0.9679101761093404 = 1.092874766314172.\n",
      "Evaluation loss in epoch 173 of generation 1: 0.11996891336925959 + 0.9974199979808555 = 1.117388911350115.\n",
      "Training loss in epoch 174 of generation 1: 0.12496459121469079 + 0.9724672307957123 = 1.0974318220104031.\n",
      "Evaluation loss in epoch 174 of generation 1: 0.11996891336925959 + 0.9733929244689471 = 1.0933618378382066.\n",
      "Training loss in epoch 175 of generation 1: 0.12496459249996615 + 0.9713456113842482 = 1.0963102038842143.\n",
      "Evaluation loss in epoch 175 of generation 1: 0.11996891336925959 + 0.980602593561534 = 1.1005715069307935.\n",
      "Training loss in epoch 176 of generation 1: 0.12496459286718768 + 0.9669218786780909 = 1.0918864715452785.\n",
      "Evaluation loss in epoch 176 of generation 1: 0.11996891336925959 + 0.987952016351403 = 1.1079209297206627.\n",
      "Training loss in epoch 177 of generation 1: 0.12496459121469079 + 0.9697747889692434 = 1.0947393801839342.\n",
      "Evaluation loss in epoch 177 of generation 1: 0.11996891336925959 + 0.9930400010801059 = 1.1130089144493656.\n",
      "Training loss in epoch 178 of generation 1: 0.12496459029663697 + 0.968259513951927 = 1.093224104248564.\n",
      "Evaluation loss in epoch 178 of generation 1: 0.11996891336925959 + 0.9761105862163432 = 1.0960794995856027.\n",
      "Training loss in epoch 179 of generation 1: 0.12496459158191232 + 0.9682894997932748 = 1.0932540913751871.\n",
      "Evaluation loss in epoch 179 of generation 1: 0.11996891336925959 + 0.9832314269384728 = 1.1032003403077324.\n",
      "Training loss in epoch 180 of generation 1: 0.12496459103108003 + 0.9782184002993426 = 1.1031829913304225.\n",
      "Evaluation loss in epoch 180 of generation 1: 0.11996891336925959 + 1.041813490387104 = 1.1617824037563635.\n",
      "Training loss in epoch 181 of generation 1: 0.12496458919497237 + 0.9742064624737904 = 1.0991710516687627.\n",
      "Evaluation loss in epoch 181 of generation 1: 0.11996891336925959 + 0.9738660086468666 = 1.093834922016126.\n",
      "Training loss in epoch 182 of generation 1: 0.12496459139830156 + 0.9724119992085406 = 1.0973765906068422.\n",
      "Evaluation loss in epoch 182 of generation 1: 0.11996891336925959 + 0.9886546436186381 = 1.1086235569878977.\n",
      "Training loss in epoch 183 of generation 1: 0.12496459103108003 + 0.9688069428395069 = 1.093771533870587.\n",
      "Evaluation loss in epoch 183 of generation 1: 0.11996891336925959 + 0.9977791437933735 = 1.117748057162633.\n",
      "Training loss in epoch 184 of generation 1: 0.12496459121469079 + 0.9753263809151221 = 1.1002909721298129.\n",
      "Evaluation loss in epoch 184 of generation 1: 0.11996891336925959 + 0.9762495390623119 = 1.0962184524315715.\n",
      "Training loss in epoch 185 of generation 1: 0.12496459121469079 + 0.9771461428060961 = 1.102110734020787.\n",
      "Evaluation loss in epoch 185 of generation 1: 0.11996891336925959 + 0.9766079472099504 = 1.0965768605792099.\n",
      "Training loss in epoch 186 of generation 1: 0.12496459139830156 + 0.9783485054189441 = 1.1033130968172458.\n",
      "Evaluation loss in epoch 186 of generation 1: 0.11996891336925959 + 1.0109634237774348 = 1.1309323371466944.\n",
      "Training loss in epoch 187 of generation 1: 0.12496459103108003 + 0.969431263509052 = 1.094395854540132.\n",
      "Evaluation loss in epoch 187 of generation 1: 0.11996891336925959 + 0.9770411857287579 = 1.0970100990980174.\n",
      "Training loss in epoch 188 of generation 1: 0.12496459249996615 + 0.9685459070863015 = 1.0935104995862677.\n",
      "Evaluation loss in epoch 188 of generation 1: 0.11996891336925959 + 0.9824353748550768 = 1.1024042882243363.\n",
      "Training loss in epoch 189 of generation 1: 0.12496459139830156 + 0.967390205110088 = 1.0923547965083895.\n",
      "Evaluation loss in epoch 189 of generation 1: 0.11996891336925959 + 0.9735481000644584 = 1.093517013433718.\n",
      "Training loss in epoch 190 of generation 1: 0.12496459084746926 + 0.9675365091058719 = 1.092501099953341.\n",
      "Evaluation loss in epoch 190 of generation 1: 0.11996891557343752 + 0.9989467585582763 = 1.1189156741317137.\n",
      "Training loss in epoch 191 of generation 1: 0.12496459543773841 + 0.968434959179537 = 1.0933995546172754.\n",
      "Evaluation loss in epoch 191 of generation 1: 0.11996892145124533 + 0.9733935651499979 = 1.0933624866012432.\n",
      "Training loss in epoch 192 of generation 1: 0.12496459158191232 + 0.9674698936512345 = 1.0924344852331467.\n",
      "Evaluation loss in epoch 192 of generation 1: 0.11996891336925959 + 0.9873128282638103 = 1.1072817416330698.\n",
      "Training loss in epoch 193 of generation 1: 0.12496459103108003 + 0.9762433110451212 = 1.1012079020762011.\n",
      "Evaluation loss in epoch 193 of generation 1: 0.11996891336925959 + 0.9897985796347972 = 1.1097674930040569.\n",
      "Training loss in epoch 194 of generation 1: 0.12496459176552309 + 0.9696560956259438 = 1.094620687391467.\n",
      "Evaluation loss in epoch 194 of generation 1: 0.11996891336925959 + 0.9880239577800357 = 1.1079928711492952.\n",
      "Training loss in epoch 195 of generation 1: 0.12496459194913385 + 0.969824869172776 = 1.0947894611219098.\n",
      "Evaluation loss in epoch 195 of generation 1: 0.11996891336925959 + 0.9743340467267853 = 1.0943029600960448.\n",
      "Training loss in epoch 196 of generation 1: 0.12496459139830156 + 0.9679057312599273 = 1.0928703226582288.\n",
      "Evaluation loss in epoch 196 of generation 1: 0.11996891777761545 + 1.0064776498474215 = 1.126446567625037.\n",
      "Training loss in epoch 197 of generation 1: 0.1260012117862518 + 0.9699200250848147 = 1.0959212368710665.\n",
      "Evaluation loss in epoch 197 of generation 1: 0.12055287853411423 + 0.9837510765792776 = 1.1043039551133917.\n",
      "Training loss in epoch 198 of generation 1: 0.12617251245985592 + 0.9711098948211728 = 1.0972824072810288.\n",
      "Evaluation loss in epoch 198 of generation 1: 0.12055287853411423 + 0.974226682689414 = 1.0947795612235283.\n",
      "Training loss in epoch 199 of generation 1: 0.12617251319429898 + 0.968449916846944 = 1.094622430041243.\n",
      "Evaluation loss in epoch 199 of generation 1: 0.12055287853411423 + 0.974026461044335 = 1.0945793395784493.\n",
      "Training loss in epoch 200 of generation 1: 0.12617251447957434 + 0.969059171151509 = 1.0952316856310833.\n",
      "Evaluation loss in epoch 200 of generation 1: 0.12055287853411423 + 0.9753596125104578 = 1.0959124910445721.\n",
      "Training loss in epoch 201 of generation 1: 0.12617251245985592 + 0.9681660722299957 = 1.0943385846898517.\n",
      "Evaluation loss in epoch 201 of generation 1: 0.12055287853411423 + 0.9962260418202734 = 1.1167789203543876.\n",
      "Training loss in epoch 202 of generation 1: 0.12617251245985592 + 0.9806260133551596 = 1.1067985258150155.\n",
      "Evaluation loss in epoch 202 of generation 1: 0.12055287853411423 + 0.9812074155763412 = 1.1017602941104554.\n",
      "Training loss in epoch 203 of generation 1: 0.12617251282707745 + 0.9723071471791934 = 1.098479660006271.\n",
      "Evaluation loss in epoch 203 of generation 1: 0.12055287853411423 + 0.9762101004413904 = 1.0967629789755047.\n",
      "Training loss in epoch 204 of generation 1: 0.12617251264346668 + 0.9700200709187622 = 1.0961925835622288.\n",
      "Evaluation loss in epoch 204 of generation 1: 0.12055287853411423 + 0.9749722620371861 = 1.0955251405713002.\n",
      "Training loss in epoch 205 of generation 1: 0.12617251282707745 + 0.9739336785693604 = 1.1001061913964378.\n",
      "Evaluation loss in epoch 205 of generation 1: 0.12055287853411423 + 0.973960206394754 = 1.0945130849288682.\n",
      "Training loss in epoch 206 of generation 1: 0.12617251484679587 + 0.9720040895317351 = 1.098176604378531.\n",
      "Evaluation loss in epoch 206 of generation 1: 0.12055287853411423 + 0.9963783152481808 = 1.116931193782295.\n",
      "Training loss in epoch 207 of generation 1: 0.12617251282707745 + 0.9779071168895864 = 1.104079629716664.\n",
      "Evaluation loss in epoch 207 of generation 1: 0.12055287853411423 + 1.0248587054354017 = 1.1454115839695158.\n",
      "Training loss in epoch 208 of generation 1: 0.1261725120926344 + 0.9700933874319001 = 1.0962658995245345.\n",
      "Evaluation loss in epoch 208 of generation 1: 0.12055287853411423 + 0.974021633894675 = 1.0945745124287891.\n",
      "Training loss in epoch 209 of generation 1: 0.12617251227624515 + 0.9721356782263226 = 1.0983081905025678.\n",
      "Evaluation loss in epoch 209 of generation 1: 0.12055287853411423 + 0.9747431627223598 = 1.095296041256474.\n",
      "Training loss in epoch 210 of generation 1: 0.1261725115418021 + 0.9705966469137128 = 1.0967691584555148.\n",
      "Evaluation loss in epoch 210 of generation 1: 0.12055287853411423 + 0.9824061930087755 = 1.1029590715428899.\n",
      "Training loss in epoch 211 of generation 1: 0.12617251319429898 + 0.9685166659702188 = 1.0946891791645177.\n",
      "Evaluation loss in epoch 211 of generation 1: 0.12055287853411423 + 0.9740126099902419 = 1.094565488524356.\n",
      "Training loss in epoch 212 of generation 1: 0.1261725109909698 + 0.9671651614735941 = 1.0933376724645638.\n",
      "Evaluation loss in epoch 212 of generation 1: 0.12055287853411423 + 0.9784625219453832 = 1.0990154004794974.\n",
      "Training loss in epoch 213 of generation 1: 0.1261725135615205 + 0.974639172823924 = 1.1008116863854445.\n",
      "Evaluation loss in epoch 213 of generation 1: 0.12055287853411423 + 0.99793802828561 = 1.1184909068197242.\n",
      "Training loss in epoch 214 of generation 1: 0.12617251264346668 + 0.9687039166356125 = 1.0948764292790791.\n",
      "Evaluation loss in epoch 214 of generation 1: 0.12055287853411423 + 0.9955492563350542 = 1.1161021348691684.\n",
      "Training loss in epoch 215 of generation 1: 0.12617251447957434 + 0.9702808158325148 = 1.0964533303120891.\n",
      "Evaluation loss in epoch 215 of generation 1: 0.12055287853411423 + 0.9850762753714398 = 1.105629153905554.\n",
      "Training loss in epoch 216 of generation 1: 0.1261725135615205 + 0.9673055061983704 = 1.093478019759891.\n",
      "Evaluation loss in epoch 216 of generation 1: 0.12055287853411423 + 0.9968092335023572 = 1.1173621120364714.\n",
      "Training loss in epoch 217 of generation 1: 0.12617251337790975 + 0.9680962369769731 = 1.094268750354883.\n",
      "Evaluation loss in epoch 217 of generation 1: 0.12055287853411423 + 0.993135023190538 = 1.1136879017246522.\n",
      "Training loss in epoch 218 of generation 1: 0.12617251282707745 + 0.971233181371384 = 1.0974056941984616.\n",
      "Evaluation loss in epoch 218 of generation 1: 0.12055287853411423 + 0.9732588634285243 = 1.0938117419626385.\n",
      "Training loss in epoch 219 of generation 1: 0.12617251245985592 + 0.9732774015477312 = 1.099449914007587.\n",
      "Evaluation loss in epoch 219 of generation 1: 0.12055287853411423 + 0.9906447018239825 = 1.1111975803580967.\n",
      "Training loss in epoch 220 of generation 1: 0.1261725135615205 + 0.9679413113540808 = 1.0941138249156013.\n",
      "Evaluation loss in epoch 220 of generation 1: 0.12055287853411423 + 1.017480294766889 = 1.1380331733010032.\n",
      "Training loss in epoch 221 of generation 1: 0.12617251190902362 + 0.9689550814740604 = 1.095127593383084.\n",
      "Evaluation loss in epoch 221 of generation 1: 0.12055287853411423 + 0.9744408141666642 = 1.0949936927007784.\n",
      "Training loss in epoch 222 of generation 1: 0.1261725135615205 + 0.9688512150673028 = 1.0950237286288231.\n",
      "Evaluation loss in epoch 222 of generation 1: 0.12055287853411423 + 0.9760482153282695 = 1.0966010938623836.\n",
      "Training loss in epoch 223 of generation 1: 0.12617251319429898 + 0.9813377547824111 = 1.1075102679767101.\n",
      "Evaluation loss in epoch 223 of generation 1: 0.12055287853411423 + 0.9735574575344844 = 1.0941103360685986.\n",
      "Training loss in epoch 224 of generation 1: 0.12617251126638596 + 0.9688355376456947 = 1.0950080489120806.\n",
      "Evaluation loss in epoch 224 of generation 1: 0.12055287853411423 + 0.9894377527692468 = 1.1099906313033612.\n",
      "Training loss in epoch 225 of generation 1: 0.12617251190902362 + 0.9722060760627309 = 1.0983785879717545.\n",
      "Evaluation loss in epoch 225 of generation 1: 0.12055287853411423 + 0.9990473322581804 = 1.1196002107922947.\n",
      "Training loss in epoch 226 of generation 1: 0.12617251457137973 + 0.9720910020548424 = 1.0982635166262222.\n",
      "Evaluation loss in epoch 226 of generation 1: 0.12055287853411423 + 0.9941396022614419 = 1.114692480795556.\n",
      "Training loss in epoch 227 of generation 1: 0.12617251025652673 + 0.9691504918018763 = 1.095323002058403.\n",
      "Evaluation loss in epoch 227 of generation 1: 0.12055287853411423 + 1.0157437831484848 = 1.1362966616825991.\n",
      "Training loss in epoch 228 of generation 1: 0.12617251264346668 + 0.9730309378793252 = 1.099203450522792.\n",
      "Evaluation loss in epoch 228 of generation 1: 0.12055287853411423 + 0.9944626877230746 = 1.1150155662571888.\n",
      "Training loss in epoch 229 of generation 1: 0.12617251245985592 + 0.9672942266218195 = 1.0934667390816755.\n",
      "Evaluation loss in epoch 229 of generation 1: 0.12055287853411423 + 0.9752133256298 = 1.0957662041639142.\n",
      "Training loss in epoch 230 of generation 1: 0.12617251319429898 + 0.966749241960751 = 1.09292175515505.\n",
      "Evaluation loss in epoch 230 of generation 1: 0.12055287853411423 + 0.9772583530937394 = 1.0978112316278537.\n",
      "Training loss in epoch 231 of generation 1: 0.12617251190902362 + 0.9677372059548502 = 1.093909717863874.\n",
      "Evaluation loss in epoch 231 of generation 1: 0.12055287853411423 + 0.9841585820555135 = 1.1047114605896278.\n",
      "Training loss in epoch 232 of generation 1: 0.12617251429596357 + 0.9729262665229712 = 1.0990987808189348.\n",
      "Evaluation loss in epoch 232 of generation 1: 0.12055287853411423 + 0.9925119961355059 = 1.1130648746696201.\n",
      "Training loss in epoch 233 of generation 1: 0.1261725120926344 + 0.9668785685707065 = 1.093051080663341.\n",
      "Evaluation loss in epoch 233 of generation 1: 0.12055287853411423 + 0.9759237013171744 = 1.0964765798512885.\n",
      "Training loss in epoch 234 of generation 1: 0.12617251227624515 + 0.9683522917373298 = 1.094524804013575.\n",
      "Evaluation loss in epoch 234 of generation 1: 0.12055287853411423 + 0.9807352321724678 = 1.1012881107065822.\n",
      "Training loss in epoch 235 of generation 1: 0.1261725104401375 + 0.9671963650215393 = 1.0933688754616768.\n",
      "Evaluation loss in epoch 235 of generation 1: 0.12055287853411423 + 1.0107210215140932 = 1.1312739000482075.\n",
      "Training loss in epoch 236 of generation 1: 0.12617251264346668 + 0.9694577093348394 = 1.0956302219783063.\n",
      "Evaluation loss in epoch 236 of generation 1: 0.12055287853411423 + 0.9950892502795016 = 1.1156421288136158.\n",
      "Training loss in epoch 237 of generation 1: 0.12617251337790975 + 0.9698335620408615 = 1.0960060754187713.\n",
      "Evaluation loss in epoch 237 of generation 1: 0.12055287853411423 + 1.0105588674912651 = 1.1311117460253794.\n",
      "Training loss in epoch 238 of generation 1: 0.12617251392874204 + 0.9715782168465116 = 1.0977507307752536.\n",
      "Evaluation loss in epoch 238 of generation 1: 0.12055287853411423 + 1.0270257943951293 = 1.1475786729292436.\n",
      "Training loss in epoch 239 of generation 1: 0.12617251025652673 + 0.9752328965954933 = 1.10140540685202.\n",
      "Evaluation loss in epoch 239 of generation 1: 0.12055287853411423 + 0.9770784642553109 = 1.097631342789425.\n",
      "Training loss in epoch 240 of generation 1: 0.12617251245985592 + 0.977515298860276 = 1.1036878113201318.\n",
      "Evaluation loss in epoch 240 of generation 1: 0.12055287853411423 + 0.9772788122732576 = 1.097831690807372.\n",
      "Training loss in epoch 241 of generation 1: 0.12617251025652673 + 0.969842467897435 = 1.0960149781539616.\n",
      "Evaluation loss in epoch 241 of generation 1: 0.12055287853411423 + 0.9736851984620646 = 1.0942380769961788.\n",
      "Training loss in epoch 242 of generation 1: 0.12617251282707745 + 0.9704164689350091 = 1.0965889817620866.\n",
      "Evaluation loss in epoch 242 of generation 1: 0.12055287853411423 + 1.0049146437681695 = 1.1254675223022836.\n",
      "Training loss in epoch 243 of generation 1: 0.12617251374513128 + 0.9676856348319012 = 1.0938581485770325.\n",
      "Evaluation loss in epoch 243 of generation 1: 0.12055287853411423 + 0.9766091007297322 = 1.0971619792638465.\n",
      "Training loss in epoch 244 of generation 1: 0.1261725115418021 + 0.966839804665878 = 1.0930123162076801.\n",
      "Evaluation loss in epoch 244 of generation 1: 0.12055287853411423 + 0.9787339708654466 = 1.0992868493995607.\n",
      "Training loss in epoch 245 of generation 1: 0.12617251392874204 + 0.9688990008707087 = 1.0950715147994508.\n",
      "Evaluation loss in epoch 245 of generation 1: 0.12055287853411423 + 0.9934075550966894 = 1.1139604336308038.\n",
      "Training loss in epoch 246 of generation 1: 0.12617251301068821 + 0.9663794998210075 = 1.0925520128316957.\n",
      "Evaluation loss in epoch 246 of generation 1: 0.12055287853411423 + 1.0239544032351078 = 1.1445072817692221.\n",
      "Training loss in epoch 247 of generation 1: 0.1261725135615205 + 0.9685453650673215 = 1.094717878628842.\n",
      "Evaluation loss in epoch 247 of generation 1: 0.12055287853411423 + 0.9747010394127233 = 1.0952539179468375.\n",
      "Training loss in epoch 248 of generation 1: 0.1261725115418021 + 0.968058185481915 = 1.094230697023717.\n",
      "Evaluation loss in epoch 248 of generation 1: 0.12055287853411423 + 1.0085966230724552 = 1.1291495016065694.\n",
      "Training loss in epoch 249 of generation 1: 0.12617251392874204 + 0.9704451048700086 = 1.0966176187987506.\n",
      "Evaluation loss in epoch 249 of generation 1: 0.12055287853411423 + 0.9781865412355021 = 1.0987394197696163.\n",
      "Training loss in epoch 250 of generation 1: 0.12617251282707745 + 0.9722023069009351 = 1.0983748197280125.\n",
      "Evaluation loss in epoch 250 of generation 1: 0.12055287853411423 + 0.9775588677733998 = 1.098111746307514.\n",
      "Training loss in epoch 251 of generation 1: 0.12617251301068821 + 0.9684580324427825 = 1.0946305454534708.\n",
      "Evaluation loss in epoch 251 of generation 1: 0.12055287853411423 + 0.9812346195403159 = 1.1017874980744302.\n",
      "Training loss in epoch 252 of generation 1: 0.12617251374513128 + 0.9689419070344087 = 1.09511442077954.\n",
      "Evaluation loss in epoch 252 of generation 1: 0.12055287853411423 + 1.0022672864799322 = 1.1228201650140466.\n",
      "Training loss in epoch 253 of generation 1: 0.12617251392874204 + 0.9696069026296305 = 1.0957794165583725.\n",
      "Evaluation loss in epoch 253 of generation 1: 0.12055287853411423 + 0.9739896218839155 = 1.0945425004180296.\n",
      "Training loss in epoch 254 of generation 1: 0.12617251135819132 + 0.9720373598024576 = 1.098209871160649.\n",
      "Evaluation loss in epoch 254 of generation 1: 0.12055287853411423 + 0.9740822399709551 = 1.0946351185050693.\n",
      "Training loss in epoch 255 of generation 1: 0.12617251025652673 + 0.9663066548203155 = 1.0924791650768422.\n",
      "Evaluation loss in epoch 255 of generation 1: 0.12055287853411423 + 0.9965370366312506 = 1.1170899151653648.\n",
      "Training loss in epoch 256 of generation 1: 0.1261725152140174 + 0.9680761528969912 = 1.0942486681110086.\n",
      "Evaluation loss in epoch 256 of generation 1: 0.12055287853411423 + 0.9805373792839345 = 1.1010902578180486.\n",
      "Training loss in epoch 257 of generation 1: 0.12617251337790975 + 0.9667776414050843 = 1.092950154782994.\n",
      "Evaluation loss in epoch 257 of generation 1: 0.12055287853411423 + 0.9745125146051768 = 1.095065393139291.\n",
      "Training loss in epoch 258 of generation 1: 0.12617251135819132 + 0.9688954696684651 = 1.0950679810266564.\n",
      "Evaluation loss in epoch 258 of generation 1: 0.12055287853411423 + 0.986297516315854 = 1.1068503948499682.\n",
      "Training loss in epoch 259 of generation 1: 0.12617251503040663 + 0.9676509000817124 = 1.0938234151121191.\n",
      "Evaluation loss in epoch 259 of generation 1: 0.12055254184593589 + 0.9770509678703978 = 1.0976035097163337.\n",
      "Training loss in epoch 260 of generation 1: 0.12812927038246363 + 0.9757340864167564 = 1.10386335679922.\n",
      "Evaluation loss in epoch 260 of generation 1: 0.12335059051337338 + 1.007931278740129 = 1.1312818692535025.\n",
      "Training loss in epoch 261 of generation 1: 0.12818789913602 + 0.9689985355322849 = 1.097186434668305.\n",
      "Evaluation loss in epoch 261 of generation 1: 0.12335059051337338 + 0.9768689189376009 = 1.1002195094509741.\n",
      "Training loss in epoch 262 of generation 1: 0.12818789913602 + 0.9698102757891279 = 1.097998174925148.\n",
      "Evaluation loss in epoch 262 of generation 1: 0.12335059051337338 + 0.9739339017353734 = 1.0972844922487468.\n",
      "Training loss in epoch 263 of generation 1: 0.12818789876879846 + 0.9708712390169622 = 1.0990591377857606.\n",
      "Evaluation loss in epoch 263 of generation 1: 0.12335059051337338 + 0.9907938688304648 = 1.1141444593438383.\n",
      "Training loss in epoch 264 of generation 1: 0.12818790124754378 + 0.9695476727344277 = 1.0977355739819714.\n",
      "Evaluation loss in epoch 264 of generation 1: 0.12335059051337338 + 0.9756317432562265 = 1.0989823337695999.\n",
      "Training loss in epoch 265 of generation 1: 0.12818790097212765 + 0.9704424785016177 = 1.0986303794737453.\n",
      "Evaluation loss in epoch 265 of generation 1: 0.12335059051337338 + 0.9732451593196263 = 1.0965957498329997.\n",
      "Training loss in epoch 266 of generation 1: 0.12818790133934918 + 0.969262385671305 = 1.0974502870106542.\n",
      "Evaluation loss in epoch 266 of generation 1: 0.12335059051337338 + 0.9840058648531169 = 1.1073564553664903.\n",
      "Training loss in epoch 267 of generation 1: 0.12818790115573842 + 0.97595299745368 = 1.1041408986094183.\n",
      "Evaluation loss in epoch 267 of generation 1: 0.12335059051337338 + 0.9766208210784959 = 1.0999714115918693.\n",
      "Training loss in epoch 268 of generation 1: 0.12818790060490612 + 0.9688724345962589 = 1.097060335201165.\n",
      "Evaluation loss in epoch 268 of generation 1: 0.12335059051337338 + 0.9853117139019841 = 1.1086623044153574.\n",
      "Training loss in epoch 269 of generation 1: 0.12818789987046306 + 0.9696946538867149 = 1.0978825537571781.\n",
      "Evaluation loss in epoch 269 of generation 1: 0.12335059051337338 + 1.0004534728721772 = 1.1238040633855506.\n",
      "Training loss in epoch 270 of generation 1: 0.1281879028082353 + 0.9686946920307499 = 1.096882594838985.\n",
      "Evaluation loss in epoch 270 of generation 1: 0.12335059051337338 + 0.9858355294390709 = 1.1091861199524442.\n",
      "Training loss in epoch 271 of generation 1: 0.12818790097212765 + 0.9668288923108582 = 1.095016793282986.\n",
      "Evaluation loss in epoch 271 of generation 1: 0.12335059051337338 + 0.982381212325603 = 1.1057318028389764.\n",
      "Training loss in epoch 272 of generation 1: 0.12818790189018148 + 0.9731731025540099 = 1.1013610044441913.\n",
      "Evaluation loss in epoch 272 of generation 1: 0.12335059051337338 + 1.0387847956229799 = 1.1621353861363533.\n",
      "Training loss in epoch 273 of generation 1: 0.12818790060490612 + 0.9745786943755151 = 1.1027665949804213.\n",
      "Evaluation loss in epoch 273 of generation 1: 0.12335059051337338 + 0.9793768663803124 = 1.1027274568936858.\n",
      "Training loss in epoch 274 of generation 1: 0.12818790124754378 + 0.97003004024889 = 1.0982179414964337.\n",
      "Evaluation loss in epoch 274 of generation 1: 0.12335059051337338 + 1.0025121192168014 = 1.1258627097301748.\n",
      "Training loss in epoch 275 of generation 1: 0.12818789876879846 + 0.9687351671879136 = 1.0969230659567122.\n",
      "Evaluation loss in epoch 275 of generation 1: 0.12335059051337338 + 0.9744251380532475 = 1.0977757285666208.\n",
      "Training loss in epoch 276 of generation 1: 0.1281878996868523 + 0.9647470018154022 = 1.0929349015022545.\n",
      "Evaluation loss in epoch 276 of generation 1: 0.12335059051337338 + 0.9737381957161408 = 1.0970887862295142.\n",
      "Training loss in epoch 277 of generation 1: 0.12818790189018148 + 0.9706660899741401 = 1.0988539918643216.\n",
      "Evaluation loss in epoch 277 of generation 1: 0.12335059051337338 + 0.9883443600224052 = 1.1116949505357787.\n",
      "Training loss in epoch 278 of generation 1: 0.12818790133934918 + 0.9747268829521969 = 1.102914784291546.\n",
      "Evaluation loss in epoch 278 of generation 1: 0.12335059051337338 + 0.9732634393019007 = 1.0966140298152742.\n",
      "Training loss in epoch 279 of generation 1: 0.12818790262462454 + 0.9719920960765276 = 1.1001799987011522.\n",
      "Evaluation loss in epoch 279 of generation 1: 0.12335059051337338 + 0.9786576240198638 = 1.1020082145332373.\n",
      "Training loss in epoch 280 of generation 1: 0.12818789950324153 + 0.9686531152089927 = 1.096841014712234.\n",
      "Evaluation loss in epoch 280 of generation 1: 0.12335059051337338 + 0.9746318252869123 = 1.0979824158002855.\n",
      "Training loss in epoch 281 of generation 1: 0.12818790189018148 + 0.9735695461057267 = 1.1017574479959082.\n",
      "Evaluation loss in epoch 281 of generation 1: 0.12335059051337338 + 0.9909183754943002 = 1.1142689660076734.\n",
      "Training loss in epoch 282 of generation 1: 0.12818789913602 + 0.9671080306166633 = 1.0952959297526832.\n",
      "Evaluation loss in epoch 282 of generation 1: 0.12335059051337338 + 0.9735098605163293 = 1.0968604510297026.\n",
      "Training loss in epoch 283 of generation 1: 0.12818790005407382 + 0.9710337639222202 = 1.099221663976294.\n",
      "Evaluation loss in epoch 283 of generation 1: 0.12335059051337338 + 1.0163116264049372 = 1.1396622169183106.\n",
      "Training loss in epoch 284 of generation 1: 0.128187902257403 + 0.9723293464551946 = 1.1005172487125976.\n",
      "Evaluation loss in epoch 284 of generation 1: 0.12335059051337338 + 0.986697446767281 = 1.1100480372806543.\n",
      "Training loss in epoch 285 of generation 1: 0.1281879017065707 + 0.9684874072275026 = 1.0966753089340733.\n",
      "Evaluation loss in epoch 285 of generation 1: 0.12335059051337338 + 0.9779829751177819 = 1.1013335656311551.\n",
      "Training loss in epoch 286 of generation 1: 0.12818790106393302 + 0.9684937190311801 = 1.096681620095113.\n",
      "Evaluation loss in epoch 286 of generation 1: 0.12335059051337338 + 1.0387850233880325 = 1.1621356139014059.\n",
      "Training loss in epoch 287 of generation 1: 0.12818790042129535 + 0.9721863386398745 = 1.1003742390611697.\n",
      "Evaluation loss in epoch 287 of generation 1: 0.12335059051337338 + 0.9732648308728986 = 1.096615421386272.\n",
      "Training loss in epoch 288 of generation 1: 0.12818790189018148 + 0.9670476828991196 = 1.0952355847893012.\n",
      "Evaluation loss in epoch 288 of generation 1: 0.12335059051337338 + 0.9747285240420208 = 1.098079114555394.\n",
      "Training loss in epoch 289 of generation 1: 0.1281879002376846 + 0.9712344196423871 = 1.0994223198800717.\n",
      "Evaluation loss in epoch 289 of generation 1: 0.12335059051337338 + 0.9964198037545744 = 1.1197703942679478.\n",
      "Training loss in epoch 290 of generation 1: 0.12818789913602 + 0.9684330040921052 = 1.096620903228125.\n",
      "Evaluation loss in epoch 290 of generation 1: 0.12335059051337338 + 0.9744685397860816 = 1.097819130299455.\n",
      "Training loss in epoch 291 of generation 1: 0.1281879002376846 + 0.9688137261556303 = 1.0970016263933149.\n",
      "Evaluation loss in epoch 291 of generation 1: 0.12335059051337338 + 1.0000251599563106 = 1.123375750469684.\n",
      "Training loss in epoch 292 of generation 1: 0.12818789876879846 + 0.9703201893250851 = 1.0985080880938836.\n",
      "Evaluation loss in epoch 292 of generation 1: 0.12335059051337338 + 0.9803323848589176 = 1.103682975372291.\n",
      "Training loss in epoch 293 of generation 1: 0.12818789913602 + 0.9675151544393922 = 1.0957030535754122.\n",
      "Evaluation loss in epoch 293 of generation 1: 0.12335059051337338 + 0.9783311764521665 = 1.1016817669655399.\n",
      "Training loss in epoch 294 of generation 1: 0.12818789876879846 + 0.9719261842183439 = 1.1001140829871423.\n",
      "Evaluation loss in epoch 294 of generation 1: 0.12335059051337338 + 0.9735159969476779 = 1.0968665874610513.\n",
      "Training loss in epoch 295 of generation 1: 0.12818789895240923 + 0.9680739378167154 = 1.0962618367691246.\n",
      "Evaluation loss in epoch 295 of generation 1: 0.12335059051337338 + 0.975689865958709 = 1.0990404564720824.\n",
      "Training loss in epoch 296 of generation 1: 0.12818789931963076 + 0.9736909319172926 = 1.1018788312369234.\n",
      "Evaluation loss in epoch 296 of generation 1: 0.12335059051337338 + 1.005269675115224 = 1.1286202656285973.\n",
      "Training loss in epoch 297 of generation 1: 0.12818790078851688 + 0.9685953894531814 = 1.0967832902416983.\n",
      "Evaluation loss in epoch 297 of generation 1: 0.12335059051337338 + 0.9773771641618114 = 1.1007277546751848.\n",
      "Training loss in epoch 298 of generation 1: 0.12818789840157693 + 0.9672860948682336 = 1.0954739932698105.\n",
      "Evaluation loss in epoch 298 of generation 1: 0.12335059051337338 + 0.9950483231037534 = 1.1183989136171268.\n",
      "Training loss in epoch 299 of generation 1: 0.12818789785074464 + 0.9850582788208149 = 1.1132461766715596.\n",
      "Evaluation loss in epoch 299 of generation 1: 0.12335058977864741 + 1.0618308057035615 = 1.1851813954822088.\n",
      "Training loss in epoch 300 of generation 1: 0.12818789729991234 + 0.9800059025132477 = 1.1081937998131601.\n",
      "Evaluation loss in epoch 300 of generation 1: 0.12335058830919544 + 0.9756205666046848 = 1.0989711549138803.\n",
      "Training loss in epoch 301 of generation 1: 0.12818789931963076 + 0.9683137966386621 = 1.096501695958293.\n",
      "Evaluation loss in epoch 301 of generation 1: 0.12335058830919544 + 1.0090680159111787 = 1.132418604220374.\n",
      "Training loss in epoch 302 of generation 1: 0.12818789931963076 + 0.9693759834486382 = 1.097563882768269.\n",
      "Evaluation loss in epoch 302 of generation 1: 0.12335058830919544 + 0.9772552319777948 = 1.1006058202869904.\n",
      "Training loss in epoch 303 of generation 1: 0.12818789785074464 + 0.9705479136787565 = 1.0987358115295012.\n",
      "Evaluation loss in epoch 303 of generation 1: 0.12335058830919544 + 0.9878778119667653 = 1.1112284002759607.\n",
      "Training loss in epoch 304 of generation 1: 0.1281878980343554 + 0.9690250077980227 = 1.097212905832378.\n",
      "Evaluation loss in epoch 304 of generation 1: 0.12335058830919544 + 0.9743191890981054 = 1.0976697774073008.\n",
      "Training loss in epoch 305 of generation 1: 0.12818789840157693 + 0.9715821417102366 = 1.0997700401118136.\n",
      "Evaluation loss in epoch 305 of generation 1: 0.12335058977864741 + 1.0085320391897428 = 1.13188262896839.\n",
      "Training loss in epoch 306 of generation 1: 0.12818789785074464 + 0.971860411904342 = 1.1000483097550866.\n",
      "Evaluation loss in epoch 306 of generation 1: 0.12335058904392142 + 0.9737406423536398 = 1.0970912313975612.\n",
      "Training loss in epoch 307 of generation 1: 0.12818789987046306 + 0.9790242679494594 = 1.1072121678199225.\n",
      "Evaluation loss in epoch 307 of generation 1: 0.12335058463556557 + 0.9733859328165627 = 1.0967365174521284.\n",
      "Training loss in epoch 308 of generation 1: 0.12818789876879846 + 0.9673093634933336 = 1.0954972622621322.\n",
      "Evaluation loss in epoch 308 of generation 1: 0.12335059069705487 + 0.9821705046713995 = 1.1055210953684544.\n",
      "Training loss in epoch 309 of generation 1: 0.12699939619820072 + 0.9728092080499284 = 1.0998086042481292.\n",
      "Evaluation loss in epoch 309 of generation 1: 0.1215352227764982 + 0.9859972455652175 = 1.1075324683417158.\n",
      "Training loss in epoch 310 of generation 1: 0.12535431185822601 + 0.9715754964694088 = 1.0969298083276349.\n",
      "Evaluation loss in epoch 310 of generation 1: 0.11996841761290751 + 0.9817841519560028 = 1.1017525695689103.\n",
      "Training loss in epoch 311 of generation 1: 0.12496475719882288 + 0.9731683786162332 = 1.098133135815056.\n",
      "Evaluation loss in epoch 311 of generation 1: 0.11996190206295537 + 0.9743808340769152 = 1.0943427361398705.\n",
      "Training loss in epoch 312 of generation 1: 0.124964782169887 + 0.9639242698836886 = 1.0888890520535757.\n",
      "Evaluation loss in epoch 312 of generation 1: 0.11996892108388234 + 0.9751877454102315 = 1.095156666494114.\n",
      "Training loss in epoch 313 of generation 1: 0.12496464336014822 + 0.9722325644862161 = 1.0971972078463643.\n",
      "Evaluation loss in epoch 313 of generation 1: 0.11996886855097508 + 0.9742226152464127 = 1.0941914837973878.\n",
      "Training loss in epoch 314 of generation 1: 0.12496463197628076 + 0.9699822764787759 = 1.0949469084550565.\n",
      "Evaluation loss in epoch 314 of generation 1: 0.11996866392979086 + 1.0092217573215856 = 1.1291904212513764.\n",
      "Training loss in epoch 315 of generation 1: 0.12496464629792046 + 0.9691566141192437 = 1.0941212604171642.\n",
      "Evaluation loss in epoch 315 of generation 1: 0.11996695716134939 + 1.0025449555901056 = 1.122511912751455.\n",
      "Training loss in epoch 316 of generation 1: 0.12601960178150504 + 0.9699156169575547 = 1.0959352187390596.\n",
      "Evaluation loss in epoch 316 of generation 1: 0.12055293051597701 + 0.978205893917701 = 1.098758824433678.\n",
      "Training loss in epoch 317 of generation 1: 0.12617256515614564 + 0.9744996403931745 = 1.10067220554932.\n",
      "Evaluation loss in epoch 317 of generation 1: 0.12055293345488091 + 1.0217822064603974 = 1.1423351399152781.\n",
      "Training loss in epoch 318 of generation 1: 0.12617257360224088 + 0.9739554239595491 = 1.1001279975617901.\n",
      "Evaluation loss in epoch 318 of generation 1: 0.1205529455778595 + 0.9732797825465034 = 1.093832728124363.\n",
      "Training loss in epoch 319 of generation 1: 0.12617259306498202 + 0.9711197378270939 = 1.097292330892076.\n",
      "Evaluation loss in epoch 319 of generation 1: 0.12055295770083811 + 1.0036751096164134 = 1.1242280673172513.\n",
      "Training loss in epoch 320 of generation 1: 0.12617259838969422 + 0.9715041908613755 = 1.0976767892510697.\n",
      "Evaluation loss in epoch 320 of generation 1: 0.12055295770083811 + 0.9812788603302143 = 1.1018318180310525.\n",
      "Training loss in epoch 321 of generation 1: 0.12617259894052651 + 0.9724381732903952 = 1.0986107722309217.\n",
      "Evaluation loss in epoch 321 of generation 1: 0.12055295696611214 + 0.9896606451190307 = 1.110213602085143.\n",
      "Training loss in epoch 322 of generation 1: 0.1261725926977605 + 0.970879558787972 = 1.0970521514857325.\n",
      "Evaluation loss in epoch 322 of generation 1: 0.12055295696611214 + 0.9795429247369751 = 1.1000958817030873.\n",
      "Training loss in epoch 323 of generation 1: 0.12617259233053896 + 0.9699308610577559 = 1.096103453388295.\n",
      "Evaluation loss in epoch 323 of generation 1: 0.12055295696611214 + 0.9840276127419963 = 1.1045805697081086.\n",
      "Training loss in epoch 324 of generation 1: 0.12617259508470044 + 0.9688333666320025 = 1.095005961716703.\n",
      "Evaluation loss in epoch 324 of generation 1: 0.12055295696611214 + 0.9796182444356439 = 1.1001712014017562.\n",
      "Training loss in epoch 325 of generation 1: 0.12617259655358656 + 0.9680073150176474 = 1.0941799115712338.\n",
      "Evaluation loss in epoch 325 of generation 1: 0.12055295696611214 + 0.9757275750346838 = 1.096280532000796.\n",
      "Training loss in epoch 326 of generation 1: 0.12617259545192197 + 0.9714220698448066 = 1.0975946652967286.\n",
      "Evaluation loss in epoch 326 of generation 1: 0.12055295696611214 + 0.9844468341586402 = 1.1049997911247524.\n",
      "Training loss in epoch 327 of generation 1: 0.1261725969208081 + 0.9705979204379829 = 1.096770517358791.\n",
      "Evaluation loss in epoch 327 of generation 1: 0.12055295696611214 + 1.0005380648126587 = 1.1210910217787708.\n",
      "Training loss in epoch 328 of generation 1: 0.12617259710441886 + 0.9702575281118951 = 1.096430125216314.\n",
      "Evaluation loss in epoch 328 of generation 1: 0.12055295696611214 + 0.973756751955382 = 1.0943097089214942.\n",
      "Training loss in epoch 329 of generation 1: 0.12617259196331743 + 0.969592705845236 = 1.0957652978085535.\n",
      "Evaluation loss in epoch 329 of generation 1: 0.1205529338222439 + 0.9897473163340235 = 1.1103002501562673.\n",
      "Training loss in epoch 330 of generation 1: 0.12617258718943752 + 0.968499422716002 = 1.0946720099054394.\n",
      "Evaluation loss in epoch 330 of generation 1: 0.12055293529169585 + 0.979291815970822 = 1.0998447512625178.\n",
      "Training loss in epoch 331 of generation 1: 0.12617257213335475 + 0.9674238572912028 = 1.0935964294245577.\n",
      "Evaluation loss in epoch 331 of generation 1: 0.12055293308751792 + 0.9988193482573852 = 1.1193722813449032.\n",
      "Training loss in epoch 332 of generation 1: 0.12617257176613322 + 0.9707214948898745 = 1.0968940666560076.\n",
      "Evaluation loss in epoch 332 of generation 1: 0.12055294667994848 + 0.9740070348895385 = 1.094559981569487.\n",
      "Training loss in epoch 333 of generation 1: 0.1261725798450069 + 0.9660649848617772 = 1.092237564706784.\n",
      "Evaluation loss in epoch 333 of generation 1: 0.12055293455696987 + 0.9965774406821041 = 1.117130375239074.\n",
      "Training loss in epoch 334 of generation 1: 0.12617256387087028 + 0.9673176553555071 = 1.0934902192263773.\n",
      "Evaluation loss in epoch 334 of generation 1: 0.12055291618882048 + 1.0061275058975572 = 1.1266804220863778.\n",
      "Training loss in epoch 335 of generation 1: 0.12617252512899876 + 0.9714051350566759 = 1.0975776601856748.\n",
      "Evaluation loss in epoch 335 of generation 1: 0.12055284436935636 + 0.9975035715911348 = 1.118056415960491.\n",
      "Training loss in epoch 336 of generation 1: 0.1256365987214392 + 0.977777462718025 = 1.1034140614394643.\n",
      "Evaluation loss in epoch 336 of generation 1: 0.11996890896090374 + 0.9927468219045865 = 1.1127157308654902.\n",
      "Training loss in epoch 337 of generation 1: 0.12496459249996615 + 0.9696280340254192 = 1.0945926265253854.\n",
      "Evaluation loss in epoch 337 of generation 1: 0.11996891189980764 + 0.9747383032447569 = 1.0947072151445645.\n",
      "Training loss in epoch 338 of generation 1: 0.12496459084746926 + 0.9726861462392943 = 1.0976507370867636.\n",
      "Evaluation loss in epoch 338 of generation 1: 0.1199689104303557 + 1.0256705746996018 = 1.1456394851299574.\n",
      "Training loss in epoch 339 of generation 1: 0.124964596539403 + 0.9714570557745227 = 1.0964216523139256.\n",
      "Evaluation loss in epoch 339 of generation 1: 0.1199689253085567 + 0.9733201498610581 = 1.0932890751696147.\n",
      "Training loss in epoch 340 of generation 1: 0.12496460939215659 + 0.970320459600132 = 1.0952850689922886.\n",
      "Evaluation loss in epoch 340 of generation 1: 0.11996892916586807 + 0.9979281109543905 = 1.1178970401202586.\n",
      "Training loss in epoch 341 of generation 1: 0.12496462573351473 + 0.9789620753109433 = 1.103926701044458.\n",
      "Evaluation loss in epoch 341 of generation 1: 0.11996894992187689 + 0.9904608822015841 = 1.1104298321234611.\n",
      "Training loss in epoch 342 of generation 1: 0.12496462903850851 + 0.966098801558138 = 1.0910634305966465.\n",
      "Evaluation loss in epoch 342 of generation 1: 0.11996894992187689 + 0.9855004694134869 = 1.1054694193353638.\n",
      "Training loss in epoch 343 of generation 1: 0.12496463087461616 + 0.9697090563151668 = 1.094673687189783.\n",
      "Evaluation loss in epoch 343 of generation 1: 0.11996895212605481 + 0.9851952980405928 = 1.1051642501666477.\n",
      "Training loss in epoch 344 of generation 1: 0.12496463142544846 + 0.9688028754938277 = 1.0937675069192763.\n",
      "Evaluation loss in epoch 344 of generation 1: 0.11996895359550676 + 0.9798469661013187 = 1.0998159196968256.\n",
      "Training loss in epoch 345 of generation 1: 0.12496463785182525 + 0.9676550496850144 = 1.0926196875368397.\n",
      "Evaluation loss in epoch 345 of generation 1: 0.11996895800386262 + 0.9751803467196566 = 1.0951493047235192.\n",
      "Training loss in epoch 346 of generation 1: 0.12496463454683147 + 0.9710950605402003 = 1.0960596950870318.\n",
      "Evaluation loss in epoch 346 of generation 1: 0.11996895433023273 + 0.9812096006513927 = 1.1011785549816255.\n",
      "Training loss in epoch 347 of generation 1: 0.12496463711738219 + 0.9665715934039539 = 1.091536230521336.\n",
      "Evaluation loss in epoch 347 of generation 1: 0.11996895194237331 + 1.0064021596923265 = 1.1263711116346997.\n",
      "Training loss in epoch 348 of generation 1: 0.12496463160905923 + 0.9706067425680472 = 1.0955713741771065.\n",
      "Evaluation loss in epoch 348 of generation 1: 0.11996894900346941 + 0.9748423051760634 = 1.0948112541795327.\n",
      "Training loss in epoch 349 of generation 1: 0.12496463179266999 + 0.9707306019838471 = 1.0956952337765171.\n",
      "Evaluation loss in epoch 349 of generation 1: 0.11996894900346941 + 0.9761835224639469 = 1.0961524714674165.\n",
      "Training loss in epoch 350 of generation 1: 0.12496462793684392 + 0.9696454550148571 = 1.0946100829517011.\n",
      "Evaluation loss in epoch 350 of generation 1: 0.11996894679929149 + 0.9971141653545832 = 1.1170831121538747.\n",
      "Training loss in epoch 351 of generation 1: 0.12496463932071138 + 0.9743529970846958 = 1.0993176364054071.\n",
      "Evaluation loss in epoch 351 of generation 1: 0.11996896516744088 + 1.0045105856262113 = 1.1244795507936522.\n",
      "Training loss in epoch 352 of generation 1: 0.12496464097320827 + 0.971249694589197 = 1.0962143355624052.\n",
      "Evaluation loss in epoch 352 of generation 1: 0.11996895855490709 + 0.9919850227461757 = 1.1119539813010828.\n",
      "Training loss in epoch 353 of generation 1: 0.1249646360157176 + 0.9685171198560313 = 1.093481755871749.\n",
      "Evaluation loss in epoch 353 of generation 1: 0.1199689581875441 + 0.9899772385419425 = 1.1099461967294866.\n",
      "Training loss in epoch 354 of generation 1: 0.12496463583210683 + 0.9758732354682274 = 1.1008378713003342.\n",
      "Evaluation loss in epoch 354 of generation 1: 0.11996896957579672 + 0.973981910200075 = 1.0939508797758717.\n",
      "Training loss in epoch 355 of generation 1: 0.12496464703236353 + 0.9659834205530495 = 1.090948067585413.\n",
      "Evaluation loss in epoch 355 of generation 1: 0.11996896626952984 + 1.004034383271251 = 1.124003349540781.\n",
      "Training loss in epoch 356 of generation 1: 0.12496464391098051 + 0.9702463910172984 = 1.0952110349282789.\n",
      "Evaluation loss in epoch 356 of generation 1: 0.11996896626952984 + 0.9758869606100355 = 1.0958559268795653.\n",
      "Training loss in epoch 357 of generation 1: 0.12496463564849607 + 0.9675754272437436 = 1.0925400628922397.\n",
      "Evaluation loss in epoch 357 of generation 1: 0.11996895653441066 + 0.9915664890330451 = 1.1115354455674558.\n",
      "Training loss in epoch 358 of generation 1: 0.12496463693377143 + 0.9678120427651271 = 1.0927766796988985.\n",
      "Evaluation loss in epoch 358 of generation 1: 0.11996895433023273 + 1.0042637000649661 = 1.124232654395199.\n",
      "Training loss in epoch 359 of generation 1: 0.12496463932071138 + 0.9710183993733568 = 1.095983038694068.\n",
      "Evaluation loss in epoch 359 of generation 1: 0.11996895267709928 + 1.0194671143001328 = 1.139436066977232.\n",
      "Training loss in epoch 360 of generation 1: 0.12496463473044224 + 0.9777564517708985 = 1.1027210865013408.\n",
      "Evaluation loss in epoch 360 of generation 1: 0.11996895267709928 + 0.994947362937986 = 1.1149163156150854.\n",
      "Training loss in epoch 361 of generation 1: 0.1249646312418377 + 0.9741599560701989 = 1.0991245873120365.\n",
      "Evaluation loss in epoch 361 of generation 1: 0.11996895414655125 + 0.9761592706289424 = 1.0961282247754935.\n",
      "Training loss in epoch 362 of generation 1: 0.12496463326155612 + 0.9717107044994073 = 1.0966753377609635.\n",
      "Evaluation loss in epoch 362 of generation 1: 0.11996895892227008 + 0.9733540839149698 = 1.09332304283724.\n",
      "Training loss in epoch 363 of generation 1: 0.12496463528127454 + 0.9699773968390699 = 1.0949420321203445.\n",
      "Evaluation loss in epoch 363 of generation 1: 0.11996895892227008 + 0.9812625200245156 = 1.1012314789467856.\n",
      "Training loss in epoch 364 of generation 1: 0.12496463730099296 + 0.974398393010377 = 1.09936303031137.\n",
      "Evaluation loss in epoch 364 of generation 1: 0.11996895965699605 + 0.9862340330526531 = 1.1062029927096493.\n",
      "Training loss in epoch 365 of generation 1: 0.12496463564849607 + 0.9677702514859957 = 1.0927348871344917.\n",
      "Evaluation loss in epoch 365 of generation 1: 0.11996896406535192 + 0.98109654983497 = 1.1010655139003218.\n",
      "Training loss in epoch 366 of generation 1: 0.12496463619932836 + 0.9697769878917719 = 1.0947416240911003.\n",
      "Evaluation loss in epoch 366 of generation 1: 0.11996896626952984 + 0.976480121054157 = 1.096449087323687.\n",
      "Training loss in epoch 367 of generation 1: 0.1249646365665499 + 0.9713029328978975 = 1.0962675694644475.\n",
      "Evaluation loss in epoch 367 of generation 1: 0.11996895598336618 + 0.9735138897535798 = 1.0934828457369459.\n",
      "Training loss in epoch 368 of generation 1: 0.12496463895348985 + 0.9706780511138529 = 1.0956426900673426.\n",
      "Evaluation loss in epoch 368 of generation 1: 0.11996896645321134 + 0.987343506012198 = 1.1073124724654093.\n",
      "Training loss in epoch 369 of generation 1: 0.12496463289433458 + 0.9709586318658323 = 1.0959232647601669.\n",
      "Evaluation loss in epoch 369 of generation 1: 0.11996896645321134 + 0.9928073177719704 = 1.1127762842251818.\n",
      "Training loss in epoch 370 of generation 1: 0.12496463748460372 + 0.9746306826621237 = 1.0995953201467275.\n",
      "Evaluation loss in epoch 370 of generation 1: 0.11996895671809216 + 0.9884649726385696 = 1.1084339293566616.\n",
      "Training loss in epoch 371 of generation 1: 0.12496463858626831 + 0.9750630096329788 = 1.100027648219247.\n",
      "Evaluation loss in epoch 371 of generation 1: 0.1199689642490334 + 1.0098889531633704 = 1.1298579174124037.\n",
      "Training loss in epoch 372 of generation 1: 0.12496463858626831 + 0.9756196543121044 = 1.1005842928983727.\n",
      "Evaluation loss in epoch 372 of generation 1: 0.11996896498375938 + 0.984263403941009 = 1.1042323689247684.\n",
      "Training loss in epoch 373 of generation 1: 0.12496463876987908 + 0.970451545935665 = 1.0954161847055441.\n",
      "Evaluation loss in epoch 373 of generation 1: 0.11996896498375938 + 0.9754997306318239 = 1.0954686956155832.\n",
      "Training loss in epoch 374 of generation 1: 0.12496464262570516 + 0.9688383637825985 = 1.0938030064083035.\n",
      "Evaluation loss in epoch 374 of generation 1: 0.11996896186117399 + 0.984605947885381 = 1.1045749097465551.\n",
      "Training loss in epoch 375 of generation 1: 0.12496464060598673 + 0.9686872565291871 = 1.0936518971351739.\n",
      "Evaluation loss in epoch 375 of generation 1: 0.11996896810634478 + 0.9764006854205727 = 1.0963696535269174.\n",
      "Training loss in epoch 376 of generation 1: 0.1249646413404298 + 0.9649901994155102 = 1.08995484075594.\n",
      "Evaluation loss in epoch 376 of generation 1: 0.11996896351430743 + 1.0198456892063512 = 1.1398146527206585.\n",
      "Training loss in epoch 377 of generation 1: 0.12496464299292669 + 0.97853387150344 = 1.1034985144963667.\n",
      "Evaluation loss in epoch 377 of generation 1: 0.11996896957579672 + 0.9902587679023919 = 1.1102277374781886.\n",
      "Training loss in epoch 378 of generation 1: 0.12496463840265755 + 0.9684226734159908 = 1.0933873118186483.\n",
      "Evaluation loss in epoch 378 of generation 1: 0.11996895745281813 + 0.9922878180152647 = 1.1122567754680828.\n",
      "Training loss in epoch 379 of generation 1: 0.12496463289433458 + 0.9718741694917852 = 1.0968388023861197.\n",
      "Evaluation loss in epoch 379 of generation 1: 0.11996895524864021 + 0.9791831999603883 = 1.0991521552090284.\n",
      "Training loss in epoch 380 of generation 1: 0.12496462151046712 + 0.9703032222214597 = 1.0952678437319268.\n",
      "Evaluation loss in epoch 380 of generation 1: 0.11996894459511356 + 1.05191858229909 = 1.1718875268942035.\n",
      "Training loss in epoch 381 of generation 1: 0.12496461875630563 + 0.9724501212101327 = 1.0974147399664385.\n",
      "Evaluation loss in epoch 381 of generation 1: 0.11996894459511356 + 0.9744006275946995 = 1.094369572189813.\n",
      "Training loss in epoch 382 of generation 1: 0.12496462261213172 + 0.9700341443167221 = 1.094998766928854.\n",
      "Evaluation loss in epoch 382 of generation 1: 0.119968946248247 + 0.9925723435882428 = 1.1125412898364897.\n",
      "Training loss in epoch 383 of generation 1: 0.12496462408101784 + 0.9680650334290279 = 1.0930296575100458.\n",
      "Evaluation loss in epoch 383 of generation 1: 0.1199689523097363 + 0.9899798688609354 = 1.1099488211706716.\n",
      "Training loss in epoch 384 of generation 1: 0.12496462867128698 + 0.9751829750316622 = 1.1001476037029492.\n",
      "Evaluation loss in epoch 384 of generation 1: 0.1199689284311421 + 0.9735162849602603 = 1.0934852133914024.\n",
      "Training loss in epoch 385 of generation 1: 0.12496460975937812 + 0.9714900205169313 = 1.0964546302763094.\n",
      "Evaluation loss in epoch 385 of generation 1: 0.119968931370046 + 0.9854377032428383 = 1.1054066346128844.\n",
      "Training loss in epoch 386 of generation 1: 0.1249646178382518 + 0.9676838516041458 = 1.0926484694423977.\n",
      "Evaluation loss in epoch 386 of generation 1: 0.11996894532983954 + 0.979588145651325 = 1.0995570909811645.\n",
      "Training loss in epoch 387 of generation 1: 0.12496462665156856 + 0.9671911974801529 = 1.0921558241317215.\n",
      "Evaluation loss in epoch 387 of generation 1: 0.11996894826874344 + 0.975052740981656 = 1.0950216892503994.\n",
      "Training loss in epoch 388 of generation 1: 0.12496461618575491 + 0.981826351842928 = 1.106790968028683.\n",
      "Evaluation loss in epoch 388 of generation 1: 0.11996893871730575 + 1.0092882926364892 = 1.129257231353795.\n",
      "Training loss in epoch 389 of generation 1: 0.12496461563492262 + 0.9693376763673943 = 1.0943022920023169.\n",
      "Evaluation loss in epoch 389 of generation 1: 0.11996890749145178 + 1.0191495098942784 = 1.1391184173857303.\n",
      "Training loss in epoch 390 of generation 1: 0.12496474416245852 + 0.9644000509127681 = 1.0893647950752268.\n",
      "Evaluation loss in epoch 390 of generation 1: 0.11996894716665447 + 0.9779276149849678 = 1.0978965621516223.\n",
      "Training loss in epoch 391 of generation 1: 0.12496462004158099 + 0.9684070385920804 = 1.0933716586336615.\n",
      "Evaluation loss in epoch 391 of generation 1: 0.11996894202357264 + 0.9763026979561028 = 1.0962716399796755.\n",
      "Training loss in epoch 392 of generation 1: 0.12496461490047955 + 0.968852374018455 = 1.0938169889189346.\n",
      "Evaluation loss in epoch 392 of generation 1: 0.11996893210477197 + 0.9943304693753987 = 1.1142994014801708.\n",
      "Training loss in epoch 393 of generation 1: 0.12496461875630563 + 0.9696465273017281 = 1.0946111460580337.\n",
      "Evaluation loss in epoch 393 of generation 1: 0.11996895084028435 + 0.9822372515873843 = 1.1022062024276686.\n",
      "Training loss in epoch 394 of generation 1: 0.1249646248154609 + 0.9743391704596048 = 1.0993037952750657.\n",
      "Evaluation loss in epoch 394 of generation 1: 0.11996893210477197 + 0.983164198042248 = 1.10313313014702.\n",
      "Training loss in epoch 395 of generation 1: 0.12496461912352716 + 0.9750913047863972 = 1.1000559239099243.\n",
      "Evaluation loss in epoch 395 of generation 1: 0.11996894386038758 + 0.9978773413894724 = 1.11784628524986.\n",
      "Training loss in epoch 396 of generation 1: 0.12496461600214415 + 0.969398212102362 = 1.0943628281045061.\n",
      "Evaluation loss in epoch 396 of generation 1: 0.11996894239093563 + 0.9744062320844418 = 1.0943751744753774.\n",
      "Training loss in epoch 397 of generation 1: 0.12496461636936568 + 0.9722144604647301 = 1.0971790768340959.\n",
      "Evaluation loss in epoch 397 of generation 1: 0.1199689372478538 + 1.0027722856847825 = 1.1227412229326363.\n",
      "Training loss in epoch 398 of generation 1: 0.12496462665156856 + 0.97183523372728 = 1.0967998603788485.\n",
      "Evaluation loss in epoch 398 of generation 1: 0.11996894698297297 + 0.9778714701648118 = 1.097840417147785.\n",
      "Training loss in epoch 399 of generation 1: 0.12496462242852095 + 0.980383892448581 = 1.1053485148771018.\n",
      "Evaluation loss in epoch 399 of generation 1: 0.11996893210477197 + 0.9735535678951692 = 1.0935224999999411.\n",
      "Training loss in epoch 400 of generation 1: 0.12496461526770108 + 0.9687277199352619 = 1.0936923352029628.\n",
      "Evaluation loss in epoch 400 of generation 1: 0.11996892971691255 + 0.9776157061748769 = 1.0975846358917896.\n",
      "Training loss in epoch 401 of generation 1: 0.12496461967435946 + 0.9754926705755176 = 1.100457290249877.\n",
      "Evaluation loss in epoch 401 of generation 1: 0.11996889867474007 + 0.9809549960590475 = 1.1009238947337876.\n",
      "Training loss in epoch 402 of generation 1: 0.12496460516910898 + 0.9699422522696524 = 1.0949068574387613.\n",
      "Evaluation loss in epoch 402 of generation 1: 0.11996885440750005 + 0.973994253596466 = 1.093963108003966.\n",
      "Training loss in epoch 403 of generation 1: 0.12496465272429727 + 0.9694116700370357 = 1.094376322761333.\n",
      "Evaluation loss in epoch 403 of generation 1: 0.11996890785881477 + 0.9733196017554802 = 1.093288509614295.\n",
      "Training loss in epoch 404 of generation 1: 0.12496461214631807 + 0.9669683204506928 = 1.0919329325970109.\n",
      "Evaluation loss in epoch 404 of generation 1: 0.11996893026795703 + 0.989944654914374 = 1.109913585182331.\n",
      "Training loss in epoch 405 of generation 1: 0.1249646136152042 + 0.9691811092642577 = 1.094145722879462.\n",
      "Evaluation loss in epoch 405 of generation 1: 0.11996889151116182 + 0.9866833620703276 = 1.1066522535814893.\n",
      "Training loss in epoch 406 of generation 1: 0.12496465290790804 + 0.9686285113664054 = 1.0935931642743133.\n",
      "Evaluation loss in epoch 406 of generation 1: 0.11996890694040731 + 0.9943573779795313 = 1.1143262849199387.\n",
      "Training loss in epoch 407 of generation 1: 0.12496461288076113 + 0.9689435859712491 = 1.0939081988520103.\n",
      "Evaluation loss in epoch 407 of generation 1: 0.1199689341252684 + 0.9732486771875978 = 1.0932176113128662.\n",
      "Training loss in epoch 408 of generation 1: 0.12496461324798266 + 0.9704872721839878 = 1.0954518854319706.\n",
      "Evaluation loss in epoch 408 of generation 1: 0.1199689190633859 + 0.9746256359552933 = 1.0945945550186793.\n",
      "Training loss in epoch 409 of generation 1: 0.12496461122826424 + 0.9658569854454302 = 1.0908215966736945.\n",
      "Evaluation loss in epoch 409 of generation 1: 0.11996886579575267 + 1.000504706783912 = 1.1204735725796646.\n",
      "Training loss in epoch 410 of generation 1: 0.12496463289433458 + 0.9711691438118953 = 1.09613377670623.\n",
      "Evaluation loss in epoch 410 of generation 1: 0.11996826534094907 + 0.9950485552771616 = 1.1150168206181108.\n",
      "Training loss in epoch 411 of generation 1: 0.12496461545131185 + 0.9704825651384015 = 1.0954471805897135.\n",
      "Evaluation loss in epoch 411 of generation 1: 0.1199689130018966 + 1.0223879424743183 = 1.142356855476215.\n",
      "Training loss in epoch 412 of generation 1: 0.12496461012659965 + 0.9715854202640667 = 1.0965500303906663.\n",
      "Evaluation loss in epoch 412 of generation 1: 0.11996887553087185 + 0.9735796638923727 = 1.0935485394232445.\n",
      "Training loss in epoch 413 of generation 1: 0.12496486176515388 + 0.9699356995686504 = 1.0949005613338043.\n",
      "Evaluation loss in epoch 413 of generation 1: 0.11996892879850508 + 0.9749687294746952 = 1.0949376582732002.\n",
      "Training loss in epoch 414 of generation 1: 0.12496458919497237 + 0.9789717993370882 = 1.1039363885320606.\n",
      "Evaluation loss in epoch 414 of generation 1: 0.1199689071240888 + 0.9732574336517757 = 1.0932263407758644.\n",
      "Training loss in epoch 415 of generation 1: 0.12496458846052931 + 0.9717868030828652 = 1.0967513915433946.\n",
      "Evaluation loss in epoch 415 of generation 1: 0.11996893026795703 + 0.9813261810914027 = 1.1012951113593596.\n",
      "Training loss in epoch 416 of generation 1: 0.12496460067064523 + 0.9696581682242656 = 1.0946227688949108.\n",
      "Evaluation loss in epoch 416 of generation 1: 0.11996892439014922 + 0.9872844105325237 = 1.107253334922673.\n",
      "Training loss in epoch 417 of generation 1: 0.12496460223133674 + 0.9720903880604423 = 1.097054990291779.\n",
      "Evaluation loss in epoch 417 of generation 1: 0.11996892732905313 + 0.9768241109392493 = 1.0967930382683024.\n",
      "Training loss in epoch 418 of generation 1: 0.12496461471686879 + 0.968756493945557 = 1.0937211086624257.\n",
      "Evaluation loss in epoch 418 of generation 1: 0.11996894569720253 + 0.9735774700006095 = 1.0935464156978119.\n",
      "Training loss in epoch 419 of generation 1: 0.12496461710380874 + 0.9721924697705586 = 1.0971570868743674.\n",
      "Evaluation loss in epoch 419 of generation 1: 0.11996894422775058 + 0.9734417043959231 = 1.0934106486236737.\n",
      "Training loss in epoch 420 of generation 1: 0.12496461747103027 + 0.9736862270750355 = 1.0986508445460659.\n",
      "Evaluation loss in epoch 420 of generation 1: 0.11996894422775058 + 0.9897249233557374 = 1.109693867583488.\n",
      "Training loss in epoch 421 of generation 1: 0.12496461820547333 + 0.9672566627969548 = 1.092221281002428.\n",
      "Evaluation loss in epoch 421 of generation 1: 0.11996892732905313 + 0.9777361189455758 = 1.097705046274629.\n",
      "Training loss in epoch 422 of generation 1: 0.12496461618575491 + 0.9724462624462834 = 1.0974108786320382.\n",
      "Evaluation loss in epoch 422 of generation 1: 0.1199689221859713 + 0.9821633601960122 = 1.1021322823819835.\n",
      "Training loss in epoch 423 of generation 1: 0.12496459929356449 + 0.981694616259728 = 1.1066592155532926.\n",
      "Evaluation loss in epoch 423 of generation 1: 0.11996892328806026 + 1.0134324513157638 = 1.133401374603824.\n",
      "Training loss in epoch 424 of generation 1: 0.12496459929356449 + 0.9688250130766118 = 1.0937896123701762.\n",
      "Evaluation loss in epoch 424 of generation 1: 0.1199689221859713 + 0.9735780283923509 = 1.0935469505783222.\n",
      "Training loss in epoch 425 of generation 1: 0.12496459910995372 + 0.9741428038869218 = 1.0991074029968755.\n",
      "Evaluation loss in epoch 425 of generation 1: 0.11996892732905313 + 0.9859498836593745 = 1.1059188109884277.\n",
      "Training loss in epoch 426 of generation 1: 0.12496459984439678 + 0.9721261980352743 = 1.0970907978796711.\n",
      "Evaluation loss in epoch 426 of generation 1: 0.11996892053283785 + 0.9734226338484989 = 1.0933915543813368.\n",
      "Training loss in epoch 427 of generation 1: 0.1249646018641152 + 0.9689622246672843 = 1.0939268265313995.\n",
      "Evaluation loss in epoch 427 of generation 1: 0.11996892053283785 + 1.0094644681698368 = 1.1294333887026746.\n",
      "Training loss in epoch 428 of generation 1: 0.1249645992017591 + 0.9723278467224614 = 1.0972924459242206.\n",
      "Evaluation loss in epoch 428 of generation 1: 0.11996892347174176 + 0.9747666621979654 = 1.094735585669707.\n",
      "Training loss in epoch 429 of generation 1: 0.12496459910995372 + 0.970076978505002 = 1.0950415776149558.\n",
      "Evaluation loss in epoch 429 of generation 1: 0.11996892732905313 + 0.9811739723193811 = 1.1011428996484343.\n",
      "Training loss in epoch 430 of generation 1: 0.12496460057883985 + 0.9696116500695853 = 1.0945762506484251.\n",
      "Evaluation loss in epoch 430 of generation 1: 0.11996892732905313 + 0.9800374643828358 = 1.100006391711889.\n",
      "Training loss in epoch 431 of generation 1: 0.12496460122147753 + 0.9678824655725995 = 1.092847066794077.\n",
      "Evaluation loss in epoch 431 of generation 1: 0.11996892347174176 + 0.9879875859053366 = 1.1079565093770785.\n",
      "Training loss in epoch 432 of generation 1: 0.12496460223133674 + 0.9678109146605834 = 1.0927755168919202.\n",
      "Evaluation loss in epoch 432 of generation 1: 0.11996892347174176 + 0.9782277373209572 = 1.098196660792699.\n",
      "Training loss in epoch 433 of generation 1: 0.12496460278216903 + 0.9766043779434128 = 1.1015689807255817.\n",
      "Evaluation loss in epoch 433 of generation 1: 0.11996894661560999 + 0.9817890555171643 = 1.1017580021327744.\n",
      "Training loss in epoch 434 of generation 1: 0.12496460223133674 + 0.9718806531551392 = 1.0968452553864758.\n",
      "Evaluation loss in epoch 434 of generation 1: 0.11996892732905313 + 0.983487180642244 = 1.1034561079712972.\n",
      "Training loss in epoch 435 of generation 1: 0.12496460516910898 + 0.9758454235783438 = 1.100810028747453.\n",
      "Evaluation loss in epoch 435 of generation 1: 0.11996894808506194 + 0.9782184459762691 = 1.098187394061331.\n",
      "Training loss in epoch 436 of generation 1: 0.12496460792327047 + 0.9766524692751327 = 1.101617077198403.\n",
      "Evaluation loss in epoch 436 of generation 1: 0.11996892714537163 + 1.0008308898502578 = 1.1207998169956295.\n",
      "Training loss in epoch 437 of generation 1: 0.12496459910995372 + 0.9729531926745215 = 1.0979177917844753.\n",
      "Evaluation loss in epoch 437 of generation 1: 0.11996891153244466 + 0.9874481045409969 = 1.1074170160734416.\n",
      "Training loss in epoch 438 of generation 1: 0.12496460094606138 + 0.9704984415940794 = 1.0954630425401408.\n",
      "Evaluation loss in epoch 438 of generation 1: 0.11996892622696416 + 0.9879565158132779 = 1.107925442040242.\n",
      "Training loss in epoch 439 of generation 1: 0.12496460103786676 + 0.9674753902231128 = 1.0924399912609795.\n",
      "Evaluation loss in epoch 439 of generation 1: 0.11996890602199983 + 0.9741292594944935 = 1.0940981655164934.\n",
      "Training loss in epoch 440 of generation 1: 0.12496460232314212 + 0.9710146757470308 = 1.095979278070173.\n",
      "Evaluation loss in epoch 440 of generation 1: 0.11996887461246437 + 1.0181860402111647 = 1.1381549148236292.\n",
      "Training loss in epoch 441 of generation 1: 0.12496462903850851 + 0.966898304524677 = 1.0918629335631853.\n",
      "Evaluation loss in epoch 441 of generation 1: 0.1199689071240888 + 0.9736212743632783 = 1.093590181487367.\n",
      "Training loss in epoch 442 of generation 1: 0.12496459360163074 + 0.9690683002787734 = 1.0940328938804043.\n",
      "Evaluation loss in epoch 442 of generation 1: 0.11996892806377911 + 0.975638881853806 = 1.0956078099175852.\n",
      "Training loss in epoch 443 of generation 1: 0.12496461049382118 + 0.9739590800171134 = 1.0989236905109345.\n",
      "Evaluation loss in epoch 443 of generation 1: 0.11996890932826673 + 0.9794650232112279 = 1.0994339325394946.\n",
      "Training loss in epoch 444 of generation 1: 0.12496459268357692 + 0.9668552132813252 = 1.091819805964902.\n",
      "Evaluation loss in epoch 444 of generation 1: 0.11996891447134855 + 0.9740470348411054 = 1.094015949312454.\n",
      "Training loss in epoch 445 of generation 1: 0.12496460131328291 + 0.9709509114003613 = 1.0959155127136442.\n",
      "Evaluation loss in epoch 445 of generation 1: 0.1199689284311421 + 0.9766256056140494 = 1.0965945340451915.\n",
      "Training loss in epoch 446 of generation 1: 0.12496460204772597 + 0.9749578917352429 = 1.0999224937829688.\n",
      "Evaluation loss in epoch 446 of generation 1: 0.11996891796129694 + 0.981825680137599 = 1.101794598098896.\n",
      "Training loss in epoch 447 of generation 1: 0.12496459745745683 + 0.9679267363315093 = 1.092891333788966.\n",
      "Evaluation loss in epoch 447 of generation 1: 0.11996886910201957 + 0.9738915080465779 = 1.0938603771485973.\n",
      "Training loss in epoch 448 of generation 1: 0.12496470340086857 + 0.9689959782015419 = 1.0939606816024106.\n",
      "Evaluation loss in epoch 448 of generation 1: 0.11996890877722224 + 0.999002970973222 = 1.1189718797504442.\n",
      "Training loss in epoch 449 of generation 1: 0.12496458864414008 + 0.9718352513539135 = 1.0967998399980536.\n",
      "Evaluation loss in epoch 449 of generation 1: 0.1199689073077703 + 1.047713769051621 = 1.1676826763593913.\n",
      "Training loss in epoch 450 of generation 1: 0.12496458735886472 + 0.9740569754017744 = 1.0990215627606392.\n",
      "Evaluation loss in epoch 450 of generation 1: 0.11996890565463685 + 0.9733082458308004 = 1.0932771514854371.\n",
      "Training loss in epoch 451 of generation 1: 0.12496460002800755 + 0.973050761966463 = 1.0980153619944706.\n",
      "Evaluation loss in epoch 451 of generation 1: 0.11996892971691255 + 0.9781317791696322 = 1.0981007088865449.\n",
      "Training loss in epoch 452 of generation 1: 0.12496460663799511 + 0.9671686456714816 = 1.0921332523094767.\n",
      "Evaluation loss in epoch 452 of generation 1: 0.11996892806377911 + 0.9760718411767427 = 1.0960407692405219.\n",
      "Training loss in epoch 453 of generation 1: 0.1249646035166121 + 0.9662264139779861 = 1.0911910174945982.\n",
      "Evaluation loss in epoch 453 of generation 1: 0.11996892292069727 + 0.9894453762859703 = 1.1094142992066676.\n",
      "Training loss in epoch 454 of generation 1: 0.12496460461827669 + 0.9667854632236978 = 1.0917500678419745.\n",
      "Evaluation loss in epoch 454 of generation 1: 0.11996892292069727 + 0.9935855365864852 = 1.1135544595071825.\n",
      "Training loss in epoch 455 of generation 1: 0.12496460480188745 + 0.9658987113591291 = 1.0908633161610166.\n",
      "Evaluation loss in epoch 455 of generation 1: 0.11996892585960117 + 0.9732522229751563 = 1.0932211488347576.\n",
      "Training loss in epoch 456 of generation 1: 0.12496460571994128 + 0.9666598088290538 = 1.091624414548995.\n",
      "Evaluation loss in epoch 456 of generation 1: 0.11996892898218657 + 0.9842076059115135 = 1.1041765348937.\n",
      "Training loss in epoch 457 of generation 1: 0.12496460590355205 + 0.9671963136105249 = 1.092160919514077.\n",
      "Evaluation loss in epoch 457 of generation 1: 0.11996892585960117 + 0.9772103593235574 = 1.0971792851831585.\n",
      "Training loss in epoch 458 of generation 1: 0.12496460516910898 + 0.9721400643202907 = 1.0971046694893998.\n",
      "Evaluation loss in epoch 458 of generation 1: 0.11996892585960117 + 0.9767863886382069 = 1.0967553144978082.\n",
      "Training loss in epoch 459 of generation 1: 0.12496460259855827 + 0.9710302885376494 = 1.0959948911362076.\n",
      "Evaluation loss in epoch 459 of generation 1: 0.11996892585960117 + 0.9854478586252738 = 1.105416784484875.\n",
      "Training loss in epoch 460 of generation 1: 0.12496460608716281 + 0.9686052280524441 = 1.093569834139607.\n",
      "Evaluation loss in epoch 460 of generation 1: 0.119968931370046 + 0.9734484697527069 = 1.0934174011227529.\n",
      "Training loss in epoch 461 of generation 1: 0.1249646071888274 + 0.9663766986551677 = 1.0913413058439951.\n",
      "Evaluation loss in epoch 461 of generation 1: 0.11996892585960117 + 0.9884784078377604 = 1.1084473336973617.\n",
      "Training loss in epoch 462 of generation 1: 0.12496460498549822 + 0.9714149281204688 = 1.096379533105967.\n",
      "Evaluation loss in epoch 462 of generation 1: 0.11996893045163852 + 1.013374878188311 = 1.1333438086399497.\n",
      "Training loss in epoch 463 of generation 1: 0.124964608290492 + 0.9719714229732098 = 1.0969360312637018.\n",
      "Evaluation loss in epoch 463 of generation 1: 0.1199689041851849 + 0.9750293502454949 = 1.0949982544306798.\n",
      "Training loss in epoch 464 of generation 1: 0.12496458735886472 + 0.9711602129842577 = 1.0961248003431223.\n",
      "Evaluation loss in epoch 464 of generation 1: 0.11996890565463685 + 0.9758487710232727 = 1.0958176766779095.\n",
      "Training loss in epoch 465 of generation 1: 0.12496458919497237 + 0.9767015021628584 = 1.101666091357831.\n",
      "Evaluation loss in epoch 465 of generation 1: 0.11996890565463685 + 0.9735228651660984 = 1.0934917708207352.\n",
      "Training loss in epoch 466 of generation 1: 0.12496458772608625 + 0.9732585520665371 = 1.0982231397926234.\n",
      "Evaluation loss in epoch 466 of generation 1: 0.1199689071240888 + 0.9736700704542257 = 1.0936389775783144.\n",
      "Training loss in epoch 467 of generation 1: 0.12496459029663697 + 0.9690938559595695 = 1.0940584462562064.\n",
      "Evaluation loss in epoch 467 of generation 1: 0.11996891079771868 + 1.0074135878787753 = 1.127382498676494.\n",
      "Training loss in epoch 468 of generation 1: 0.12496459470329534 + 0.9697051549536199 = 1.094669749656915.\n",
      "Evaluation loss in epoch 468 of generation 1: 0.11996891887970441 + 1.0026385626183085 = 1.122607481498013.\n",
      "Training loss in epoch 469 of generation 1: 0.12496459929356449 + 0.99062845217617 = 1.1155930514697345.\n",
      "Evaluation loss in epoch 469 of generation 1: 0.11996890565463685 + 0.9811500173136707 = 1.1011189229683076.\n",
      "Training loss in epoch 470 of generation 1: 0.12496459672301377 + 0.9695989853334188 = 1.0945635820564326.\n",
      "Evaluation loss in epoch 470 of generation 1: 0.11996890436886638 + 0.9732689938302768 = 1.093237898199143.\n",
      "Training loss in epoch 471 of generation 1: 0.12496459910995372 + 0.9730674690772443 = 1.098032068187198.\n",
      "Evaluation loss in epoch 471 of generation 1: 0.11996886616311565 + 1.0234689977025766 = 1.1434378638656921.\n",
      "Training loss in epoch 472 of generation 1: 0.12496458901136161 + 0.9736018395671764 = 1.098566428578538.\n",
      "Evaluation loss in epoch 472 of generation 1: 0.11996863215289241 + 0.9758101068035296 = 1.095778738956422.\n",
      "Training loss in epoch 473 of generation 1: 0.12497447736914176 + 0.973948479065952 = 1.0989229564350937.\n",
      "Evaluation loss in epoch 473 of generation 1: 0.11996891336925959 + 0.9748108882933442 = 1.0947798016626038.\n",
      "Training loss in epoch 474 of generation 1: 0.12496459194913385 + 0.9698743779796061 = 1.09483896992874.\n",
      "Evaluation loss in epoch 474 of generation 1: 0.11996891336925959 + 0.9775701090808274 = 1.0975390224500872.\n",
      "Training loss in epoch 475 of generation 1: 0.12496459341801998 + 0.9703367701116598 = 1.0953013635296798.\n",
      "Evaluation loss in epoch 475 of generation 1: 0.11996891336925959 + 0.9791104650277018 = 1.0990793783969615.\n",
      "Training loss in epoch 476 of generation 1: 0.12496459103108003 + 0.9749907712822563 = 1.0999553623133362.\n",
      "Evaluation loss in epoch 476 of generation 1: 0.11996891336925959 + 0.9918805079761381 = 1.1118494213453978.\n",
      "Training loss in epoch 477 of generation 1: 0.12496459084746926 + 0.9699486786464475 = 1.0949132694939168.\n",
      "Evaluation loss in epoch 477 of generation 1: 0.11996891336925959 + 0.9856483712218392 = 1.1056172845910988.\n",
      "Training loss in epoch 478 of generation 1: 0.12496459249996615 + 0.9669708381215104 = 1.0919354306214766.\n",
      "Evaluation loss in epoch 478 of generation 1: 0.11996891336925959 + 0.9997661073329085 = 1.1197350207021681.\n",
      "Training loss in epoch 479 of generation 1: 0.12496459213274462 + 0.9721416624683943 = 1.097106254601139.\n",
      "Evaluation loss in epoch 479 of generation 1: 0.11996891336925959 + 0.9804330379092271 = 1.1004019512784866.\n",
      "Training loss in epoch 480 of generation 1: 0.12496459075566388 + 0.9702095190377983 = 1.0951741097934622.\n",
      "Evaluation loss in epoch 480 of generation 1: 0.11996891336925959 + 1.0013389866598215 = 1.121307900029081.\n",
      "Training loss in epoch 481 of generation 1: 0.12496459121469079 + 0.9689234446047084 = 1.0938880358193992.\n",
      "Evaluation loss in epoch 481 of generation 1: 0.11996891336925959 + 0.9738147012625343 = 1.0937836146317939.\n",
      "Training loss in epoch 482 of generation 1: 0.12496459121469079 + 0.9685200884748891 = 1.0934846796895799.\n",
      "Evaluation loss in epoch 482 of generation 1: 0.11996891336925959 + 0.9970274280142527 = 1.1169963413835122.\n",
      "Training loss in epoch 483 of generation 1: 0.12496459176552309 + 0.9690000837382602 = 1.0939646755037833.\n",
      "Evaluation loss in epoch 483 of generation 1: 0.11996891336925959 + 0.9732618934384479 = 1.0932308068077075.\n",
      "Training loss in epoch 484 of generation 1: 0.1249645906638585 + 0.9684425885740683 = 1.0934071792379267.\n",
      "Evaluation loss in epoch 484 of generation 1: 0.11996891336925959 + 0.9752549023033107 = 1.0952238156725702.\n",
      "Training loss in epoch 485 of generation 1: 0.12496459213274462 + 0.96735175554689 = 1.0923163476796347.\n",
      "Evaluation loss in epoch 485 of generation 1: 0.11996891336925959 + 0.9787685176808221 = 1.0987374310500817.\n",
      "Training loss in epoch 486 of generation 1: 0.12496459139830156 + 0.9700321848226319 = 1.0949967762209334.\n",
      "Evaluation loss in epoch 486 of generation 1: 0.11996891336925959 + 0.9777011327640669 = 1.0976700461333264.\n",
      "Training loss in epoch 487 of generation 1: 0.12496459084746926 + 0.9677612809984327 = 1.0927258718459019.\n",
      "Evaluation loss in epoch 487 of generation 1: 0.11996891336925959 + 0.9981002822311708 = 1.1180691956004303.\n",
      "Training loss in epoch 488 of generation 1: 0.12496459103108003 + 0.968578091850178 = 1.093542682881258.\n",
      "Evaluation loss in epoch 488 of generation 1: 0.11996891336925959 + 0.9740882368043685 = 1.094057150173628.\n",
      "Training loss in epoch 489 of generation 1: 0.12496458846052931 + 0.96767614435865 = 1.0926407328191794.\n",
      "Evaluation loss in epoch 489 of generation 1: 0.11996891171612614 + 0.9812167671685593 = 1.1011856788846854.\n",
      "Training loss in epoch 490 of generation 1: 0.12496459029663697 + 0.9708997589099577 = 1.0958643492065947.\n",
      "Evaluation loss in epoch 490 of generation 1: 0.11996890877722224 + 0.9963377289852854 = 1.1163066377625075.\n",
      "Training loss in epoch 491 of generation 1: 0.12496458846052931 + 0.9674113688213715 = 1.0923759572819007.\n",
      "Evaluation loss in epoch 491 of generation 1: 0.11996890877722224 + 0.9738096919008322 = 1.0937786006780545.\n",
      "Training loss in epoch 492 of generation 1: 0.12496458919497237 + 0.9799331853056487 = 1.104897774500621.\n",
      "Evaluation loss in epoch 492 of generation 1: 0.11996890877722224 + 0.9836649035084964 = 1.1036338122857186.\n",
      "Training loss in epoch 493 of generation 1: 0.1249645895621939 + 0.9661673060003316 = 1.0911318955625255.\n",
      "Evaluation loss in epoch 493 of generation 1: 0.11996890877722224 + 1.0108551090046511 = 1.1308240177818734.\n",
      "Training loss in epoch 494 of generation 1: 0.12496458699164319 + 0.967489985075647 = 1.09245457206729.\n",
      "Evaluation loss in epoch 494 of generation 1: 0.11996890877722224 + 0.9754436592842655 = 1.0954125680614877.\n",
      "Training loss in epoch 495 of generation 1: 0.12496458901136161 + 0.9686694888826237 = 1.0936340778939853.\n",
      "Evaluation loss in epoch 495 of generation 1: 0.11996890877722224 + 0.9777708303395699 = 1.0977397391167922.\n",
      "Training loss in epoch 496 of generation 1: 0.12496458937858314 + 0.9698143005371094 = 1.0947788899156925.\n",
      "Evaluation loss in epoch 496 of generation 1: 0.11996890877722224 + 0.9849212922741342 = 1.1048902010513564.\n",
      "Training loss in epoch 497 of generation 1: 0.12496458827691855 + 0.9703512107311508 = 1.0953157990080695.\n",
      "Evaluation loss in epoch 497 of generation 1: 0.11996890877722224 + 0.9754211061357166 = 1.0953900149129387.\n",
      "Training loss in epoch 498 of generation 1: 0.12496458882775084 + 0.9736091267112406 = 1.0985737155389914.\n",
      "Evaluation loss in epoch 498 of generation 1: 0.11996890877722224 + 0.985300005308836 = 1.1052689140860583.\n",
      "Training loss in epoch 499 of generation 1: 0.12496458992941543 + 0.969567974209556 = 1.0945325641389714.\n",
      "Evaluation loss in epoch 499 of generation 1: 0.11996890877722224 + 1.011471314863725 = 1.1314402236409473.\n",
      "Training loss in epoch 500 of generation 1: 0.12496458882775084 + 0.9684055212327137 = 1.0933701100604645.\n",
      "Evaluation loss in epoch 500 of generation 1: 0.11996890877722224 + 0.9881131637996444 = 1.1080820725768668.\n",
      "Training loss in epoch 501 of generation 1: 0.12496458846052931 + 0.9736666159763858 = 1.098631204436915.\n",
      "Evaluation loss in epoch 501 of generation 1: 0.11996890877722224 + 0.9929094020684806 = 1.1128783108457028.\n",
      "Training loss in epoch 502 of generation 1: 0.12496458937858314 + 0.9714581412813689 = 1.0964227306599519.\n",
      "Evaluation loss in epoch 502 of generation 1: 0.11996890877722224 + 0.9849241503581795 = 1.1048930591354018.\n",
      "Training loss in epoch 503 of generation 1: 0.12496459029663697 + 0.9722660110232736 = 1.0972306013199105.\n",
      "Evaluation loss in epoch 503 of generation 1: 0.11996890877722224 + 0.9941646740506352 = 1.1141335828278576.\n",
      "Training loss in epoch 504 of generation 1: 0.12496459048024773 + 0.9785853177710685 = 1.1035499082513163.\n",
      "Evaluation loss in epoch 504 of generation 1: 0.11996890877722224 + 0.9736925956831874 = 1.0936615044604097.\n",
      "Training loss in epoch 505 of generation 1: 0.12496458901136161 + 0.9788009766941121 = 1.103765565705474.\n",
      "Evaluation loss in epoch 505 of generation 1: 0.11996890877722224 + 0.9758216478791553 = 1.0957905566563775.\n",
      "Training loss in epoch 506 of generation 1: 0.12496459103108003 + 0.971395469785976 = 1.096360060817056.\n",
      "Evaluation loss in epoch 506 of generation 1: 0.11996890877722224 + 1.0033403978509419 = 1.123309306628164.\n",
      "Training loss in epoch 507 of generation 1: 0.1249645895621939 + 0.970357954387349 = 1.0953225439495429.\n",
      "Evaluation loss in epoch 507 of generation 1: 0.11996890877722224 + 0.9748370195573944 = 1.0948059283346168.\n",
      "Training loss in epoch 508 of generation 1: 0.12496458625720012 + 0.9686173037652746 = 1.0935818900224747.\n",
      "Evaluation loss in epoch 508 of generation 1: 0.11996890877722224 + 1.0103596039288585 = 1.1303285127060807.\n",
      "Training loss in epoch 509 of generation 1: 0.12496458919497237 + 0.9689053112055 = 1.0938699004004724.\n",
      "Evaluation loss in epoch 509 of generation 1: 0.11996890877722224 + 0.9959229835192852 = 1.1158918922965075.\n",
      "Training loss in epoch 510 of generation 1: 0.12496458937858314 + 0.966991107281144 = 1.091955696659727.\n",
      "Evaluation loss in epoch 510 of generation 1: 0.11996890877722224 + 0.9876127345903629 = 1.1075816433675851.\n",
      "Training loss in epoch 511 of generation 1: 0.12496458919497237 + 0.9681455327953152 = 1.0931101219902877.\n",
      "Evaluation loss in epoch 511 of generation 1: 0.11996890877722224 + 0.9751286014386429 = 1.095097510215865.\n",
      "Training loss in epoch 512 of generation 1: 0.12496458901136161 + 0.979041213019793 = 1.1040058020311545.\n",
      "Evaluation loss in epoch 512 of generation 1: 0.11996890877722224 + 0.9761862894419713 = 1.0961551982191935.\n",
      "Training loss in epoch 513 of generation 1: 0.1249645901130262 + 0.9728157401865246 = 1.0977803302995508.\n",
      "Evaluation loss in epoch 513 of generation 1: 0.11996890877722224 + 1.0099083734403589 = 1.129877282217581.\n",
      "Training loss in epoch 514 of generation 1: 0.12496458919497237 + 0.9681171641975905 = 1.093081753392563.\n",
      "Evaluation loss in epoch 514 of generation 1: 0.11996890877722224 + 0.9927545203633595 = 1.1127234291405816.\n",
      "Training loss in epoch 515 of generation 1: 0.12496458937858314 + 0.9696966119119191 = 1.0946612012905022.\n",
      "Evaluation loss in epoch 515 of generation 1: 0.11996890877722224 + 0.9822956285050544 = 1.1022645372822766.\n",
      "Training loss in epoch 516 of generation 1: 0.1249645901130262 + 0.9693762772258631 = 1.0943408673388892.\n",
      "Evaluation loss in epoch 516 of generation 1: 0.11996890877722224 + 0.9991636540746468 = 1.1191325628518691.\n",
      "Training loss in epoch 517 of generation 1: 0.12496458974580467 + 0.9704398007222125 = 1.0954043904680173.\n",
      "Evaluation loss in epoch 517 of generation 1: 0.11996890877722224 + 0.9797269500826467 = 1.0996958588598689.\n",
      "Training loss in epoch 518 of generation 1: 0.12496458974580467 + 0.9682567362882653 = 1.09322132603407.\n",
      "Evaluation loss in epoch 518 of generation 1: 0.11996891024667419 + 0.9805146057175929 = 1.100483515964267.\n",
      "Training loss in epoch 519 of generation 1: 0.1249645906638585 + 0.9666557356078302 = 1.0916203262716888.\n",
      "Evaluation loss in epoch 519 of generation 1: 0.11996891024667419 + 0.9744616333619105 = 1.0944305436085846.\n",
      "Training loss in epoch 520 of generation 1: 0.12496459176552309 + 0.973892076776539 = 1.0988566685420622.\n",
      "Evaluation loss in epoch 520 of generation 1: 0.11996891098140017 + 0.9889741465564134 = 1.1089430575378136.\n",
      "Training loss in epoch 521 of generation 1: 0.12496459507051688 + 0.9673855413966423 = 1.0923501364671593.\n",
      "Evaluation loss in epoch 521 of generation 1: 0.11996891465503005 + 0.9739436985715328 = 1.0939126132265629.\n",
      "Training loss in epoch 522 of generation 1: 0.12496459580495994 + 0.9696652423798419 = 1.0946298381848019.\n",
      "Evaluation loss in epoch 522 of generation 1: 0.11996892126756382 + 0.9828126375407027 = 1.1027815588082666.\n",
      "Training loss in epoch 523 of generation 1: 0.12496459984439678 + 0.9682221689910947 = 1.0931867688354915.\n",
      "Evaluation loss in epoch 523 of generation 1: 0.11996890198100697 + 0.9835468110024286 = 1.1035157129834356.\n",
      "Training loss in epoch 524 of generation 1: 0.12496459396885227 + 0.9728901128287861 = 1.0978547067976383.\n",
      "Evaluation loss in epoch 524 of generation 1: 0.11996890547095536 + 0.9816220464250891 = 1.1015909518960445.\n",
      "Training loss in epoch 525 of generation 1: 0.12496459635579224 + 0.96869217142216 = 1.0936567677779523.\n",
      "Evaluation loss in epoch 525 of generation 1: 0.11996889849105859 + 0.9844550190060091 = 1.1044239174970676.\n",
      "Training loss in epoch 526 of generation 1: 0.12496458570636783 + 0.969337967206847 = 1.0943025529132149.\n",
      "Evaluation loss in epoch 526 of generation 1: 0.11996887424510139 + 0.984756578466007 = 1.1047254527111083.\n",
      "Training loss in epoch 527 of generation 1: 0.12496458405387094 + 0.9688756558635302 = 1.0938402399174012.\n",
      "Evaluation loss in epoch 527 of generation 1: 0.11996863417338884 + 0.9817104104487298 = 1.1016790446221185.\n",
      "Training loss in epoch 528 of generation 1: 0.12496466080317095 + 0.9684732691985531 = 1.093437930001724.\n",
      "Evaluation loss in epoch 528 of generation 1: 0.11996891483871154 + 0.9954483108638064 = 1.115417225702518.\n",
      "Training loss in epoch 529 of generation 1: 0.12496459121469079 + 0.9720625820461034 = 1.0970271732607941.\n",
      "Evaluation loss in epoch 529 of generation 1: 0.11996891557343752 + 0.9799213762092296 = 1.0998902917826672.\n",
      "Training loss in epoch 530 of generation 1: 0.12496459470329534 + 0.9681843334222969 = 1.0931489281255922.\n",
      "Evaluation loss in epoch 530 of generation 1: 0.1199689192470674 + 0.9746061025305044 = 1.0945750217775718.\n",
      "Training loss in epoch 531 of generation 1: 0.12496460057883985 + 0.9687662091583851 = 1.093730809737225.\n",
      "Evaluation loss in epoch 531 of generation 1: 0.11996889610319916 + 1.0030041416914695 = 1.1229730377946685.\n",
      "Training loss in epoch 532 of generation 1: 0.12496459305079845 + 0.9755506181331336 = 1.100515211183932.\n",
      "Evaluation loss in epoch 532 of generation 1: 0.11996891649184499 + 0.9863633139658783 = 1.1063322304577232.\n",
      "Training loss in epoch 533 of generation 1: 0.12496459286718768 + 0.9729803112501542 = 1.097944904117342.\n",
      "Evaluation loss in epoch 533 of generation 1: 0.11996885716272246 + 0.9733383196344361 = 1.0933071767971585.\n",
      "Training loss in epoch 534 of generation 1: 0.12496465804900947 + 0.9703490661574089 = 1.0953137242064184.\n",
      "Evaluation loss in epoch 534 of generation 1: 0.11996889334797675 + 0.9804431859444029 = 1.1004120792923797.\n",
      "Training loss in epoch 535 of generation 1: 0.12496459507051688 + 0.9685592107879322 = 1.093523805858449.\n",
      "Evaluation loss in epoch 535 of generation 1: 0.11996891557343752 + 0.9824872656118337 = 1.1024561811852713.\n",
      "Training loss in epoch 536 of generation 1: 0.12496459543773841 + 0.9681890206379206 = 1.093153616075659.\n",
      "Evaluation loss in epoch 536 of generation 1: 0.11996892145124533 + 1.011695867694214 = 1.1316647891454594.\n",
      "Training loss in epoch 537 of generation 1: 0.12496460149689367 + 0.9723402397146949 = 1.0973048412115884.\n",
      "Evaluation loss in epoch 537 of generation 1: 0.11996889757265111 + 1.0149865319438633 = 1.1349554295165145.\n",
      "Training loss in epoch 538 of generation 1: 0.12496459231635539 + 0.9725123050353323 = 1.0974768973516875.\n",
      "Evaluation loss in epoch 538 of generation 1: 0.11996889040907285 + 0.9814962422351807 = 1.1014651326442537.\n",
      "Training loss in epoch 539 of generation 1: 0.12496458331942788 + 0.9741672182431991 = 1.099131801562627.\n",
      "Evaluation loss in epoch 539 of generation 1: 0.11996877322027974 + 0.9769844795779564 = 1.0969532527982362.\n",
      "Training loss in epoch 540 of generation 1: 0.12496468595784584 + 0.9673170134522706 = 1.0922816994101165.\n",
      "Evaluation loss in epoch 540 of generation 1: 0.11996891336925959 + 0.9911752386342947 = 1.1111441520035543.\n",
      "Training loss in epoch 541 of generation 1: 0.12496459204093924 + 0.9709656854570027 = 1.0959302774979418.\n",
      "Evaluation loss in epoch 541 of generation 1: 0.11996891336925959 + 0.9744623592711744 = 1.094431272640434.\n",
      "Training loss in epoch 542 of generation 1: 0.12496459130649618 + 0.9691669007287743 = 1.0941314920352705.\n",
      "Evaluation loss in epoch 542 of generation 1: 0.11996891336925959 + 0.9898294234312555 = 1.109798336800515.\n",
      "Training loss in epoch 543 of generation 1: 0.12496459249996615 + 0.9666158700384082 = 1.0915804625383743.\n",
      "Evaluation loss in epoch 543 of generation 1: 0.11996891483871154 + 0.9862150976948084 = 1.10618401253352.\n",
      "Training loss in epoch 544 of generation 1: 0.12496459213274462 + 0.9684033957544914 = 1.093367987887236.\n",
      "Evaluation loss in epoch 544 of generation 1: 0.11996891630816349 + 0.9754167830080758 = 1.0953856993162394.\n",
      "Training loss in epoch 545 of generation 1: 0.12496459488690612 + 0.9700927425908914 = 1.0950573374777974.\n",
      "Evaluation loss in epoch 545 of generation 1: 0.11996892145124533 + 1.0049720229279646 = 1.12494094437921.\n",
      "Training loss in epoch 546 of generation 1: 0.12496459727384607 + 0.9682324085962695 = 1.0931970058701155.\n",
      "Evaluation loss in epoch 546 of generation 1: 0.11996889904210306 + 0.9733673677606097 = 1.093336266802713.\n",
      "Training loss in epoch 547 of generation 1: 0.12496459598857071 + 0.9683192094840313 = 1.093283805472602.\n",
      "Evaluation loss in epoch 547 of generation 1: 0.11996890142996249 + 0.9759079825896512 = 1.0958768840196136.\n",
      "Training loss in epoch 548 of generation 1: 0.12496458625720012 + 0.9665873383943236 = 1.0915519246515237.\n",
      "Evaluation loss in epoch 548 of generation 1: 0.11996870140081561 + 0.974462221142691 = 1.0944309225435065.\n",
      "Training loss in epoch 549 of generation 1: 0.12496491583852434 + 0.9727258854845097 = 1.097690801323034.\n",
      "Evaluation loss in epoch 549 of generation 1: 0.11996891336925959 + 0.9884962455149975 = 1.1084651588842571.\n",
      "Training loss in epoch 550 of generation 1: 0.12496459139830156 + 0.9677684315360873 = 1.0927330229343888.\n",
      "Evaluation loss in epoch 550 of generation 1: 0.11996891336925959 + 1.0097373321721292 = 1.1297062455413889.\n",
      "Training loss in epoch 551 of generation 1: 0.12496458974580467 + 0.9705985491212442 = 1.0955631388670488.\n",
      "Evaluation loss in epoch 551 of generation 1: 0.11996891336925959 + 1.0025622010782063 = 1.122531114447466.\n",
      "Training loss in epoch 552 of generation 1: 0.12496459084746926 + 0.9700059975208023 = 1.0949705883682714.\n",
      "Evaluation loss in epoch 552 of generation 1: 0.11996891336925959 + 0.9928187251274685 = 1.1127876384967281.\n",
      "Training loss in epoch 553 of generation 1: 0.12496459093927464 + 0.9678512679001988 = 1.0928158588394734.\n",
      "Evaluation loss in epoch 553 of generation 1: 0.11996891336925959 + 0.9741414427206219 = 1.0941103560898815.\n",
      "Training loss in epoch 554 of generation 1: 0.12496459194913385 + 0.9752168930811657 = 1.1001814850302996.\n",
      "Evaluation loss in epoch 554 of generation 1: 0.11996891336925959 + 1.0199923919419112 = 1.1399613053111706.\n",
      "Training loss in epoch 555 of generation 1: 0.12496459029663697 + 0.9809487834544103 = 1.1059133737510471.\n",
      "Evaluation loss in epoch 555 of generation 1: 0.11996891336925959 + 0.990520412639036 = 1.1104893260082955.\n",
      "Training loss in epoch 556 of generation 1: 0.12496459176552309 + 0.9684914275688257 = 1.0934560193343488.\n",
      "Evaluation loss in epoch 556 of generation 1: 0.11996891336925959 + 0.9732692642094358 = 1.0932381775786955.\n",
      "Training loss in epoch 557 of generation 1: 0.12496459158191232 + 0.9742982928276429 = 1.0992628844095553.\n",
      "Evaluation loss in epoch 557 of generation 1: 0.11996891336925959 + 0.9977103381752049 = 1.1176792515444645.\n",
      "Training loss in epoch 558 of generation 1: 0.1249645901130262 + 0.9740498366152088 = 1.099014426728235.\n",
      "Evaluation loss in epoch 558 of generation 1: 0.11996891336925959 + 1.0145575651218786 = 1.134526478491138.\n",
      "Training loss in epoch 559 of generation 1: 0.12496459103108003 + 0.9704810573267946 = 1.0954456483578747.\n",
      "Evaluation loss in epoch 559 of generation 1: 0.11996891336925959 + 0.9732689100715155 = 1.0932378234407751.\n",
      "Training loss in epoch 560 of generation 1: 0.12496459139830156 + 0.9700647133058615 = 1.095029304704163.\n",
      "Evaluation loss in epoch 560 of generation 1: 0.11996891336925959 + 0.9973273049517662 = 1.1172962183210258.\n",
      "Training loss in epoch 561 of generation 1: 0.12496459360163074 + 0.9732693586617559 = 1.0982339522633866.\n",
      "Evaluation loss in epoch 561 of generation 1: 0.11996891336925959 + 1.0004766813962962 = 1.1204455947655558.\n",
      "Training loss in epoch 562 of generation 1: 0.12496459158191232 + 0.9722375161013421 = 1.0972021076832545.\n",
      "Evaluation loss in epoch 562 of generation 1: 0.11996891336925959 + 0.9808496437013976 = 1.1008185570706572.\n",
      "Training loss in epoch 563 of generation 1: 0.12496458974580467 + 0.9716130162276898 = 1.0965776059734946.\n",
      "Evaluation loss in epoch 563 of generation 1: 0.11996891336925959 + 0.9896213651989201 = 1.1095902785681797.\n",
      "Training loss in epoch 564 of generation 1: 0.1249645906638585 + 0.9705840585596248 = 1.0955486492234834.\n",
      "Evaluation loss in epoch 564 of generation 1: 0.11996891336925959 + 0.98874683409516 = 1.1087157474644196.\n",
      "Training loss in epoch 565 of generation 1: 0.12496459360163074 + 0.9700825808366813 = 1.0950471744383121.\n",
      "Evaluation loss in epoch 565 of generation 1: 0.11996891336925959 + 0.9870972905210428 = 1.1070662038903023.\n",
      "Training loss in epoch 566 of generation 1: 0.1249645901130262 + 0.9753455733812264 = 1.1003101634942525.\n",
      "Evaluation loss in epoch 566 of generation 1: 0.11996891336925959 + 0.9740935180146816 = 1.094062431383941.\n",
      "Training loss in epoch 567 of generation 1: 0.12496459029663697 + 0.9665351606214051 = 1.091499750918042.\n",
      "Evaluation loss in epoch 567 of generation 1: 0.11996891336925959 + 1.0021696796035178 = 1.1221385929727774.\n",
      "Training loss in epoch 568 of generation 1: 0.12496459103108003 + 0.9730213813061984 = 1.0979859723372785.\n",
      "Evaluation loss in epoch 568 of generation 1: 0.11996891336925959 + 0.9739290716468094 = 1.093897985016069.\n",
      "Training loss in epoch 569 of generation 1: 0.12496459029663697 + 0.9718611918828741 = 1.096825782179511.\n",
      "Evaluation loss in epoch 569 of generation 1: 0.11996891336925959 + 1.0116548685153204 = 1.1316237818845798.\n",
      "Training loss in epoch 570 of generation 1: 0.12496459084746926 + 0.9709254923259748 = 1.0958900831734442.\n",
      "Evaluation loss in epoch 570 of generation 1: 0.11996891483871154 + 0.9757267668361106 = 1.095695681674822.\n",
      "Training loss in epoch 571 of generation 1: 0.12496459121469079 + 0.9719110311890824 = 1.0968756224037732.\n",
      "Evaluation loss in epoch 571 of generation 1: 0.11996891483871154 + 0.9735484512634748 = 1.0935173661021862.\n",
      "Training loss in epoch 572 of generation 1: 0.12496459470329534 + 0.9666162651687757 = 1.091580859872071.\n",
      "Evaluation loss in epoch 572 of generation 1: 0.11996892145124533 + 1.0115332992859356 = 1.131502220737181.\n",
      "Training loss in epoch 573 of generation 1: 0.12496460112967214 + 0.9751467537687153 = 1.1001113548983874.\n",
      "Evaluation loss in epoch 573 of generation 1: 0.11996890051155502 + 0.9802010775714516 = 1.1001699780830065.\n",
      "Training loss in epoch 574 of generation 1: 0.1249645943360738 + 0.9684141112787704 = 1.0933787056148443.\n",
      "Evaluation loss in epoch 574 of generation 1: 0.11996889849105859 + 1.0508330305478606 = 1.170801929038919.\n",
      "Training loss in epoch 575 of generation 1: 0.12496457781110491 + 0.9695977925978856 = 1.0945623704089906.\n",
      "Evaluation loss in epoch 575 of generation 1: 0.11996848245247486 + 0.9882608598847235 = 1.1082293423371983.\n",
      "Training loss in epoch 576 of generation 1: 0.12496463234350229 + 0.9702470299827626 = 1.0952116623262649.\n",
      "Evaluation loss in epoch 576 of generation 1: 0.11996891630816349 + 0.9760643146438481 = 1.0960332309520116.\n",
      "Training loss in epoch 577 of generation 1: 0.12496459837551066 + 0.9674978171764633 = 1.092462415551974.\n",
      "Evaluation loss in epoch 577 of generation 1: 0.11996892439014922 + 0.9824724270860293 = 1.1024413514761786.\n",
      "Training loss in epoch 578 of generation 1: 0.12496459396885227 + 0.9746451056549813 = 1.0996096996238336.\n",
      "Evaluation loss in epoch 578 of generation 1: 0.11996890565463685 + 0.9756523067668333 = 1.0956212124214701.\n",
      "Training loss in epoch 579 of generation 1: 0.12496459479510072 + 0.9704287444163525 = 1.0953933392114532.\n",
      "Evaluation loss in epoch 579 of generation 1: 0.11996890583831835 + 0.977437732032341 = 1.0974066378706593.\n",
      "Training loss in epoch 580 of generation 1: 0.12496457744388338 + 0.9679029183429987 = 1.092867495786882.\n",
      "Evaluation loss in epoch 580 of generation 1: 0.11996797255264775 + 0.9758370051214985 = 1.0958049776741463.\n",
      "Training loss in epoch 581 of generation 1: 0.12496516334583634 + 0.9692363144114792 = 1.0942014777573155.\n",
      "Evaluation loss in epoch 581 of generation 1: 0.11996891336925959 + 0.9806301134576783 = 1.1005990268269379.\n",
      "Training loss in epoch 582 of generation 1: 0.12496459093927464 + 0.9662219470952812 = 1.0911865380345558.\n",
      "Evaluation loss in epoch 582 of generation 1: 0.11996891336925959 + 0.9858900696176961 = 1.1058589829869556.\n",
      "Training loss in epoch 583 of generation 1: 0.12496459084746926 + 0.9687429375955129 = 1.093707528442982.\n",
      "Evaluation loss in epoch 583 of generation 1: 0.11996891336925959 + 0.9738953433161708 = 1.0938642566854304.\n",
      "Training loss in epoch 584 of generation 1: 0.12496459029663697 + 0.9694436329991073 = 1.0944082232957442.\n",
      "Evaluation loss in epoch 584 of generation 1: 0.11996891336925959 + 0.9846879991434389 = 1.1046569125126984.\n",
      "Training loss in epoch 585 of generation 1: 0.12496459194913385 + 0.9666907009731406 = 1.0916552929222745.\n",
      "Evaluation loss in epoch 585 of generation 1: 0.11996891336925959 + 0.9887516274474252 = 1.1087205408166847.\n",
      "Training loss in epoch 586 of generation 1: 0.12496459002122082 + 0.9669915156314866 = 1.0919561056527074.\n",
      "Evaluation loss in epoch 586 of generation 1: 0.11996891336925959 + 0.9735241171391609 = 1.0934930305084205.\n",
      "Training loss in epoch 587 of generation 1: 0.12496459093927464 + 0.9819346293147179 = 1.1068992202539925.\n",
      "Evaluation loss in epoch 587 of generation 1: 0.11996891336925959 + 0.9771717700458639 = 1.0971406834151234.\n",
      "Training loss in epoch 588 of generation 1: 0.12496459103108003 + 0.9693933251182254 = 1.0943579161493053.\n",
      "Evaluation loss in epoch 588 of generation 1: 0.11996891336925959 + 0.9746560506717817 = 1.0946249640410413.\n",
      "Training loss in epoch 589 of generation 1: 0.12496459029663697 + 0.9686837943645914 = 1.0936483846612284.\n",
      "Evaluation loss in epoch 589 of generation 1: 0.11996891336925959 + 0.981311649681056 = 1.1012805630503157.\n",
      "Training loss in epoch 590 of generation 1: 0.12496459121469079 + 0.9703251056869442 = 1.095289696901635.\n",
      "Evaluation loss in epoch 590 of generation 1: 0.11996891336925959 + 0.990743061059796 = 1.1107119744290557.\n",
      "Training loss in epoch 591 of generation 1: 0.12496459176552309 + 0.9783915335001925 = 1.1033561252657156.\n",
      "Evaluation loss in epoch 591 of generation 1: 0.11996891336925959 + 0.9769810763272372 = 1.0969499896964967.\n",
      "Training loss in epoch 592 of generation 1: 0.12496458974580467 + 0.9694123354424501 = 1.0943769251882547.\n",
      "Evaluation loss in epoch 592 of generation 1: 0.11996891336925959 + 0.9766779636565268 = 1.0966468770257864.\n",
      "Training loss in epoch 593 of generation 1: 0.12496459139830156 + 0.9777867357961297 = 1.1027513271944314.\n",
      "Evaluation loss in epoch 593 of generation 1: 0.11996891336925959 + 0.9818687747549754 = 1.1018376881242349.\n",
      "Training loss in epoch 594 of generation 1: 0.12496459231635539 + 0.9663054077359957 = 1.0912700000523512.\n",
      "Evaluation loss in epoch 594 of generation 1: 0.11996891336925959 + 0.975062427608919 = 1.0950313409781787.\n",
      "Training loss in epoch 595 of generation 1: 0.12496459194913385 + 0.967177642598995 = 1.0921422345481289.\n",
      "Evaluation loss in epoch 595 of generation 1: 0.11996891336925959 + 0.9823845479815326 = 1.1023534613507922.\n",
      "Training loss in epoch 596 of generation 1: 0.12496458937858314 + 0.9744072973980644 = 1.0993718867766475.\n",
      "Evaluation loss in epoch 596 of generation 1: 0.11996891336925959 + 0.9733009015099477 = 1.0932698148792073.\n",
      "Training loss in epoch 597 of generation 1: 0.12496459213274462 + 0.9677051328263193 = 1.092669724959064.\n",
      "Evaluation loss in epoch 597 of generation 1: 0.11996891336925959 + 0.978915824361134 = 1.0988847377303936.\n",
      "Training loss in epoch 598 of generation 1: 0.1249645906638585 + 0.9734978864961373 = 1.0984624771599958.\n",
      "Evaluation loss in epoch 598 of generation 1: 0.11996891336925959 + 0.9868681309586864 = 1.106837044327946.\n",
      "Training loss in epoch 599 of generation 1: 0.12496459222455 + 0.9693658833876453 = 1.0943304756121952.\n",
      "Evaluation loss in epoch 599 of generation 1: 0.11996891336925959 + 0.9816185256182137 = 1.1015874389874734.\n",
      "Training loss in epoch 600 of generation 1: 0.12496459139830156 + 0.9662102341973235 = 1.091174825595625.\n",
      "Evaluation loss in epoch 600 of generation 1: 0.11996891336925959 + 0.9802281360596832 = 1.1001970494289428.\n",
      "Training loss in epoch 601 of generation 1: 0.1249645906638585 + 0.9728468746968219 = 1.0978114653606803.\n",
      "Evaluation loss in epoch 601 of generation 1: 0.11996891336925959 + 0.9911311462590433 = 1.111100059628303.\n",
      "Training loss in epoch 602 of generation 1: 0.12496459103108003 + 0.9723114862688055 = 1.0972760772998855.\n",
      "Evaluation loss in epoch 602 of generation 1: 0.11996891336925959 + 0.9784298545590534 = 1.098398767928313.\n",
      "Training loss in epoch 603 of generation 1: 0.12496459158191232 + 0.9701482855819215 = 1.0951128771638339.\n",
      "Evaluation loss in epoch 603 of generation 1: 0.11996891336925959 + 0.9986851079438244 = 1.118654021313084.\n",
      "Training loss in epoch 604 of generation 1: 0.12496459231635539 + 0.9669902597338501 = 1.0919548520502054.\n",
      "Evaluation loss in epoch 604 of generation 1: 0.11996891336925959 + 0.9786749958808323 = 1.098643909250092.\n",
      "Training loss in epoch 605 of generation 1: 0.12496458882775084 + 0.9681379401229371 = 1.093102528950688.\n",
      "Evaluation loss in epoch 605 of generation 1: 0.11996891336925959 + 1.010749790444396 = 1.1307187038136557.\n",
      "Training loss in epoch 606 of generation 1: 0.12496459158191232 + 0.9711135126876977 = 1.0960781042696102.\n",
      "Evaluation loss in epoch 606 of generation 1: 0.11996891336925959 + 0.9810613770330632 = 1.1010302904023228.\n",
      "Training loss in epoch 607 of generation 1: 0.12496459176552309 + 0.9714996990076065 = 1.0964642907731295.\n",
      "Evaluation loss in epoch 607 of generation 1: 0.11996891336925959 + 1.0007294888489051 = 1.1206984022181647.\n",
      "Training loss in epoch 608 of generation 1: 0.12496459194913385 + 0.9753277381659013 = 1.1002923301150351.\n",
      "Evaluation loss in epoch 608 of generation 1: 0.11996891336925959 + 0.990258281513796 = 1.1102271948830555.\n",
      "Training loss in epoch 609 of generation 1: 0.12496459286718768 + 0.9685291852866589 = 1.0934937781538465.\n",
      "Evaluation loss in epoch 609 of generation 1: 0.11996891336925959 + 0.9918468957322036 = 1.1118158091014634.\n",
      "Training loss in epoch 610 of generation 1: 0.12496459378524151 + 0.9676461937705692 = 1.0926107875558106.\n",
      "Evaluation loss in epoch 610 of generation 1: 0.11996891336925959 + 1.000582822849644 = 1.1205517362189035.\n",
      "Training loss in epoch 611 of generation 1: 0.12496459084746926 + 0.9679644367627837 = 1.092929027610253.\n",
      "Evaluation loss in epoch 611 of generation 1: 0.11996891336925959 + 0.9832887002977274 = 1.103257613666987.\n",
      "Training loss in epoch 612 of generation 1: 0.12496459002122082 + 0.972137957937588 = 1.0971025479588088.\n",
      "Evaluation loss in epoch 612 of generation 1: 0.11996891336925959 + 0.9761490285488417 = 1.0961179419181013.\n",
      "Training loss in epoch 613 of generation 1: 0.12496458992941543 + 0.9664659452383271 = 1.0914305351677425.\n",
      "Evaluation loss in epoch 613 of generation 1: 0.11996891336925959 + 0.9782543535041515 = 1.098223266873411.\n",
      "Training loss in epoch 614 of generation 1: 0.12496459084746926 + 0.9694337371132857 = 1.094398327960755.\n",
      "Evaluation loss in epoch 614 of generation 1: 0.11996891336925959 + 0.9777324717658327 = 1.0977013851350923.\n",
      "Training loss in epoch 615 of generation 1: 0.12496459121469079 + 0.969354564151169 = 1.0943191553658598.\n",
      "Evaluation loss in epoch 615 of generation 1: 0.11996891336925959 + 0.9771755538846385 = 1.0971444672538981.\n",
      "Training loss in epoch 616 of generation 1: 0.12496459121469079 + 0.9688973557182491 = 1.0938619469329398.\n",
      "Evaluation loss in epoch 616 of generation 1: 0.11996891336925959 + 1.004771968800408 = 1.1247408821696676.\n",
      "Training loss in epoch 617 of generation 1: 0.12496459158191232 + 0.9679409669002846 = 1.092905558482197.\n",
      "Evaluation loss in epoch 617 of generation 1: 0.11996891557343752 + 1.0142928124943573 = 1.134261728067795.\n",
      "Training loss in epoch 618 of generation 1: 0.12496459470329534 + 0.9729178527432494 = 1.0978824474465447.\n",
      "Evaluation loss in epoch 618 of generation 1: 0.11996891630816349 + 0.97596609500597 = 1.0959350113141335.\n",
      "Training loss in epoch 619 of generation 1: 0.12496459837551066 + 0.9677404316287799 = 1.0927050300042906.\n",
      "Evaluation loss in epoch 619 of generation 1: 0.11996892292069727 + 0.9976919876592368 = 1.1176609105799342.\n",
      "Training loss in epoch 620 of generation 1: 0.12496459690662454 + 0.9675860076304992 = 1.0925506045371236.\n",
      "Evaluation loss in epoch 620 of generation 1: 0.11996891226717063 + 0.9864141026336717 = 1.1063830149008422.\n",
      "Training loss in epoch 621 of generation 1: 0.12496459635579224 + 0.9718499049618926 = 1.0968145013176849.\n",
      "Evaluation loss in epoch 621 of generation 1: 0.1199688861843985 + 0.9788835140516284 = 1.098852400236027.\n",
      "Training loss in epoch 622 of generation 1: 0.12496460278216903 + 0.9697165035678187 = 1.0946811063499877.\n",
      "Evaluation loss in epoch 622 of generation 1: 0.11996890859354074 + 1.0174885751286347 = 1.1374574837221754.\n",
      "Training loss in epoch 623 of generation 1: 0.12496459617218147 + 0.9726909656546692 = 1.0976555618268506.\n",
      "Evaluation loss in epoch 623 of generation 1: 0.1199688496317812 + 0.9875240061426384 = 1.1074928557744195.\n",
      "Training loss in epoch 624 of generation 1: 0.12496471092890996 + 0.9729443911088629 = 1.0979091020377727.\n",
      "Evaluation loss in epoch 624 of generation 1: 0.11996891336925959 + 0.9767766138438267 = 1.0967455272130864.\n",
      "Training loss in epoch 625 of generation 1: 0.12496459158191232 + 0.966649920287663 = 1.0916145118695753.\n",
      "Evaluation loss in epoch 625 of generation 1: 0.11996891336925959 + 0.9855237190822608 = 1.1054926324515204.\n",
      "Training loss in epoch 626 of generation 1: 0.12496459139830156 + 0.9676657108605069 = 1.0926303022588084.\n",
      "Evaluation loss in epoch 626 of generation 1: 0.11996891336925959 + 0.9789439599576459 = 1.0989128733269056.\n",
      "Training loss in epoch 627 of generation 1: 0.12496459213274462 + 0.9710298610917871 = 1.0959944532245318.\n",
      "Evaluation loss in epoch 627 of generation 1: 0.11996891336925959 + 0.9746854867332705 = 1.0946544001025302.\n",
      "Training loss in epoch 628 of generation 1: 0.12496459176552309 + 0.9685455545536316 = 1.0935101463191548.\n",
      "Evaluation loss in epoch 628 of generation 1: 0.11996891336925959 + 0.9732960258683733 = 1.0932649392376328.\n",
      "Training loss in epoch 629 of generation 1: 0.12496459084746926 + 0.9774351667155199 = 1.1023997575629891.\n",
      "Evaluation loss in epoch 629 of generation 1: 0.11996891336925959 + 0.9740350132546917 = 1.0940039266239514.\n",
      "Training loss in epoch 630 of generation 1: 0.12496459103108003 + 0.9707745539944678 = 1.0957391450255478.\n",
      "Evaluation loss in epoch 630 of generation 1: 0.11996891336925959 + 0.9765432766295729 = 1.0965121899988324.\n",
      "Training loss in epoch 631 of generation 1: 0.12496458974580467 + 0.9695985182276312 = 1.094563107973436.\n",
      "Evaluation loss in epoch 631 of generation 1: 0.11996891336925959 + 0.9976406508858656 = 1.1176095642551251.\n",
      "Training loss in epoch 632 of generation 1: 0.1249645906638585 + 0.9733341585731433 = 1.0982987492370018.\n",
      "Evaluation loss in epoch 632 of generation 1: 0.11996891336925959 + 0.9737345426585898 = 1.0937034560278494.\n",
      "Training loss in epoch 633 of generation 1: 0.12496459176552309 + 0.9694152981857634 = 1.0943798899512867.\n",
      "Evaluation loss in epoch 633 of generation 1: 0.11996891336925959 + 0.9740061561572717 = 1.0939750695265313.\n",
      "Training loss in epoch 634 of generation 1: 0.12496458937858314 + 0.9734021753451436 = 1.0983667647237267.\n",
      "Evaluation loss in epoch 634 of generation 1: 0.11996891336925959 + 0.9742476752799906 = 1.09421658864925.\n",
      "Training loss in epoch 635 of generation 1: 0.12496459029663697 + 0.9739062515276415 = 1.0988708418242785.\n",
      "Evaluation loss in epoch 635 of generation 1: 0.11996891336925959 + 0.9737330085507526 = 1.0937019219200121.\n",
      "Training loss in epoch 636 of generation 1: 0.12496459121469079 + 0.9697734229051475 = 1.0947380141198382.\n",
      "Evaluation loss in epoch 636 of generation 1: 0.11996891336925959 + 0.9826714070442094 = 1.102640320413469.\n",
      "Training loss in epoch 637 of generation 1: 0.12496458992941543 + 0.9663781381635699 = 1.0913427280929853.\n",
      "Evaluation loss in epoch 637 of generation 1: 0.11996891336925959 + 0.9859979303298269 = 1.1059668436990864.\n",
      "Training loss in epoch 638 of generation 1: 0.12496459286718768 + 0.9671859550255741 = 1.0921505478927618.\n",
      "Evaluation loss in epoch 638 of generation 1: 0.11996891336925959 + 0.9989518560870954 = 1.118920769456355.\n",
      "Training loss in epoch 639 of generation 1: 0.12496459158191232 + 0.9692132484926643 = 1.0941778400745767.\n",
      "Evaluation loss in epoch 639 of generation 1: 0.11996891336925959 + 0.9800424208442676 = 1.1000113342135271.\n",
      "Training loss in epoch 640 of generation 1: 0.12496458974580467 + 0.9705533500263036 = 1.0955179397721082.\n",
      "Evaluation loss in epoch 640 of generation 1: 0.11996891336925959 + 0.9925193624981379 = 1.1124882758673975.\n",
      "Training loss in epoch 641 of generation 1: 0.12496459213274462 + 0.9685134755495561 = 1.0934780676823008.\n",
      "Evaluation loss in epoch 641 of generation 1: 0.11996891336925959 + 0.9732458396758799 = 1.0932147530451395.\n",
      "Training loss in epoch 642 of generation 1: 0.12496459378524151 + 0.9677435588873392 = 1.0927081526725808.\n",
      "Evaluation loss in epoch 642 of generation 1: 0.11996891336925959 + 0.9780743632735384 = 1.0980432766427979.\n",
      "Training loss in epoch 643 of generation 1: 0.12496459139830156 + 0.9671200035074651 = 1.0920845949057667.\n",
      "Evaluation loss in epoch 643 of generation 1: 0.11996891557343752 + 0.9760739219207059 = 1.0960428374941433.\n",
      "Training loss in epoch 644 of generation 1: 0.124964596539403 + 0.9673102132439567 = 1.0922748097833597.\n",
      "Evaluation loss in epoch 644 of generation 1: 0.11996892145124533 + 0.9924659258779281 = 1.1124348473291734.\n",
      "Training loss in epoch 645 of generation 1: 0.12496459910995372 + 0.966247463116152 = 1.0912120622261057.\n",
      "Evaluation loss in epoch 645 of generation 1: 0.11996890859354074 + 0.9800256029666847 = 1.0999945115602254.\n",
      "Training loss in epoch 646 of generation 1: 0.12496463454683147 + 0.970914190716132 = 1.0958788252629634.\n",
      "Evaluation loss in epoch 646 of generation 1: 0.11996871260538675 + 0.9781683023978822 = 1.098137015003269.\n",
      "Training loss in epoch 647 of generation 1: 0.12496461545131185 + 0.9668959983734613 = 1.091860613824773.\n",
      "Evaluation loss in epoch 647 of generation 1: 0.11996891336925959 + 0.979092470119106 = 1.0990613834883656.\n",
      "Training loss in epoch 648 of generation 1: 0.12496459268357692 + 0.9686907715736833 = 1.0936553642572602.\n",
      "Evaluation loss in epoch 648 of generation 1: 0.11996891336925959 + 0.9737941509769951 = 1.0937630643462546.\n",
      "Training loss in epoch 649 of generation 1: 0.12496459194913385 + 0.9709046605829555 = 1.0958692525320894.\n",
      "Evaluation loss in epoch 649 of generation 1: 0.11996891336925959 + 0.9819479414788527 = 1.1019168548481122.\n",
      "Training loss in epoch 650 of generation 1: 0.12496459084746926 + 0.9702170162325783 = 1.0951816070800475.\n",
      "Evaluation loss in epoch 650 of generation 1: 0.11996891336925959 + 0.9908667213193073 = 1.1108356346885668.\n",
      "Training loss in epoch 651 of generation 1: 0.12496459048024773 + 0.9674033472342449 = 1.0923679377144926.\n",
      "Evaluation loss in epoch 651 of generation 1: 0.11996891336925959 + 1.0140352293229067 = 1.1340041426921663.\n",
      "Training loss in epoch 652 of generation 1: 0.12496459112288541 + 0.9705750043455527 = 1.0955395954684382.\n",
      "Evaluation loss in epoch 652 of generation 1: 0.11996891336925959 + 0.9808860023510291 = 1.1008549157202887.\n",
      "Training loss in epoch 653 of generation 1: 0.12496459231635539 + 0.9683701196082243 = 1.0933347119245798.\n",
      "Evaluation loss in epoch 653 of generation 1: 0.11996891336925959 + 0.9735801017890542 = 1.0935490151583138.\n",
      "Training loss in epoch 654 of generation 1: 0.12496459213274462 + 0.9717519934194842 = 1.0967165855522287.\n",
      "Evaluation loss in epoch 654 of generation 1: 0.11996891336925959 + 0.9912356272316861 = 1.1112045406009456.\n",
      "Training loss in epoch 655 of generation 1: 0.12496459103108003 + 0.9742974012137653 = 1.0992619922448452.\n",
      "Evaluation loss in epoch 655 of generation 1: 0.11996891336925959 + 0.9746781953126881 = 1.0946471086819476.\n",
      "Training loss in epoch 656 of generation 1: 0.12496459158191232 + 0.9675952572064258 = 1.0925598487883381.\n",
      "Evaluation loss in epoch 656 of generation 1: 0.11996891336925959 + 0.9744552059790754 = 1.094424119348335.\n",
      "Training loss in epoch 657 of generation 1: 0.12496459084746926 + 0.968664271399109 = 1.0936288622465784.\n",
      "Evaluation loss in epoch 657 of generation 1: 0.11996891336925959 + 0.98894919526228 = 1.1089181086315396.\n",
      "Training loss in epoch 658 of generation 1: 0.1249645906638585 + 0.9681090148173711 = 1.0930736054812296.\n",
      "Evaluation loss in epoch 658 of generation 1: 0.11996891336925959 + 0.976564673319436 = 1.0965335866886956.\n",
      "Training loss in epoch 659 of generation 1: 0.12496459305079845 + 0.9702376981492129 = 1.0952022912000114.\n",
      "Evaluation loss in epoch 659 of generation 1: 0.11996891336925959 + 0.9810433351020049 = 1.1010122484712646.\n",
      "Training loss in epoch 660 of generation 1: 0.1249645906638585 + 0.9700256482793773 = 1.0949902389432358.\n",
      "Evaluation loss in epoch 660 of generation 1: 0.11996891336925959 + 0.9789766508552069 = 1.0989455642244665.\n",
      "Training loss in epoch 661 of generation 1: 0.12496458937858314 + 0.9724592870595504 = 1.0974238764381334.\n",
      "Evaluation loss in epoch 661 of generation 1: 0.11996891336925959 + 1.0219368986062165 = 1.1419058119754761.\n",
      "Training loss in epoch 662 of generation 1: 0.12496459139830156 + 0.9737996573993505 = 1.098764248797652.\n",
      "Evaluation loss in epoch 662 of generation 1: 0.11996891336925959 + 1.0152313749668962 = 1.1352002883361558.\n",
      "Training loss in epoch 663 of generation 1: 0.12496459103108003 + 0.9719690022488764 = 1.0969335932799564.\n",
      "Evaluation loss in epoch 663 of generation 1: 0.11996891336925959 + 0.9857887185777097 = 1.1057576319469693.\n",
      "Training loss in epoch 664 of generation 1: 0.12496459194913385 + 0.9676689982276537 = 1.0926335901767876.\n",
      "Evaluation loss in epoch 664 of generation 1: 0.11996891336925959 + 0.974365989673303 = 1.0943349030425626.\n",
      "Training loss in epoch 665 of generation 1: 0.12496459341801998 + 0.9714094242041599 = 1.09637401762218.\n",
      "Evaluation loss in epoch 665 of generation 1: 0.11996891336925959 + 0.9839882270213451 = 1.1039571403906048.\n",
      "Training loss in epoch 666 of generation 1: 0.12496459231635539 + 0.9673490807052572 = 1.0923136730216125.\n",
      "Evaluation loss in epoch 666 of generation 1: 0.11996891336925959 + 0.9775472782058598 = 1.0975161915751195.\n",
      "Training loss in epoch 667 of generation 1: 0.12496459158191232 + 0.9660849264598049 = 1.0910495180417172.\n",
      "Evaluation loss in epoch 667 of generation 1: 0.11996891336925959 + 0.9941892476956172 = 1.1141581610648768.\n",
      "Training loss in epoch 668 of generation 1: 0.12496459139830156 + 0.9700844228198816 = 1.0950490142181832.\n",
      "Evaluation loss in epoch 668 of generation 1: 0.11996891557343752 + 1.00818987877233 = 1.1281587943457676.\n",
      "Training loss in epoch 669 of generation 1: 0.1249645943360738 + 0.9743107372308907 = 1.0992753315669646.\n",
      "Evaluation loss in epoch 669 of generation 1: 0.11996891630816349 + 1.0073565848986799 = 1.1273255012068433.\n",
      "Training loss in epoch 670 of generation 1: 0.1249645943360738 + 0.9688622552154154 = 1.0938268495514891.\n",
      "Evaluation loss in epoch 670 of generation 1: 0.11996889610319916 + 0.9753510529528414 = 1.0953199490560406.\n",
      "Training loss in epoch 671 of generation 1: 0.12496459103108003 + 0.9712878254041059 = 1.096252416435186.\n",
      "Evaluation loss in epoch 671 of generation 1: 0.11996888434758356 + 0.9875402377088925 = 1.1075091220564761.\n",
      "Training loss in epoch 672 of generation 1: 0.12496466190483554 + 0.9687921952228157 = 1.0937568571276512.\n",
      "Evaluation loss in epoch 672 of generation 1: 0.11996891336925959 + 0.9732896484469045 = 1.093258561816164.\n",
      "Training loss in epoch 673 of generation 1: 0.1249645901130262 + 0.9690960798531622 = 1.0940606699661883.\n",
      "Evaluation loss in epoch 673 of generation 1: 0.11996891336925959 + 0.9904407463014953 = 1.110409659670755.\n",
      "Training loss in epoch 674 of generation 1: 0.1249645906638585 + 0.9693004327597047 = 1.0942650234235631.\n",
      "Evaluation loss in epoch 674 of generation 1: 0.11996891336925959 + 0.9963677498886508 = 1.1163366632579104.\n",
      "Training loss in epoch 675 of generation 1: 0.12496459249996615 + 0.9699226749553834 = 1.0948872674553496.\n",
      "Evaluation loss in epoch 675 of generation 1: 0.11996891336925959 + 0.9876776314663409 = 1.1076465448356005.\n",
      "Training loss in epoch 676 of generation 1: 0.12496458919497237 + 0.9681875855361768 = 1.0931521747311492.\n",
      "Evaluation loss in epoch 676 of generation 1: 0.11996891336925959 + 0.9871388451527741 = 1.1071077585220337.\n",
      "Training loss in epoch 677 of generation 1: 0.12496459194913385 + 0.9683691192967735 = 1.0933337112459074.\n",
      "Evaluation loss in epoch 677 of generation 1: 0.11996891336925959 + 0.9756619037575274 = 1.095630817126787.\n",
      "Training loss in epoch 678 of generation 1: 0.12496459139830156 + 0.9715855069283481 = 1.0965500983266496.\n",
      "Evaluation loss in epoch 678 of generation 1: 0.11996891336925959 + 0.9737001148688224 = 1.0936690282380819.\n",
      "Training loss in epoch 679 of generation 1: 0.12496459139830156 + 0.9713057634414597 = 1.0962703548397612.\n",
      "Evaluation loss in epoch 679 of generation 1: 0.11996891336925959 + 0.9806396355063228 = 1.1006085488755823.\n",
      "Training loss in epoch 680 of generation 1: 0.12496459286718768 + 0.9715748075618164 = 1.096539400429004.\n",
      "Evaluation loss in epoch 680 of generation 1: 0.11996891336925959 + 0.9796021083837665 = 1.0995710217530261.\n",
      "Training loss in epoch 681 of generation 1: 0.12496459103108003 + 0.9707898406923666 = 1.0957544317234467.\n",
      "Evaluation loss in epoch 681 of generation 1: 0.11996891336925959 + 0.9841887881098249 = 1.1041577014790844.\n",
      "Training loss in epoch 682 of generation 1: 0.12496459121469079 + 0.96803794716889 = 1.0930025383835809.\n",
      "Evaluation loss in epoch 682 of generation 1: 0.11996891336925959 + 0.9821782383970193 = 1.1021471517662789.\n",
      "Training loss in epoch 683 of generation 1: 0.12496458974580467 + 0.9714364766799168 = 1.0964010664257215.\n",
      "Evaluation loss in epoch 683 of generation 1: 0.11996891336925959 + 0.9859226679397841 = 1.1058915813090437.\n",
      "Training loss in epoch 684 of generation 1: 0.12496459139830156 + 0.9666850354793579 = 1.0916496268776594.\n",
      "Evaluation loss in epoch 684 of generation 1: 0.11996891336925959 + 0.9752602863752603 = 1.0952291997445198.\n",
      "Training loss in epoch 685 of generation 1: 0.12496459176552309 + 0.9687287246533711 = 1.0936933164188942.\n",
      "Evaluation loss in epoch 685 of generation 1: 0.11996891336925959 + 1.0124190085106894 = 1.132387921879949.\n",
      "Training loss in epoch 686 of generation 1: 0.12496459121469079 + 0.9671213401938384 = 1.0920859314085292.\n",
      "Evaluation loss in epoch 686 of generation 1: 0.11996891336925959 + 0.9768940127685735 = 1.096862926137833.\n",
      "Training loss in epoch 687 of generation 1: 0.12496459176552309 + 0.9680913639472547 = 1.093055955712778.\n",
      "Evaluation loss in epoch 687 of generation 1: 0.11996891336925959 + 0.9912581436439362 = 1.1112270570131957.\n",
      "Training loss in epoch 688 of generation 1: 0.12496459213274462 + 0.9669756825079494 = 1.091940274640694.\n",
      "Evaluation loss in epoch 688 of generation 1: 0.11996891336925959 + 0.9743037054828944 = 1.0942726188521539.\n",
      "Training loss in epoch 689 of generation 1: 0.12496459286718768 + 0.96745913846703 = 1.0924237313342178.\n",
      "Evaluation loss in epoch 689 of generation 1: 0.11996891336925959 + 0.9734149735954767 = 1.0933838869647363.\n",
      "Training loss in epoch 690 of generation 1: 0.12496459231635539 + 0.9768013070994815 = 1.101765899415837.\n",
      "Evaluation loss in epoch 690 of generation 1: 0.11996891336925959 + 0.9944878947018475 = 1.114456808071107.\n",
      "Training loss in epoch 691 of generation 1: 0.12496459121469079 + 0.9701329518796666 = 1.0950975430943575.\n",
      "Evaluation loss in epoch 691 of generation 1: 0.11996891336925959 + 0.9845965081260458 = 1.1045654214953053.\n",
      "Training loss in epoch 692 of generation 1: 0.12496459121469079 + 0.9696588541940859 = 1.0946234454087767.\n",
      "Evaluation loss in epoch 692 of generation 1: 0.11996891336925959 + 0.9859016871048232 = 1.1058706004740828.\n",
      "Training loss in epoch 693 of generation 1: 0.12496459213274462 + 0.9756199965625715 = 1.100584588695316.\n",
      "Evaluation loss in epoch 693 of generation 1: 0.11996891336925959 + 0.9800285080731925 = 1.0999974214424522.\n",
      "Training loss in epoch 694 of generation 1: 0.12496459213274462 + 0.9718296681177538 = 1.0967942602504985.\n",
      "Evaluation loss in epoch 694 of generation 1: 0.11996891336925959 + 0.9844321602114544 = 1.104401073580714.\n",
      "Training loss in epoch 695 of generation 1: 0.12496459084746926 + 0.9667388349025581 = 1.0917034257500273.\n",
      "Evaluation loss in epoch 695 of generation 1: 0.11996891336925959 + 0.979955989149943 = 1.0999249025192026.\n",
      "Training loss in epoch 696 of generation 1: 0.12496459176552309 + 0.9704711100299587 = 1.0954357017954819.\n",
      "Evaluation loss in epoch 696 of generation 1: 0.11996891336925959 + 0.9734124711188036 = 1.093381384488063.\n",
      "Training loss in epoch 697 of generation 1: 0.12496459103108003 + 0.9709526020882907 = 1.0959171931193707.\n",
      "Evaluation loss in epoch 697 of generation 1: 0.11996891336925959 + 0.9743574874243126 = 1.0943264007935722.\n",
      "Training loss in epoch 698 of generation 1: 0.1249645906638585 + 0.9681413787853548 = 1.0931059694492133.\n",
      "Evaluation loss in epoch 698 of generation 1: 0.11996891336925959 + 0.9911189160104525 = 1.1110878293797122.\n",
      "Training loss in epoch 699 of generation 1: 0.12496459139830156 + 0.9708872836601014 = 1.095851875058403.\n",
      "Evaluation loss in epoch 699 of generation 1: 0.11996891336925959 + 0.9767324832628247 = 1.0967013966320842.\n",
      "Training loss in epoch 700 of generation 1: 0.1249645906638585 + 0.9668430670619608 = 1.0918076577258193.\n",
      "Evaluation loss in epoch 700 of generation 1: 0.11996891336925959 + 0.9824897284133042 = 1.1024586417825637.\n",
      "Training loss in epoch 701 of generation 1: 0.1249645906638585 + 0.9684005857753349 = 1.0933651764391934.\n",
      "Evaluation loss in epoch 701 of generation 1: 0.11996891336925959 + 0.9784284629880555 = 1.0983973763573152.\n",
      "Training loss in epoch 702 of generation 1: 0.12496458992941543 + 0.9701869025981376 = 1.095151492527553.\n",
      "Evaluation loss in epoch 702 of generation 1: 0.11996891483871154 + 1.0001001137614434 = 1.120069028600155.\n",
      "Training loss in epoch 703 of generation 1: 0.12496459176552309 + 0.9688334136363586 = 1.0937980054018817.\n",
      "Evaluation loss in epoch 703 of generation 1: 0.11996891630816349 + 0.976155388336887 = 1.0961243046450504.\n",
      "Training loss in epoch 704 of generation 1: 0.12496459672301377 + 0.9751358898869376 = 1.1001004866099513.\n",
      "Evaluation loss in epoch 704 of generation 1: 0.11996892439014922 + 0.9890083936035908 = 1.10897731799374.\n",
      "Training loss in epoch 705 of generation 1: 0.12496459947717525 + 0.9666524761495198 = 1.091617075626695.\n",
      "Evaluation loss in epoch 705 of generation 1: 0.11996891336925959 + 0.9988300047229361 = 1.1187989180921958.\n",
      "Training loss in epoch 706 of generation 1: 0.12496459194913385 + 0.9696313302058829 = 1.0945959221550168.\n",
      "Evaluation loss in epoch 706 of generation 1: 0.11996891336925959 + 0.9919995761983016 = 1.1119684895675612.\n",
      "Training loss in epoch 707 of generation 1: 0.12496459231635539 + 0.9650596336626208 = 1.090024225978976.\n",
      "Evaluation loss in epoch 707 of generation 1: 0.11996891336925959 + 1.0083623365922407 = 1.1283312499615004.\n",
      "Training loss in epoch 708 of generation 1: 0.12496459231635539 + 0.9727058043423 = 1.0976703966586554.\n",
      "Evaluation loss in epoch 708 of generation 1: 0.11996891557343752 + 0.973419371665167 = 1.0933882872386045.\n",
      "Training loss in epoch 709 of generation 1: 0.12496459360163074 + 0.971508264082599 = 1.0964728576842298.\n",
      "Evaluation loss in epoch 709 of generation 1: 0.11996891777761545 + 0.9848847470041049 = 1.1048536647817204.\n",
      "Training loss in epoch 710 of generation 1: 0.1249645981918999 + 0.9684753344524444 = 1.0934399326443442.\n",
      "Evaluation loss in epoch 710 of generation 1: 0.11996890289941443 + 0.978176851669335 = 1.0981457545687494.\n",
      "Training loss in epoch 711 of generation 1: 0.12496459782467836 + 0.972561762431148 = 1.0975263602558263.\n",
      "Evaluation loss in epoch 711 of generation 1: 0.11996891483871154 + 0.9733384518851117 = 1.0933073667238233.\n",
      "Training loss in epoch 712 of generation 1: 0.12496459305079845 + 0.9773207140464621 = 1.1022853070972605.\n",
      "Evaluation loss in epoch 712 of generation 1: 0.11996891630816349 + 0.973257875956813 = 1.0932267922649765.\n",
      "Training loss in epoch 713 of generation 1: 0.12496460149689367 + 0.9777154580969327 = 1.1026800595938264.\n",
      "Evaluation loss in epoch 713 of generation 1: 0.11996892439014922 + 0.9925409560945625 = 1.1125098804847118.\n",
      "Training loss in epoch 714 of generation 1: 0.12496459268357692 + 0.9666676482743009 = 1.0916322409578778.\n",
      "Evaluation loss in epoch 714 of generation 1: 0.11996889849105859 + 0.9902624356544624 = 1.110231334145521.\n",
      "Training loss in epoch 715 of generation 1: 0.12496459727384607 + 0.967920096966226 = 1.092884694240072.\n",
      "Evaluation loss in epoch 715 of generation 1: 0.11996891336925959 + 0.9892709156030499 = 1.1092398289723095.\n",
      "Training loss in epoch 716 of generation 1: 0.12496459084746926 + 0.9694783941892463 = 1.0944429850367157.\n",
      "Evaluation loss in epoch 716 of generation 1: 0.11996891336925959 + 0.9733596972214239 = 1.0933286105906834.\n",
      "Training loss in epoch 717 of generation 1: 0.12496459139830156 + 0.9703611213058336 = 1.0953257127041351.\n",
      "Evaluation loss in epoch 717 of generation 1: 0.11996891557343752 + 0.9765495026974906 = 1.096518418270928.\n",
      "Training loss in epoch 718 of generation 1: 0.12496459451968457 + 0.9670069036825281 = 1.0919714982022126.\n",
      "Evaluation loss in epoch 718 of generation 1: 0.11996892292069727 + 0.9856649892539566 = 1.105633912174654.\n",
      "Training loss in epoch 719 of generation 1: 0.12496459810009451 + 0.9679061895523982 = 1.0928707876524928.\n",
      "Evaluation loss in epoch 719 of generation 1: 0.11996891336925959 + 0.9977776023382766 = 1.1177465157075361.\n",
      "Training loss in epoch 720 of generation 1: 0.12496459158191232 + 0.9663382432164254 = 1.0913028347983378.\n",
      "Evaluation loss in epoch 720 of generation 1: 0.11996891336925959 + 0.9789196331805918 = 1.0988885465498515.\n",
      "Training loss in epoch 721 of generation 1: 0.12496459029663697 + 0.9685441547051549 = 1.0935087450017917.\n",
      "Evaluation loss in epoch 721 of generation 1: 0.11996891336925959 + 0.9739257271741681 = 1.0938946405434278.\n",
      "Training loss in epoch 722 of generation 1: 0.12496459286718768 + 0.9682719759818081 = 1.0932365688489958.\n",
      "Evaluation loss in epoch 722 of generation 1: 0.11996891557343752 + 0.9755279367620294 = 1.095496852335467.\n",
      "Training loss in epoch 723 of generation 1: 0.1249645976410676 + 0.9680119948888404 = 1.092976592529908.\n",
      "Evaluation loss in epoch 723 of generation 1: 0.11996889610319916 + 0.9869480588586745 = 1.1069169549618738.\n",
      "Training loss in epoch 724 of generation 1: 0.12496460516910898 + 0.9714492633336317 = 1.0964138685027407.\n",
      "Evaluation loss in epoch 724 of generation 1: 0.11996891355294109 + 0.9926331083653337 = 1.1126020219182748.\n",
      "Training loss in epoch 725 of generation 1: 0.12496459048024773 + 0.9675284728298841 = 1.0924930633101317.\n",
      "Evaluation loss in epoch 725 of generation 1: 0.11996891336925959 + 0.9795195178368425 = 1.0994884312061022.\n",
      "Training loss in epoch 726 of generation 1: 0.12496459249996615 + 0.9699450137755666 = 1.0949096062755328.\n",
      "Evaluation loss in epoch 726 of generation 1: 0.11996891336925959 + 0.9734078864287155 = 1.093376799797975.\n",
      "Training loss in epoch 727 of generation 1: 0.12496459103108003 + 0.9692213964039975 = 1.0941859874350777.\n",
      "Evaluation loss in epoch 727 of generation 1: 0.11996891336925959 + 0.9885186399627356 = 1.1084875533319953.\n",
      "Training loss in epoch 728 of generation 1: 0.12496459048024773 + 0.9666682049821421 = 1.09163279546239.\n",
      "Evaluation loss in epoch 728 of generation 1: 0.11996891336925959 + 0.9908068646635223 = 1.1107757780327818.\n",
      "Training loss in epoch 729 of generation 1: 0.12496459194913385 + 0.965620516078216 = 1.0905851080273499.\n",
      "Evaluation loss in epoch 729 of generation 1: 0.11996891336925959 + 0.9850075138368298 = 1.1049764272060894.\n",
      "Training loss in epoch 730 of generation 1: 0.12496458992941543 + 0.9706766277631981 = 1.0956412176926136.\n",
      "Evaluation loss in epoch 730 of generation 1: 0.11996891336925959 + 0.9802820708241043 = 1.100250984193364.\n",
      "Training loss in epoch 731 of generation 1: 0.1249645943360738 + 0.9749821151363239 = 1.0999467094723978.\n",
      "Evaluation loss in epoch 731 of generation 1: 0.11996891336925959 + 0.9785853813535811 = 1.0985542947228406.\n",
      "Training loss in epoch 732 of generation 1: 0.12496459158191232 + 0.9703491983571602 = 1.0953137899390726.\n",
      "Evaluation loss in epoch 732 of generation 1: 0.11996891336925959 + 1.0280176112758361 = 1.1479865246450955.\n",
      "Training loss in epoch 733 of generation 1: 0.12496459286718768 + 0.9754044536815316 = 1.1003690465487193.\n",
      "Evaluation loss in epoch 733 of generation 1: 0.11996891336925959 + 0.973630761145076 = 1.0935996745143355.\n",
      "Training loss in epoch 734 of generation 1: 0.12496459213274462 + 0.9663493068667159 = 1.0913138989994606.\n",
      "Evaluation loss in epoch 734 of generation 1: 0.11996891336925959 + 0.9944670402397544 = 1.114435953609014.\n",
      "Training loss in epoch 735 of generation 1: 0.12496459121469079 + 0.9701935581211683 = 1.095158149335859.\n",
      "Evaluation loss in epoch 735 of generation 1: 0.11996891336925959 + 0.9752317158209708 = 1.0952006291902303.\n",
      "Training loss in epoch 736 of generation 1: 0.12496458992941543 + 0.9705720004734278 = 1.0955365904028433.\n",
      "Evaluation loss in epoch 736 of generation 1: 0.11996891336925959 + 0.9794495586988923 = 1.099418472068152.\n",
      "Training loss in epoch 737 of generation 1: 0.12496459048024773 + 0.9714362034670977 = 1.0964007939473455.\n",
      "Evaluation loss in epoch 737 of generation 1: 0.11996891336925959 + 0.9843325798610694 = 1.104301493230329.\n",
      "Training loss in epoch 738 of generation 1: 0.12496458864414008 + 0.9707300056160805 = 1.0956945942602205.\n",
      "Evaluation loss in epoch 738 of generation 1: 0.11996891336925959 + 0.9769491304418155 = 1.096918043811075.\n",
      "Training loss in epoch 739 of generation 1: 0.12496458901136161 + 0.9669220652266287 = 1.0918866542379904.\n",
      "Evaluation loss in epoch 739 of generation 1: 0.11996891336925959 + 0.9863036174803559 = 1.1062725308496155.\n",
      "Training loss in epoch 740 of generation 1: 0.1249645906638585 + 0.970004568294603 = 1.0949691589584616.\n",
      "Evaluation loss in epoch 740 of generation 1: 0.11996891336925959 + 0.9737606019194945 = 1.093729515288754.\n",
      "Training loss in epoch 741 of generation 1: 0.12496459176552309 + 0.9713283181879031 = 1.0962929099534262.\n",
      "Evaluation loss in epoch 741 of generation 1: 0.11996891336925959 + 0.9974185814291744 = 1.117387494798434.\n",
      "Training loss in epoch 742 of generation 1: 0.12496459213274462 + 0.9781534079638728 = 1.1031180000966174.\n",
      "Evaluation loss in epoch 742 of generation 1: 0.11996891557343752 + 0.997383849462853 = 1.1173527650362904.\n",
      "Training loss in epoch 743 of generation 1: 0.12496459415246304 + 0.9685540006488481 = 1.093518594801311.\n",
      "Evaluation loss in epoch 743 of generation 1: 0.11996892292069727 + 0.9818867505606959 = 1.1018556734813931.\n",
      "Training loss in epoch 744 of generation 1: 0.12496458276859558 + 0.9676892952961237 = 1.0926538780647193.\n",
      "Evaluation loss in epoch 744 of generation 1: 0.11993056177541912 + 0.9757430689360584 = 1.0956736307114776.\n",
      "Training loss in epoch 745 of generation 1: 0.12804921590506135 + 0.9695852615303565 = 1.0976344774354179.\n",
      "Evaluation loss in epoch 745 of generation 1: 0.11998639782698386 + 1.0031285748827072 = 1.123114972709691.\n",
      "Training loss in epoch 746 of generation 1: 0.12498628629552982 + 0.9718329187627476 = 1.0968192050582775.\n",
      "Evaluation loss in epoch 746 of generation 1: 0.11998765549417269 + 1.0222373427521834 = 1.1422249982463561.\n",
      "Training loss in epoch 747 of generation 1: 0.12499106715264392 + 0.9715582047419499 = 1.0965492718945937.\n",
      "Evaluation loss in epoch 747 of generation 1: 0.11999059403071234 + 0.9844685511890288 = 1.1044591452197412.\n",
      "Training loss in epoch 748 of generation 1: 0.12499705928997852 + 0.9689955522245658 = 1.0939926115145442.\n",
      "Evaluation loss in epoch 748 of generation 1: 0.1199931729188869 + 1.0016661968730816 = 1.1216593697919683.\n",
      "Training loss in epoch 749 of generation 1: 0.12500277004381488 + 0.9740751323031608 = 1.0990779023469757.\n",
      "Evaluation loss in epoch 749 of generation 1: 0.11999356691569135 + 1.0003784162054077 = 1.120371983121099.\n",
      "Training loss in epoch 750 of generation 1: 0.12500825862862489 + 0.9740653098616455 = 1.0990735684902704.\n",
      "Evaluation loss in epoch 750 of generation 1: 0.11998765457576523 + 0.9811040837151244 = 1.1010917382908896.\n",
      "Training loss in epoch 751 of generation 1: 0.12501284687804587 + 0.9721851062444159 = 1.0971979531224618.\n",
      "Evaluation loss in epoch 751 of generation 1: 0.1199824876153414 + 0.9732508314041585 = 1.0932333190194998.\n",
      "Training loss in epoch 752 of generation 1: 0.125021472269175 + 0.9660122650698786 = 1.0910337373390535.\n",
      "Evaluation loss in epoch 752 of generation 1: 0.1199868008241815 + 0.9733553549909078 = 1.0933421558150893.\n",
      "Training loss in epoch 753 of generation 1: 0.1250215448872328 + 0.9690624041698692 = 1.094083949057102.\n",
      "Evaluation loss in epoch 753 of generation 1: 0.1200037259717568 + 0.9961935228485906 = 1.1161972488203475.\n",
      "Training loss in epoch 754 of generation 1: 0.12502581714252622 + 0.9707165697146986 = 1.095742386857225.\n",
      "Evaluation loss in epoch 754 of generation 1: 0.11999375041350378 + 0.983837818327964 = 1.1038315687414677.\n",
      "Training loss in epoch 755 of generation 1: 0.12502570991383913 + 0.9730434336935873 = 1.0980691436074264.\n",
      "Evaluation loss in epoch 755 of generation 1: 0.11999885326908623 + 0.980355978379448 = 1.100354831648534.\n",
      "Training loss in epoch 756 of generation 1: 0.12503053391948313 + 0.9680234184162315 = 1.0930539523357146.\n",
      "Evaluation loss in epoch 756 of generation 1: 0.12001106276166824 + 0.9763663575535378 = 1.0963774203152061.\n",
      "Training loss in epoch 757 of generation 1: 0.12503462807233398 + 0.9698211146998414 = 1.0948557427721755.\n",
      "Evaluation loss in epoch 757 of generation 1: 0.12000803991532288 + 0.974480844976722 = 1.0944888848920449.\n",
      "Training loss in epoch 758 of generation 1: 0.12502865613218359 + 0.9780779600969681 = 1.1031066162291516.\n",
      "Evaluation loss in epoch 758 of generation 1: 0.12001621612966153 + 0.9733180059306611 = 1.0933342220603226.\n",
      "Training loss in epoch 759 of generation 1: 0.12502692394822112 + 0.9695873811330343 = 1.0946143050812556.\n",
      "Evaluation loss in epoch 759 of generation 1: 0.12000819237096284 + 0.9774101871555134 = 1.0974183795264763.\n",
      "Training loss in epoch 760 of generation 1: 0.12502550078117713 + 0.9673954108425136 = 1.0924209116236907.\n",
      "Evaluation loss in epoch 760 of generation 1: 0.11999781932595699 + 0.9794521273009031 = 1.0994499466268601.\n",
      "Training loss in epoch 761 of generation 1: 0.12502389033115227 + 0.966379305928039 = 1.0914031962591912.\n",
      "Evaluation loss in epoch 761 of generation 1: 0.11998990265356925 + 0.991117542072878 = 1.1111074447264473.\n",
      "Training loss in epoch 762 of generation 1: 0.12502443749123368 + 0.9648087545225241 = 1.0898331920137578.\n",
      "Evaluation loss in epoch 762 of generation 1: 0.11999378310880969 + 1.025472513148891 = 1.145466296257701.\n",
      "Training loss in epoch 763 of generation 1: 0.12502355065123597 + 0.9739897106994699 = 1.099013261350706.\n",
      "Evaluation loss in epoch 763 of generation 1: 0.1199850835858949 + 0.9757461871130991 = 1.0957312706989941.\n",
      "Training loss in epoch 764 of generation 1: 0.12502303433776316 + 0.9701161904200986 = 1.0951392247578617.\n",
      "Evaluation loss in epoch 764 of generation 1: 0.11999470151627926 + 0.9804681974860663 = 1.1004628990023455.\n",
      "Training loss in epoch 765 of generation 1: 0.12502073975402572 + 0.9689059428265336 = 1.0939266825805594.\n",
      "Evaluation loss in epoch 765 of generation 1: 0.11999287076282943 + 1.0134946752586425 = 1.1334875460214717.\n",
      "Training loss in epoch 766 of generation 1: 0.12501998915321605 + 0.9787668368428039 = 1.10378682599602.\n",
      "Evaluation loss in epoch 766 of generation 1: 0.1199915884823204 + 0.9961286024613814 = 1.1161201909437017.\n",
      "Training loss in epoch 767 of generation 1: 0.12501865724072256 + 0.969101429536428 = 1.0941200867771506.\n",
      "Evaluation loss in epoch 767 of generation 1: 0.11998272952386889 + 0.9953945877004662 = 1.115377317224335.\n",
      "Training loss in epoch 768 of generation 1: 0.12501756604194275 + 0.9706629171801109 = 1.0956804832220537.\n",
      "Evaluation loss in epoch 768 of generation 1: 0.1199862637394933 + 0.9861982518976385 = 1.1061845156371317.\n",
      "Training loss in epoch 769 of generation 1: 0.12501701190465223 + 0.9727836038958902 = 1.0978006158005424.\n",
      "Evaluation loss in epoch 769 of generation 1: 0.11998293010406023 + 0.9752317319849423 = 1.0952146620890024.\n",
      "Training loss in epoch 770 of generation 1: 0.1250149595035146 + 0.9730645283672228 = 1.0980794878707374.\n",
      "Evaluation loss in epoch 770 of generation 1: 0.11998359172480132 + 1.012715867193895 = 1.1326994589186963.\n",
      "Training loss in epoch 771 of generation 1: 0.12501437525405854 + 0.9716573369287279 = 1.0966717121827865.\n",
      "Evaluation loss in epoch 771 of generation 1: 0.1199720436692789 + 0.9740314674671332 = 1.0940035111364121.\n",
      "Training loss in epoch 772 of generation 1: 0.12501317150187943 + 0.9702981207799489 = 1.0953112922818284.\n",
      "Evaluation loss in epoch 772 of generation 1: 0.11997575715804137 + 0.9806384746392813 = 1.1006142317973227.\n",
      "Training loss in epoch 773 of generation 1: 0.12501484364512153 + 0.9720263622520426 = 1.0970412058971641.\n",
      "Evaluation loss in epoch 773 of generation 1: 0.11998309578476775 + 0.9768144772122563 = 1.096797572997024.\n",
      "Training loss in epoch 774 of generation 1: 0.12501205771897544 + 0.9676066307916887 = 1.0926186885106641.\n",
      "Evaluation loss in epoch 774 of generation 1: 0.11997330005069727 + 0.9896801006628663 = 1.1096534007135634.\n",
      "Training loss in epoch 775 of generation 1: 0.1250105778162049 + 0.9699645293966183 = 1.094975107212823.\n",
      "Evaluation loss in epoch 775 of generation 1: 0.11997774955120587 + 0.9930130263506356 = 1.1129907759018414.\n",
      "Training loss in epoch 776 of generation 1: 0.12500873197717854 + 0.966912355889345 = 1.0919210878665235.\n",
      "Evaluation loss in epoch 776 of generation 1: 0.1199695619486146 + 0.9776842561084059 = 1.0976538180570206.\n",
      "Training loss in epoch 777 of generation 1: 0.12501226978940969 + 0.9704038526720848 = 1.0954161224614944.\n",
      "Evaluation loss in epoch 777 of generation 1: 0.11997925573945596 + 0.9733749765828137 = 1.0933542323222696.\n",
      "Training loss in epoch 778 of generation 1: 0.12500853450380017 + 0.9742964185289479 = 1.0993049530327481.\n",
      "Evaluation loss in epoch 778 of generation 1: 0.11998463870931664 + 0.9965884116103726 = 1.1165730503196893.\n",
      "Training loss in epoch 779 of generation 1: 0.12500675467284403 + 0.9694926335713383 = 1.0944993882441825.\n",
      "Evaluation loss in epoch 779 of generation 1: 0.11997726389733596 + 0.9768651542017015 = 1.0968424180990375.\n",
      "Training loss in epoch 780 of generation 1: 0.12500698492074408 + 0.9688814138971387 = 1.0938883988178827.\n",
      "Evaluation loss in epoch 780 of generation 1: 0.11997722128322937 + 0.9820505062861509 = 1.1020277275693802.\n",
      "Training loss in epoch 781 of generation 1: 0.12500521528018543 + 0.9772893386388404 = 1.1022945539190259.\n",
      "Evaluation loss in epoch 781 of generation 1: 0.1199713372302533 + 0.9788793055412398 = 1.098850642771493.\n",
      "Training loss in epoch 782 of generation 1: 0.12500471934750762 + 0.9719476916489805 = 1.096952410996488.\n",
      "Evaluation loss in epoch 782 of generation 1: 0.11996674739708335 + 0.975530486261165 = 1.0954972336582485.\n",
      "Training loss in epoch 783 of generation 1: 0.1250071138155015 + 0.970578614867647 = 1.0955857286831485.\n",
      "Evaluation loss in epoch 783 of generation 1: 0.11996857906894066 + 0.9744124978275622 = 1.094381076896503.\n",
      "Training loss in epoch 784 of generation 1: 0.12500261021064343 + 0.971563250365788 = 1.0965658605764312.\n",
      "Evaluation loss in epoch 784 of generation 1: 0.11996885495854452 + 1.0200142426924272 = 1.1399830976509717.\n",
      "Training loss in epoch 785 of generation 1: 0.12500340910108446 + 0.9693499650687128 = 1.0943533741697973.\n",
      "Evaluation loss in epoch 785 of generation 1: 0.11996761474109761 + 0.9738286360753887 = 1.0937962508164862.\n",
      "Training loss in epoch 786 of generation 1: 0.12500139819597986 + 0.9670182126368014 = 1.0920196108327813.\n",
      "Evaluation loss in epoch 786 of generation 1: 0.11997317073892555 + 0.9779812352866715 = 1.097954406025597.\n",
      "Training loss in epoch 787 of generation 1: 0.12499983970780165 + 0.9656622471330163 = 1.090662086840818.\n",
      "Evaluation loss in epoch 787 of generation 1: 0.11996494180799816 + 0.9733498166265032 = 1.0933147584345013.\n",
      "Training loss in epoch 788 of generation 1: 0.12500615169508988 + 0.9707454171693 = 1.0957515688643897.\n",
      "Evaluation loss in epoch 788 of generation 1: 0.11997665480950213 + 0.9839908852599252 = 1.1039675400694273.\n",
      "Training loss in epoch 789 of generation 1: 0.12499947643390195 + 0.9684678005355112 = 1.0934672769694131.\n",
      "Evaluation loss in epoch 789 of generation 1: 0.11997718160802667 + 0.9748916802310796 = 1.0948688618391063.\n",
      "Training loss in epoch 790 of generation 1: 0.12499828287212032 + 0.9726604377943413 = 1.0976587206664616.\n",
      "Evaluation loss in epoch 790 of generation 1: 0.11997163038591759 + 0.9813799689106287 = 1.1013515992965464.\n",
      "Training loss in epoch 791 of generation 1: 0.12499680094963135 + 0.9671950562440023 = 1.0921918571936335.\n",
      "Evaluation loss in epoch 791 of generation 1: 0.11996444090856426 + 0.9804349555440236 = 1.1003993964525878.\n",
      "Training loss in epoch 792 of generation 1: 0.12499747131253648 + 0.970928299367359 = 1.0959257706798955.\n",
      "Evaluation loss in epoch 792 of generation 1: 0.119956554910986 + 0.9811189545688718 = 1.1010755094798577.\n",
      "Training loss in epoch 793 of generation 1: 0.12499558140692639 + 0.9676356119149274 = 1.0926311933218538.\n",
      "Evaluation loss in epoch 793 of generation 1: 0.1199646238553322 + 0.992587669972095 = 1.1125522938274273.\n",
      "Training loss in epoch 794 of generation 1: 0.12499706039164311 + 0.9716121995270046 = 1.0966092599186477.\n",
      "Evaluation loss in epoch 794 of generation 1: 0.11996147757502293 + 0.9837935672519019 = 1.1037550448269249.\n",
      "Training loss in epoch 795 of generation 1: 0.12499487652519735 + 0.9721241224991802 = 1.0971189990243775.\n",
      "Evaluation loss in epoch 795 of generation 1: 0.1199615567417468 + 0.9777729904339383 = 1.0977345471756852.\n",
      "Training loss in epoch 796 of generation 1: 0.12499521400178447 + 0.9670043301940378 = 1.0919995441958221.\n",
      "Evaluation loss in epoch 796 of generation 1: 0.11996358256494319 + 0.9740914754764692 = 1.0940550580414123.\n",
      "Training loss in epoch 797 of generation 1: 0.12499461157486262 + 0.9760603965315674 = 1.10105500810643.\n",
      "Evaluation loss in epoch 797 of generation 1: 0.11995827931285089 + 0.9732501583951647 = 1.0932084377080156.\n",
      "Training loss in epoch 798 of generation 1: 0.12499410462553885 + 0.9680307540335378 = 1.0930248586590767.\n",
      "Evaluation loss in epoch 798 of generation 1: 0.1199673041356914 + 0.974505569975255 = 1.0944728741109464.\n",
      "Training loss in epoch 799 of generation 1: 0.12499689220418185 + 0.9771804912392341 = 1.1021773834434159.\n",
      "Evaluation loss in epoch 799 of generation 1: 0.11995180839750175 + 1.0066096771663438 = 1.1265614855638455.\n",
      "Training loss in epoch 800 of generation 1: 0.12499101372191114 + 0.9726415082588533 = 1.0976325219807646.\n",
      "Evaluation loss in epoch 800 of generation 1: 0.11996580749887899 + 0.9734397132885291 = 1.093405520787408.\n",
      "Training loss in epoch 801 of generation 1: 0.12499091457209772 + 0.9733812393112095 = 1.0983721538833071.\n",
      "Evaluation loss in epoch 801 of generation 1: 0.11995862371565197 + 0.9752057770551261 = 1.0951644007707781.\n",
      "Training loss in epoch 802 of generation 1: 0.12499075868655775 + 0.9686980396222279 = 1.0936887983087855.\n",
      "Evaluation loss in epoch 802 of generation 1: 0.11995799130026842 + 1.0000857483991672 = 1.1200437396994356.\n",
      "Training loss in epoch 803 of generation 1: 0.1249903797139376 + 0.9795813239167 = 1.1045717036306375.\n",
      "Evaluation loss in epoch 803 of generation 1: 0.119949884333853 + 0.9842477542777275 = 1.1041976386115804.\n",
      "Training loss in epoch 804 of generation 1: 0.12499188881081988 + 0.9671949637041763 = 1.0921868525149963.\n",
      "Evaluation loss in epoch 804 of generation 1: 0.11995970339547321 + 0.9743581310442673 = 1.0943178344397406.\n",
      "Training loss in epoch 805 of generation 1: 0.1249884435384146 + 0.9693068708875889 = 1.0942953144260035.\n",
      "Evaluation loss in epoch 805 of generation 1: 0.11995368360187313 + 0.9787841717524595 = 1.0987378553543325.\n",
      "Training loss in epoch 806 of generation 1: 0.12499357949874931 + 0.9664404145285952 = 1.0914339940273445.\n",
      "Evaluation loss in epoch 806 of generation 1: 0.11995285868828395 + 0.9803707199214236 = 1.1003235786097076.\n",
      "Training loss in epoch 807 of generation 1: 0.12498771460367525 + 0.9696916514834762 = 1.0946793660871514.\n",
      "Evaluation loss in epoch 807 of generation 1: 0.11995865769672835 + 0.9732882142618 = 1.0932468719585284.\n",
      "Training loss in epoch 808 of generation 1: 0.1249874545190258 + 0.9779381823622358 = 1.1029256368812617.\n",
      "Evaluation loss in epoch 808 of generation 1: 0.11995694302998265 + 0.9896175167042596 = 1.1095744597342423.\n",
      "Training loss in epoch 809 of generation 1: 0.12498758350558863 + 0.9670857769918754 = 1.092073360497464.\n",
      "Evaluation loss in epoch 809 of generation 1: 0.11995418670548497 + 0.984529974280594 = 1.104484160986079.\n",
      "Training loss in epoch 810 of generation 1: 0.12498654702281695 + 0.9675938896734438 = 1.0925804366962608.\n",
      "Evaluation loss in epoch 810 of generation 1: 0.11994940529251687 + 0.973857180179543 = 1.09380658547206.\n",
      "Training loss in epoch 811 of generation 1: 0.12498600463661544 + 0.9731616775577326 = 1.098147682194348.\n",
      "Evaluation loss in epoch 811 of generation 1: 0.11995500225131794 + 0.9749077560354271 = 1.094862758286745.\n",
      "Training loss in epoch 812 of generation 1: 0.12498532894899811 + 0.9674952260613395 = 1.0924805550103376.\n",
      "Evaluation loss in epoch 812 of generation 1: 0.11994731628288657 + 1.0123875651778351 = 1.1323348814607217.\n",
      "Training loss in epoch 813 of generation 1: 0.1249859835213774 + 0.9702526264388973 = 1.0952386099602747.\n",
      "Evaluation loss in epoch 813 of generation 1: 0.11995222241558903 + 0.9981637258191689 = 1.118115948234758.\n",
      "Training loss in epoch 814 of generation 1: 0.12498456384293795 + 0.9690278780015101 = 1.0940124418444481.\n",
      "Evaluation loss in epoch 814 of generation 1: 0.11994271010206367 + 0.9734477497212508 = 1.0933904598233144.\n",
      "Training loss in epoch 815 of generation 1: 0.12498527129521772 + 0.973226088214297 = 1.0982113595095147.\n",
      "Evaluation loss in epoch 815 of generation 1: 0.11994792610544637 + 0.9782411034559064 = 1.0981890295613528.\n",
      "Training loss in epoch 816 of generation 1: 0.12498373080089453 + 0.972851413554947 = 1.0978351443558414.\n",
      "Evaluation loss in epoch 816 of generation 1: 0.11994919479352484 + 0.9740802870693118 = 1.0940294818628367.\n",
      "Training loss in epoch 817 of generation 1: 0.12498398803957711 + 0.971399183130099 = 1.096383171169676.\n",
      "Evaluation loss in epoch 817 of generation 1: 0.11995350653291299 + 0.9734993157291265 = 1.0934528222620394.\n",
      "Training loss in epoch 818 of generation 1: 0.12498608358924464 + 0.9722140491766152 = 1.09720013276586.\n",
      "Evaluation loss in epoch 818 of generation 1: 0.11994980204454372 + 0.973370017182478 = 1.0933198192270217.\n",
      "Training loss in epoch 819 of generation 1: 0.12498220150682804 + 0.9705756844398284 = 1.0955578859466564.\n",
      "Evaluation loss in epoch 819 of generation 1: 0.1199562597348253 + 0.994452319270106 = 1.1144085790049314.\n",
      "Training loss in epoch 820 of generation 1: 0.12498263758239629 + 0.9706285614025423 = 1.0956111989849386.\n",
      "Evaluation loss in epoch 820 of generation 1: 0.11995205214284418 + 0.9912168593913638 = 1.111168911534208.\n",
      "Training loss in epoch 821 of generation 1: 0.12498132265389861 + 0.9674682529054333 = 1.0924495755593318.\n",
      "Evaluation loss in epoch 821 of generation 1: 0.11995195111802252 + 0.973470323442127 = 1.0934222745601496.\n",
      "Training loss in epoch 822 of generation 1: 0.12498409563548574 + 0.9756316330784507 = 1.1006157287139364.\n",
      "Evaluation loss in epoch 822 of generation 1: 0.1199401010901241 + 0.988303044911342 = 1.1082431460014661.\n",
      "Training loss in epoch 823 of generation 1: 0.12498139600639946 + 0.9701122435230818 = 1.0950936395294812.\n",
      "Evaluation loss in epoch 823 of generation 1: 0.11995774737124451 + 0.9733201748417412 = 1.0932779222129858.\n",
      "Training loss in epoch 824 of generation 1: 0.12497986395817148 + 0.9689748952789953 = 1.0939547592371668.\n",
      "Evaluation loss in epoch 824 of generation 1: 0.1199453259102185 + 0.9976478159335802 = 1.1175931418437988.\n",
      "Training loss in epoch 825 of generation 1: 0.12498245764384602 + 0.97017568618369 = 1.095158143827536.\n",
      "Evaluation loss in epoch 825 of generation 1: 0.11993591480519629 + 1.00024625663581 = 1.1201821714410063.\n",
      "Training loss in epoch 826 of generation 1: 0.12497944716173362 + 0.9698506128709959 = 1.0948300600327296.\n",
      "Evaluation loss in epoch 826 of generation 1: 0.11994686240591511 + 0.9739054840040868 = 1.0938523464100018.\n",
      "Training loss in epoch 827 of generation 1: 0.12497903256862496 + 0.9723457774153846 = 1.0973248099840096.\n",
      "Evaluation loss in epoch 827 of generation 1: 0.11994668386750303 + 0.9753065623561112 = 1.0952532462236142.\n",
      "Training loss in epoch 828 of generation 1: 0.12497935590718313 + 0.9751862330210865 = 1.1001655889282698.\n",
      "Evaluation loss in epoch 828 of generation 1: 0.11993829697049087 + 0.9958509583318913 = 1.1157892553023823.\n",
      "Training loss in epoch 829 of generation 1: 0.1249796397694267 + 0.9742013470778614 = 1.0991809868472882.\n",
      "Evaluation loss in epoch 829 of generation 1: 0.1199485981960326 + 1.0430326946711137 = 1.1629812928671461.\n",
      "Training loss in epoch 830 of generation 1: 0.12498047134258401 + 0.9706764015547349 = 1.0956568728973188.\n",
      "Evaluation loss in epoch 830 of generation 1: 0.11994339854030286 + 0.9764148053843729 = 1.0963582039246758.\n",
      "Training loss in epoch 831 of generation 1: 0.12497771920081878 + 0.9703887319583181 = 1.0953664511591368.\n",
      "Evaluation loss in epoch 831 of generation 1: 0.1199492985735689 + 0.9779579856178977 = 1.0979072841914665.\n",
      "Training loss in epoch 832 of generation 1: 0.12497726311167709 + 0.9722793837625521 = 1.097256646874229.\n",
      "Evaluation loss in epoch 832 of generation 1: 0.11994441007428985 + 0.9838893461300889 = 1.1038337562043787.\n",
      "Training loss in epoch 833 of generation 1: 0.1249770198274127 + 0.9687618495043673 = 1.09373886933178.\n",
      "Evaluation loss in epoch 833 of generation 1: 0.1199459717343511 + 0.981301600099161 = 1.1012475718335122.\n",
      "Training loss in epoch 834 of generation 1: 0.12497727284304766 + 0.9665408907461772 = 1.0915181635892248.\n",
      "Evaluation loss in epoch 834 of generation 1: 0.1199412720596478 + 0.9841918122419407 = 1.1041330843015886.\n",
      "Training loss in epoch 835 of generation 1: 0.12497874016848076 + 0.9671133053867368 = 1.0920920455552177.\n",
      "Evaluation loss in epoch 835 of generation 1: 0.11994981802483369 + 0.9765194274244029 = 1.0964692454492366.\n",
      "Training loss in epoch 836 of generation 1: 0.12497554690185124 + 0.9692078473983841 = 1.0941833943002353.\n",
      "Evaluation loss in epoch 836 of generation 1: 0.11994699410554624 + 0.975892565099778 = 1.095839559205324.\n",
      "Training loss in epoch 837 of generation 1: 0.12497573895871203 + 0.968616284358304 = 1.093592023317016.\n",
      "Evaluation loss in epoch 837 of generation 1: 0.11994174191690923 + 0.9732529826818151 = 1.0931947245987244.\n",
      "Training loss in epoch 838 of generation 1: 0.12497676387400548 + 0.9726843777004003 = 1.0976611415744058.\n",
      "Evaluation loss in epoch 838 of generation 1: 0.11994327363688699 + 1.0054278660261393 = 1.1253711396630262.\n",
      "Training loss in epoch 839 of generation 1: 0.12497601859790801 + 0.9737995795483859 = 1.0987755981462939.\n",
      "Evaluation loss in epoch 839 of generation 1: 0.11994375451503807 + 0.9792858220763125 = 1.0992295765913507.\n",
      "Training loss in epoch 840 of generation 1: 0.12497543434845194 + 0.9703744631985035 = 1.0953498975469556.\n",
      "Evaluation loss in epoch 840 of generation 1: 0.11994111978768934 + 0.9846571979610872 = 1.1045983177487766.\n",
      "Training loss in epoch 841 of generation 1: 0.12497472983394442 + 0.9683038508107124 = 1.0932785806446568.\n",
      "Evaluation loss in epoch 841 of generation 1: 0.119940828101477 + 0.9732840233848349 = 1.0932248514863119.\n",
      "Training loss in epoch 842 of generation 1: 0.12497421902879459 + 0.9715101075346855 = 1.09648432656348.\n",
      "Evaluation loss in epoch 842 of generation 1: 0.11994206758419795 + 0.9795882029599512 = 1.0995302705441492.\n",
      "Training loss in epoch 843 of generation 1: 0.12497481998683031 + 0.9687718364611285 = 1.0937466564479588.\n",
      "Evaluation loss in epoch 843 of generation 1: 0.11993942128491512 + 1.0036540068169404 = 1.1235934281018556.\n",
      "Training loss in epoch 844 of generation 1: 0.12497528966316866 + 0.9694651830274416 = 1.0944404726906103.\n",
      "Evaluation loss in epoch 844 of generation 1: 0.11993295551264782 + 0.9739012328795916 = 1.0938341883922396.\n",
      "Training loss in epoch 845 of generation 1: 0.12497482824931477 + 0.9697579863808639 = 1.0947328146301787.\n",
      "Evaluation loss in epoch 845 of generation 1: 0.11994208668707332 + 0.9821264299295717 = 1.1020685166166448.\n",
      "Training loss in epoch 846 of generation 1: 0.12497436224519173 + 0.9698774332627453 = 1.094851795507937.\n",
      "Evaluation loss in epoch 846 of generation 1: 0.11993270276691219 + 0.9735280538009385 = 1.0934607565678507.\n",
      "Training loss in epoch 847 of generation 1: 0.12497438409487284 + 0.9725462230848357 = 1.0975206071797086.\n",
      "Evaluation loss in epoch 847 of generation 1: 0.11993672171799905 + 0.9743564911358896 = 1.0942932128538887.\n",
      "Training loss in epoch 848 of generation 1: 0.12497264126148598 + 0.9731813297851939 = 1.0981539710466799.\n",
      "Evaluation loss in epoch 848 of generation 1: 0.11994340368338469 + 0.9874763841437999 = 1.1074197878271848.\n",
      "Training loss in epoch 849 of generation 1: 0.12497413126284863 + 0.9666892555891939 = 1.0916633868520427.\n",
      "Evaluation loss in epoch 849 of generation 1: 0.11993582020922691 + 0.9754360813205525 = 1.0953719015297794.\n",
      "Training loss in epoch 850 of generation 1: 0.12497304428711642 + 0.9674816329891424 = 1.092454677276259.\n",
      "Evaluation loss in epoch 850 of generation 1: 0.11994357597662599 + 1.0203182590760875 = 1.1402618350527136.\n",
      "Training loss in epoch 851 of generation 1: 0.12497432331970944 + 0.9721084671108641 = 1.0970827904305736.\n",
      "Evaluation loss in epoch 851 of generation 1: 0.11994218587508003 + 0.9823444730879162 = 1.1022866589629963.\n",
      "Training loss in epoch 852 of generation 1: 0.12497195859665956 + 0.9732391084209056 = 1.098211067017565.\n",
      "Evaluation loss in epoch 852 of generation 1: 0.11994130787753912 + 1.0175023145043796 = 1.1374436223819187.\n",
      "Training loss in epoch 853 of generation 1: 0.12497160606398966 + 0.9748480528375025 = 1.0998196589014921.\n",
      "Evaluation loss in epoch 853 of generation 1: 0.11993851500042414 + 0.9884770544725133 = 1.1084155694729374.\n",
      "Training loss in epoch 854 of generation 1: 0.12497219270038568 + 0.9707241388848987 = 1.0956963315852843.\n",
      "Evaluation loss in epoch 854 of generation 1: 0.11994176873440734 + 0.97707251444436 = 1.0970142831787673.\n",
      "Training loss in epoch 855 of generation 1: 0.12497501075841576 + 0.9717886303772043 = 1.09676364113562.\n",
      "Evaluation loss in epoch 855 of generation 1: 0.11993602483041114 + 0.9764879282523744 = 1.0964239530827855.\n",
      "Training loss in epoch 856 of generation 1: 0.12497347981185238 + 0.9663912039056484 = 1.0913646837175006.\n",
      "Evaluation loss in epoch 856 of generation 1: 0.11994606688136496 + 0.9738491422773693 = 1.0937952091587342.\n",
      "Training loss in epoch 857 of generation 1: 0.12497527479069666 + 0.9692342197798655 = 1.094209494570562.\n",
      "Evaluation loss in epoch 857 of generation 1: 0.11993134903430204 + 0.9764728840032968 = 1.096404233037599.\n",
      "Training loss in epoch 858 of generation 1: 0.12497267082281924 + 0.9665840113672515 = 1.0915566821900706.\n",
      "Evaluation loss in epoch 858 of generation 1: 0.11994530331739475 + 0.9946377082671884 = 1.1145830115845832.\n",
      "Training loss in epoch 859 of generation 1: 0.12497041828594714 + 0.9762092490447775 = 1.1011796673307246.\n",
      "Evaluation loss in epoch 859 of generation 1: 0.11994706206769899 + 0.974564124696611 = 1.09451118676431.\n",
      "Training loss in epoch 860 of generation 1: 0.12497006465161264 + 0.9722402922961176 = 1.0972103569477303.\n",
      "Evaluation loss in epoch 860 of generation 1: 0.11994018797147071 + 0.9993199538009009 = 1.1192601417723715.\n",
      "Training loss in epoch 861 of generation 1: 0.12497204452649786 + 0.9670469602071463 = 1.0920190047336442.\n",
      "Evaluation loss in epoch 861 of generation 1: 0.11994183173715976 + 0.9777683954576866 = 1.0977102271948462.\n",
      "Training loss in epoch 862 of generation 1: 0.12497137085859895 + 0.9688534947785681 = 1.0938248656371672.\n",
      "Evaluation loss in epoch 862 of generation 1: 0.11993047140412412 + 0.9929678789088833 = 1.1128983503130074.\n",
      "Training loss in epoch 863 of generation 1: 0.12497109177023528 + 0.9795125712327513 = 1.1044836630029866.\n",
      "Evaluation loss in epoch 863 of generation 1: 0.11993228066683918 + 0.9813048123211265 = 1.1012370929879658.\n",
      "Training loss in epoch 864 of generation 1: 0.12497084132515103 + 0.975512419749463 = 1.100483261074614.\n",
      "Evaluation loss in epoch 864 of generation 1: 0.119943877214276 + 0.9876460940885617 = 1.1075899713028377.\n",
      "Training loss in epoch 865 of generation 1: 0.12496996366569157 + 0.967676944901588 = 1.0926469085672794.\n",
      "Evaluation loss in epoch 865 of generation 1: 0.11993105991963061 + 0.9982524454501818 = 1.1181835053698124.\n",
      "Training loss in epoch 866 of generation 1: 0.12496957936835923 + 0.9688829870741782 = 1.0938525664425374.\n",
      "Evaluation loss in epoch 866 of generation 1: 0.11993799867174475 + 0.9745112920211534 = 1.094449290692898.\n",
      "Training loss in epoch 867 of generation 1: 0.12497014415507414 + 0.9705491005387451 = 1.0955192446938193.\n",
      "Evaluation loss in epoch 867 of generation 1: 0.1199405478035173 + 0.979657994580379 = 1.0995985423838963.\n",
      "Training loss in epoch 868 of generation 1: 0.1249696585045992 + 0.974855696920895 = 1.0998253554254942.\n",
      "Evaluation loss in epoch 868 of generation 1: 0.11993743293274349 + 0.9848015760236603 = 1.1047390089564038.\n",
      "Training loss in epoch 869 of generation 1: 0.12496964051074416 + 0.9665955861899134 = 1.0915652267006575.\n",
      "Evaluation loss in epoch 869 of generation 1: 0.11994267612098729 + 0.9894642087821784 = 1.1094068849031657.\n",
      "Training loss in epoch 870 of generation 1: 0.12496777575980897 + 0.9692396296874624 = 1.0942074054472715.\n",
      "Evaluation loss in epoch 870 of generation 1: 0.11993319025759705 + 0.9739250453484628 = 1.0938582356060598.\n",
      "Training loss in epoch 871 of generation 1: 0.1249699550359856 + 0.9691807537938156 = 1.0941507088298013.\n",
      "Evaluation loss in epoch 871 of generation 1: 0.11993234403695457 + 0.9762724507571369 = 1.0962047947940914.\n",
      "Training loss in epoch 872 of generation 1: 0.12496920360892746 + 0.9706228797510122 = 1.0955920833599397.\n",
      "Evaluation loss in epoch 872 of generation 1: 0.11993839597481609 + 0.9738490981938108 = 1.093787494168627.\n",
      "Training loss in epoch 873 of generation 1: 0.12497057398787635 + 0.9702611547917369 = 1.0952317287796132.\n",
      "Evaluation loss in epoch 873 of generation 1: 0.11993995690015137 + 1.0034355051895871 = 1.1233754620897385.\n",
      "Training loss in epoch 874 of generation 1: 0.12496975545108341 + 0.9750786870545867 = 1.1000484425056702.\n",
      "Evaluation loss in epoch 874 of generation 1: 0.1199333456521409 + 0.9786693046934248 = 1.0986026503455657.\n",
      "Training loss in epoch 875 of generation 1: 0.1249683596420435 + 0.9670026027839552 = 1.0919709624259988.\n",
      "Evaluation loss in epoch 875 of generation 1: 0.11994065874713962 + 1.0039357095727202 = 1.1238763683198596.\n",
      "Training loss in epoch 876 of generation 1: 0.1249676859741446 + 0.9684618823933151 = 1.0934295683674597.\n",
      "Evaluation loss in epoch 876 of generation 1: 0.1199373357652332 + 0.984209763066978 = 1.1041470988322113.\n",
      "Training loss in epoch 877 of generation 1: 0.12496748234980556 + 0.9729640506807546 = 1.0979315330305603.\n",
      "Evaluation loss in epoch 877 of generation 1: 0.11993259953791262 + 0.9796812295546333 = 1.099613829092546.\n",
      "Training loss in epoch 878 of generation 1: 0.1249673040637522 + 0.9689968286866081 = 1.0939641327503602.\n",
      "Evaluation loss in epoch 878 of generation 1: 0.11993284218116608 + 0.9745667961602585 = 1.0944996383414247.\n",
      "Training loss in epoch 879 of generation 1: 0.12496737989499838 + 0.9734951264591092 = 1.0984625063541076.\n",
      "Evaluation loss in epoch 879 of generation 1: 0.11992910591589835 + 0.9784358895982175 = 1.098364995514116.\n",
      "Training loss in epoch 880 of generation 1: 0.12496814224689705 + 0.9689048969796129 = 1.09387303922651.\n",
      "Evaluation loss in epoch 880 of generation 1: 0.11993362907268601 + 0.9767800597086526 = 1.0967136887813387.\n",
      "Training loss in epoch 881 of generation 1: 0.12497212531523472 + 0.96850307730468 = 1.0934752026199146.\n",
      "Evaluation loss in epoch 881 of generation 1: 0.11993010698004018 + 0.9873363953342056 = 1.1072665023142458.\n",
      "Training loss in epoch 882 of generation 1: 0.12496743057156968 + 0.9668837772409045 = 1.0918512078124742.\n",
      "Evaluation loss in epoch 882 of generation 1: 0.11995095831954791 + 0.9785346279526345 = 1.0984855862721823.\n",
      "Training loss in epoch 883 of generation 1: 0.12496645100813533 + 0.967065438794594 = 1.0920318898027292.\n",
      "Evaluation loss in epoch 883 of generation 1: 0.1199434788091157 + 1.0076124870170982 = 1.1275559658262138.\n",
      "Training loss in epoch 884 of generation 1: 0.12496586400451777 + 0.979938623122082 = 1.1049044871265998.\n",
      "Evaluation loss in epoch 884 of generation 1: 0.11993444186329658 + 0.9793366812777997 = 1.0992711231410963.\n",
      "Training loss in epoch 885 of generation 1: 0.12496801886046259 + 0.9682779470039048 = 1.0932459658643674.\n",
      "Evaluation loss in epoch 885 of generation 1: 0.11993092509741408 + 0.9759832861243485 = 1.0959142112217626.\n",
      "Training loss in epoch 886 of generation 1: 0.12496673578843273 + 0.9697870394795226 = 1.0947537752679555.\n",
      "Evaluation loss in epoch 886 of generation 1: 0.11993708742785343 + 0.9752375642397371 = 1.0951746516675904.\n",
      "Training loss in epoch 887 of generation 1: 0.12496649268777911 + 0.9712705248633301 = 1.0962370175511091.\n",
      "Evaluation loss in epoch 887 of generation 1: 0.11993857635004311 + 1.0312516781141283 = 1.1511902544641714.\n",
      "Training loss in epoch 888 of generation 1: 0.12496566019656799 + 0.9768558380279717 = 1.1018214982245398.\n",
      "Evaluation loss in epoch 888 of generation 1: 0.11992907083273302 + 0.9866623430296159 = 1.106591413862349.\n",
      "Training loss in epoch 889 of generation 1: 0.12496675947422149 + 0.9725760826019769 = 1.0975428420761983.\n",
      "Evaluation loss in epoch 889 of generation 1: 0.11993469056803933 + 1.0284868719397782 = 1.1484215625078176.\n",
      "Training loss in epoch 890 of generation 1: 0.12496634194334057 + 0.9763026422934666 = 1.1012689842368073.\n",
      "Evaluation loss in epoch 890 of generation 1: 0.11992980445661972 + 0.9891220527731215 = 1.1090518572297412.\n",
      "Training loss in epoch 891 of generation 1: 0.12496694859331003 + 0.9682766485085705 = 1.0932435971018806.\n",
      "Evaluation loss in epoch 891 of generation 1: 0.11993496406978378 + 0.984355982352846 = 1.1042909464226298.\n",
      "Training loss in epoch 892 of generation 1: 0.12496553056736749 + 0.9715414021535703 = 1.0965069327209378.\n",
      "Evaluation loss in epoch 892 of generation 1: 0.11992674928233142 + 0.9739174188928355 = 1.093844168175167.\n",
      "Training loss in epoch 893 of generation 1: 0.12496749208117615 + 0.9675145933248926 = 1.0924820854060688.\n",
      "Evaluation loss in epoch 893 of generation 1: 0.11993340185867803 + 0.9821064233412544 = 1.1020398251999324.\n",
      "Training loss in epoch 894 of generation 1: 0.12496547162831174 + 0.9706375495167389 = 1.0956030211450505.\n",
      "Evaluation loss in epoch 894 of generation 1: 0.1199429352955752 + 0.9775089931047202 = 1.0974519284002953.\n",
      "Training loss in epoch 895 of generation 1: 0.12496514902419661 + 0.9702251318284169 = 1.0951902808526135.\n",
      "Evaluation loss in epoch 895 of generation 1: 0.11993608269008173 + 0.997972376724972 = 1.1179084594150537.\n",
      "Training loss in epoch 896 of generation 1: 0.12496533557273444 + 0.9690797796238373 = 1.0940451151965718.\n",
      "Evaluation loss in epoch 896 of generation 1: 0.11992867022339479 + 0.9735869949881578 = 1.0935156652115527.\n",
      "Training loss in epoch 897 of generation 1: 0.12496592184190894 + 0.9712911024890498 = 1.0962570243309588.\n",
      "Evaluation loss in epoch 897 of generation 1: 0.11993119474184716 + 0.985530803310118 = 1.105461998051965.\n",
      "Training loss in epoch 898 of generation 1: 0.12496788849681903 + 0.9681061695849478 = 1.0930740580817668.\n",
      "Evaluation loss in epoch 898 of generation 1: 0.11993265170345689 + 0.9758086711489732 = 1.09574132285243.\n",
      "Training loss in epoch 899 of generation 1: 0.12496470174837168 + 0.9722896218988406 = 1.0972543236472123.\n",
      "Evaluation loss in epoch 899 of generation 1: 0.11994089422681482 + 0.9732516160915005 = 1.0931925103183153.\n",
      "Training loss in epoch 900 of generation 1: 0.12496556618785601 + 0.9687008980746263 = 1.0936664642624825.\n",
      "Evaluation loss in epoch 900 of generation 1: 0.11993340736912285 + 0.9758254934349119 = 1.0957589008040347.\n",
      "Training loss in epoch 901 of generation 1: 0.12496512148258178 + 0.9757428013181337 = 1.1007079228007155.\n",
      "Evaluation loss in epoch 901 of generation 1: 0.11993361676602592 + 0.9736324789344071 = 1.093566095700433.\n",
      "Training loss in epoch 902 of generation 1: 0.12496483541700902 + 0.9729837102526465 = 1.0979485456696556.\n",
      "Evaluation loss in epoch 902 of generation 1: 0.11993298857531673 + 1.012734914230088 = 1.1326679028054047.\n",
      "Training loss in epoch 903 of generation 1: 0.12496453080674892 + 0.9696351184631983 = 1.0945996492699472.\n",
      "Evaluation loss in epoch 903 of generation 1: 0.11992938107077623 + 0.9887503842910743 = 1.1086797653618505.\n",
      "Training loss in epoch 904 of generation 1: 0.12496513415172461 + 0.9683075891258994 = 1.093272723277624.\n",
      "Evaluation loss in epoch 904 of generation 1: 0.1199346857923205 + 0.9801794149107852 = 1.1001141007031057.\n",
      "Training loss in epoch 905 of generation 1: 0.1249642269309319 + 0.9709888953266577 = 1.0959531222575896.\n",
      "Evaluation loss in epoch 905 of generation 1: 0.11993282656823909 + 0.9735403016829527 = 1.0934731282511918.\n",
      "Training loss in epoch 906 of generation 1: 0.12496742249269599 + 0.9659640562272687 = 1.0909314787199647.\n",
      "Evaluation loss in epoch 906 of generation 1: 0.11993981399594911 + 0.9959682558644535 = 1.1159080698604025.\n",
      "Training loss in epoch 907 of generation 1: 0.12496928173530822 + 0.9676519650241527 = 1.092621246759461.\n",
      "Evaluation loss in epoch 907 of generation 1: 0.11994308756753366 + 0.982160048051314 = 1.1021031356188475.\n",
      "Training loss in epoch 908 of generation 1: 0.12496467255425994 + 0.9682080280243731 = 1.093172700578633.\n",
      "Evaluation loss in epoch 908 of generation 1: 0.11993659681458319 + 0.9958777023574054 = 1.1158142991719886.\n",
      "Training loss in epoch 909 of generation 1: 0.12496557647005889 + 0.9722918604812946 = 1.0972574369513535.\n",
      "Evaluation loss in epoch 909 of generation 1: 0.11993914466058528 + 0.9996001342044589 = 1.1195392788650442.\n",
      "Training loss in epoch 910 of generation 1: 0.12496405378597995 + 0.9811298868313725 = 1.1060939406173524.\n",
      "Evaluation loss in epoch 910 of generation 1: 0.11993416119797387 + 0.9962751523739384 = 1.1162093135719122.\n",
      "Training loss in epoch 911 of generation 1: 0.12496852948200166 + 0.9718280141519775 = 1.096796543633979.\n",
      "Evaluation loss in epoch 911 of generation 1: 0.11992846982688492 + 0.9732461703025689 = 1.0931746401294538.\n",
      "Training loss in epoch 912 of generation 1: 0.12496488315580806 + 0.9747714372061288 = 1.0997363203619368.\n",
      "Evaluation loss in epoch 912 of generation 1: 0.11994434468367801 + 1.0021421582379215 = 1.1220865029215996.\n",
      "Training loss in epoch 913 of generation 1: 0.12496369381707403 + 0.9720489566584114 = 1.0970126504754856.\n",
      "Evaluation loss in epoch 913 of generation 1: 0.11993657073181105 + 0.9842490385787329 = 1.104185609310544.\n",
      "Training loss in epoch 914 of generation 1: 0.12496329161769204 + 0.9683767810067995 = 1.0933400726244915.\n",
      "Evaluation loss in epoch 914 of generation 1: 0.11993101142771623 + 0.9819584892049592 = 1.1018895006326754.\n",
      "Training loss in epoch 915 of generation 1: 0.1249662292063305 + 0.9759827541487924 = 1.100948983355123.\n",
      "Evaluation loss in epoch 915 of generation 1: 0.11993284677320343 + 1.0021948439681843 = 1.1221276907413877.\n",
      "Training loss in epoch 916 of generation 1: 0.12496299233214415 + 0.9695776394802559 = 1.0945406318124.\n",
      "Evaluation loss in epoch 916 of generation 1: 0.11992915055050138 + 0.9893045278469844 = 1.1092336783974857.\n",
      "Training loss in epoch 917 of generation 1: 0.12496378993730982 + 0.9701490288383005 = 1.0951128187756103.\n",
      "Evaluation loss in epoch 917 of generation 1: 0.11993484467681273 + 0.9732950398661139 = 1.0932298845429267.\n",
      "Training loss in epoch 918 of generation 1: 0.12496787729656232 + 0.9725152090232005 = 1.0974830863197629.\n",
      "Evaluation loss in epoch 918 of generation 1: 0.11993661077437673 + 0.9813330551876309 = 1.1012696659620076.\n",
      "Training loss in epoch 919 of generation 1: 0.1249644604838257 + 0.9703848526300629 = 1.0953493131138887.\n",
      "Evaluation loss in epoch 919 of generation 1: 0.11993312890797808 + 0.9733569537546308 = 1.0932900826626089.\n",
      "Training loss in epoch 920 of generation 1: 0.12496284048604102 + 0.9724691271276992 = 1.0974319676137403.\n",
      "Evaluation loss in epoch 920 of generation 1: 0.11993541739571076 + 0.9756482363849281 = 1.095583653780639.\n",
      "Training loss in epoch 921 of generation 1: 0.12496451134400777 + 0.9707555069480899 = 1.0957200182920976.\n",
      "Evaluation loss in epoch 921 of generation 1: 0.11993114037212495 + 0.9906031457227992 = 1.110534286094924.\n",
      "Training loss in epoch 922 of generation 1: 0.12496240569574814 + 0.9719034532055655 = 1.0968658589013136.\n",
      "Evaluation loss in epoch 922 of generation 1: 0.11993352235373804 + 0.9735006059079399 = 1.0934341282616777.\n",
      "Training loss in epoch 923 of generation 1: 0.12496281110831853 + 0.9718171737723779 = 1.0967799848806963.\n",
      "Evaluation loss in epoch 923 of generation 1: 0.11993327658789918 + 0.9997053352085578 = 1.119638611796457.\n",
      "Training loss in epoch 924 of generation 1: 0.12496389294294931 + 0.9673744557130598 = 1.0923383486560092.\n",
      "Evaluation loss in epoch 924 of generation 1: 0.1199328113226751 + 0.986642930099887 = 1.106575741422562.\n",
      "Training loss in epoch 925 of generation 1: 0.12496307183560565 + 0.969051757683238 = 1.0940148295188437.\n",
      "Evaluation loss in epoch 925 of generation 1: 0.11993092601582156 + 0.9748505443931544 = 1.094781470408976.\n",
      "Training loss in epoch 926 of generation 1: 0.12496297672522907 + 0.9761631847759096 = 1.1011261615011387.\n",
      "Evaluation loss in epoch 926 of generation 1: 0.11993184589274308 + 0.9742394463490631 = 1.0941712922418063.\n",
      "Training loss in epoch 927 of generation 1: 0.12496407894065484 + 0.9698000376528394 = 1.0947641165934943.\n",
      "Evaluation loss in epoch 927 of generation 1: 0.11993285154892226 + 0.9750846839281737 = 1.0950175354770961.\n",
      "Training loss in epoch 928 of generation 1: 0.12496303235929106 + 0.9693715341925667 = 1.0943345665518578.\n",
      "Evaluation loss in epoch 928 of generation 1: 0.11992894813349508 + 0.9759144834450838 = 1.0958434315785788.\n",
      "Training loss in epoch 929 of generation 1: 0.12496279825556494 + 0.9679520467083226 = 1.0929148449638875.\n",
      "Evaluation loss in epoch 929 of generation 1: 0.11993239418200241 + 0.9781346842761399 = 1.0980670784581423.\n",
      "Training loss in epoch 930 of generation 1: 0.12496282175774294 + 0.9693522579999534 = 1.0943150797576964.\n",
      "Evaluation loss in epoch 930 of generation 1: 0.11992460829083836 + 1.0008847540809855 = 1.1208093623718238.\n",
      "Training loss in epoch 931 of generation 1: 0.12496261464479937 + 0.9692327082960432 = 1.0941953229408425.\n",
      "Evaluation loss in epoch 931 of generation 1: 0.11993169398814761 + 0.9732770875716246 = 1.0932087815597722.\n",
      "Training loss in epoch 932 of generation 1: 0.12496260619870414 + 0.9698290319960532 = 1.0947916381947573.\n",
      "Evaluation loss in epoch 932 of generation 1: 0.11993145446747955 + 0.9866833106395092 = 1.1066147651069889.\n",
      "Training loss in epoch 933 of generation 1: 0.12496190792696266 + 0.9738206389687545 = 1.098782546895717.\n",
      "Evaluation loss in epoch 933 of generation 1: 0.11992235341681905 + 0.9762180604626107 = 1.0961404138794297.\n",
      "Training loss in epoch 934 of generation 1: 0.12496448123184221 + 0.9678831823890284 = 1.0928476636208706.\n",
      "Evaluation loss in epoch 934 of generation 1: 0.1199297954562265 + 1.0091210939851123 = 1.129050889441339.\n",
      "Training loss in epoch 935 of generation 1: 0.12496401981798833 + 0.9678369859204092 = 1.0928010057383974.\n",
      "Evaluation loss in epoch 935 of generation 1: 0.11993880797240694 + 0.9843128627547865 = 1.1042516707271934.\n",
      "Training loss in epoch 936 of generation 1: 0.12496220023530145 + 0.9679320463548498 = 1.0928942465901512.\n",
      "Evaluation loss in epoch 936 of generation 1: 0.11993592417295247 + 0.9964136438119944 = 1.1163495679849469.\n",
      "Training loss in epoch 937 of generation 1: 0.12496219950085839 + 0.9671380473046198 = 1.0921002468054781.\n",
      "Evaluation loss in epoch 937 of generation 1: 0.11992657460123071 + 0.9769627552003082 = 1.096889329801539.\n",
      "Training loss in epoch 938 of generation 1: 0.12496222759330552 + 0.9734277295570535 = 1.098389957150359.\n",
      "Evaluation loss in epoch 938 of generation 1: 0.11992755086837087 + 1.0127553631234427 = 1.1326829139918135.\n",
      "Training loss in epoch 939 of generation 1: 0.12496162268763833 + 0.9722190037295136 = 1.097180626417152.\n",
      "Evaluation loss in epoch 939 of generation 1: 0.11992817575281317 + 1.0098689627390245 = 1.1297971384918377.\n",
      "Training loss in epoch 940 of generation 1: 0.12496169732541454 + 0.9708936380614764 = 1.095855335386891.\n",
      "Evaluation loss in epoch 940 of generation 1: 0.11992756758338681 + 0.979824167554294 = 1.0997517351376809.\n",
      "Training loss in epoch 941 of generation 1: 0.1249659897778922 + 0.9667965195295578 = 1.0917625093074501.\n",
      "Evaluation loss in epoch 941 of generation 1: 0.11993094511869692 + 1.0041938335024887 = 1.1241247786211856.\n",
      "Training loss in epoch 942 of generation 1: 0.12496353067890882 + 0.9717241874051452 = 1.096687718084054.\n",
      "Evaluation loss in epoch 942 of generation 1: 0.11993189879301333 + 0.986829748873718 = 1.1067616476667312.\n",
      "Training loss in epoch 943 of generation 1: 0.12496421830122591 + 0.9682721375592819 = 1.0932363558605078.\n",
      "Evaluation loss in epoch 943 of generation 1: 0.11992790831255802 + 1.0056473624724636 = 1.1255752707850217.\n",
      "Training loss in epoch 944 of generation 1: 0.12496137710823937 + 0.9669129698837451 = 1.0918743469919845.\n",
      "Evaluation loss in epoch 944 of generation 1: 0.11993064535049885 + 0.9809967093915896 = 1.1009273547420884.\n",
      "Training loss in epoch 945 of generation 1: 0.12496231499202994 + 0.9697877592337237 = 1.0947500742257537.\n",
      "Evaluation loss in epoch 945 of generation 1: 0.11993305029229868 + 0.9737711951986115 = 1.0937042454909103.\n",
      "Training loss in epoch 946 of generation 1: 0.12496194262939735 + 0.9680653007663026 = 1.0930272433957.\n",
      "Evaluation loss in epoch 946 of generation 1: 0.11992610419292479 + 0.9737490961107157 = 1.0936752003036405.\n",
      "Training loss in epoch 947 of generation 1: 0.12496279274724197 + 0.9713661713833345 = 1.0963289641305765.\n",
      "Evaluation loss in epoch 947 of generation 1: 0.11993030535605362 + 0.9732599860898151 = 1.0931902914458689.\n",
      "Training loss in epoch 948 of generation 1: 0.12496167474129037 + 0.9743237398108657 = 1.0992854145521562.\n",
      "Evaluation loss in epoch 948 of generation 1: 0.11992904915831673 + 0.973276229411685 = 1.0932052785700017.\n",
      "Training loss in epoch 949 of generation 1: 0.12496211448907393 + 0.9711240857300227 = 1.0960862002190968.\n",
      "Evaluation loss in epoch 949 of generation 1: 0.11991854220950181 + 0.9734591174015462 = 1.093377659611048.\n",
      "Training loss in epoch 950 of generation 1: 0.12496116154920059 + 0.9731827854513434 = 1.0981439470005439.\n",
      "Evaluation loss in epoch 950 of generation 1: 0.11992803523647032 + 0.9739078674551519 = 1.0938359026916222.\n",
      "Training loss in epoch 951 of generation 1: 0.1249625285313503 + 0.970194251435419 = 1.0951567799667694.\n",
      "Evaluation loss in epoch 951 of generation 1: 0.11992717028031548 + 1.001802653120185 = 1.1217298234005004.\n",
      "Training loss in epoch 952 of generation 1: 0.12496148911080637 + 0.9743716166852114 = 1.0993331057960178.\n",
      "Evaluation loss in epoch 952 of generation 1: 0.11993296047204816 + 0.9878221447184935 = 1.1077551051905417.\n",
      "Training loss in epoch 953 of generation 1: 0.12496184641735619 + 0.9747603471158879 = 1.0997221935332442.\n",
      "Evaluation loss in epoch 953 of generation 1: 0.11993030535605362 + 0.9917996234629297 = 1.1117299288189835.\n",
      "Training loss in epoch 954 of generation 1: 0.12496188736255691 + 0.9730538995072251 = 1.098015786869782.\n",
      "Evaluation loss in epoch 954 of generation 1: 0.11992289839981149 + 1.0071596327841925 = 1.127082531184004.\n",
      "Training loss in epoch 955 of generation 1: 0.12496301546710062 + 0.9677149802388987 = 1.0926779957059993.\n",
      "Evaluation loss in epoch 955 of generation 1: 0.11992487977208635 + 0.9931584080488913 = 1.1130832878209778.\n",
      "Training loss in epoch 956 of generation 1: 0.12496192022888396 + 0.9677335322706526 = 1.0926954524995365.\n",
      "Evaluation loss in epoch 956 of generation 1: 0.11992843823366797 + 0.9735713306303568 = 1.0934997688640247.\n",
      "Training loss in epoch 957 of generation 1: 0.12496085712255126 + 0.9688355743678478 = 1.0937964314903992.\n",
      "Evaluation loss in epoch 957 of generation 1: 0.1199290138914699 + 0.9743101475602489 = 1.0942391614517188.\n",
      "Training loss in epoch 958 of generation 1: 0.12496193638663132 + 0.9753773101348349 = 1.100339246521466.\n",
      "Evaluation loss in epoch 958 of generation 1: 0.11992742669968098 + 0.9737424982714543 = 1.0936699249711352.\n",
      "Training loss in epoch 959 of generation 1: 0.12496126308595396 + 0.969418651652786 = 1.0943799147387399.\n",
      "Evaluation loss in epoch 959 of generation 1: 0.119925705604083 + 0.9858449897707335 = 1.1057706953748165.\n",
      "Training loss in epoch 960 of generation 1: 0.12496100658171444 + 0.9696848725740111 = 1.0946458791557256.\n",
      "Evaluation loss in epoch 960 of generation 1: 0.11992650939430037 + 0.9733830159544393 = 1.0933095253487397.\n",
      "Training loss in epoch 961 of generation 1: 0.12496305163842143 + 0.9693126582989199 = 1.0942757099373412.\n",
      "Evaluation loss in epoch 961 of generation 1: 0.11993114441311782 + 0.9972182187167081 = 1.117149363129826.\n",
      "Training loss in epoch 962 of generation 1: 0.12496266219998765 + 0.9777526356047468 = 1.1027152978047343.\n",
      "Evaluation loss in epoch 962 of generation 1: 0.11993010202063985 + 1.022626480079028 = 1.1425565820996677.\n",
      "Training loss in epoch 963 of generation 1: 0.12496182952516575 + 0.9720023371505884 = 1.0969641666757541.\n",
      "Evaluation loss in epoch 963 of generation 1: 0.11992360392042963 + 0.9854445273777002 = 1.1053681312981298.\n",
      "Training loss in epoch 964 of generation 1: 0.12496246426758235 + 0.9665805976758978 = 1.0915430619434803.\n",
      "Evaluation loss in epoch 964 of generation 1: 0.11992798215251858 + 1.0214976088842735 = 1.141425591036792.\n",
      "Training loss in epoch 965 of generation 1: 0.12496132496278195 + 0.9675404192807355 = 1.0925017442435174.\n",
      "Evaluation loss in epoch 965 of generation 1: 0.11993347275973469 + 0.9747553709291714 = 1.094688843688906.\n",
      "Training loss in epoch 966 of generation 1: 0.12496061218578998 + 0.9689518293601804 = 1.0939124415459704.\n",
      "Evaluation loss in epoch 966 of generation 1: 0.119924377586882 + 1.0048064832878407 = 1.1247308608747226.\n",
      "Training loss in epoch 967 of generation 1: 0.12496325948580807 + 0.9706403036782225 = 1.0956035631640306.\n",
      "Evaluation loss in epoch 967 of generation 1: 0.11993194397866083 + 1.006736746554382 = 1.126668690533043.\n",
      "Training loss in epoch 968 of generation 1: 0.12496308266864081 + 0.9729579268945011 = 1.097921009563142.\n",
      "Evaluation loss in epoch 968 of generation 1: 0.1199165110595421 + 0.982266869860915 = 1.102183380920457.\n",
      "Training loss in epoch 969 of generation 1: 0.12496202323452343 + 0.9696494312895965 = 1.09461145452412.\n",
      "Evaluation loss in epoch 969 of generation 1: 0.11993066904541157 + 0.9817199795198367 = 1.1016506485652482.\n",
      "Training loss in epoch 970 of generation 1: 0.12496078679962805 + 0.9698614091840119 = 1.09482219598364.\n",
      "Evaluation loss in epoch 970 of generation 1: 0.11993524491878797 + 0.9983794619379499 = 1.1183147068567378.\n",
      "Training loss in epoch 971 of generation 1: 0.12496154896791596 + 0.9659020236973401 = 1.090863572665256.\n",
      "Evaluation loss in epoch 971 of generation 1: 0.11992956107864028 + 0.9942688023548096 = 1.11419836343345.\n",
      "Training loss in epoch 972 of generation 1: 0.12496210163632034 + 0.9711216047813583 = 1.0960837064176787.\n",
      "Evaluation loss in epoch 972 of generation 1: 0.11992843878471245 + 0.9878311862563499 = 1.1077596250410624.\n",
      "Training loss in epoch 973 of generation 1: 0.1249610126408697 + 0.9728498961955803 = 1.09781090883645.\n",
      "Evaluation loss in epoch 973 of generation 1: 0.11993160012690422 + 0.9777484696892267 = 1.0976800698161309.\n",
      "Training loss in epoch 974 of generation 1: 0.12496153501349777 + 0.9771590410951564 = 1.102120576108654.\n",
      "Evaluation loss in epoch 974 of generation 1: 0.11993173843906914 + 0.9732709364457564 = 1.0932026748848256.\n",
      "Training loss in epoch 975 of generation 1: 0.12496074677248115 + 0.9673009879046511 = 1.0922617346771322.\n",
      "Evaluation loss in epoch 975 of generation 1: 0.11992902987175987 + 0.9785575161362282 = 1.098486546007988.\n",
      "Training loss in epoch 976 of generation 1: 0.12496332503485139 + 0.9674466705616043 = 1.0924099955964557.\n",
      "Evaluation loss in epoch 976 of generation 1: 0.1199307674986923 + 0.9869288296104396 = 1.106859597109132.\n",
      "Training loss in epoch 977 of generation 1: 0.12496162222861142 + 0.9726001693966483 = 1.0975617916252598.\n",
      "Evaluation loss in epoch 977 of generation 1: 0.11992125224626303 + 0.974085838658784 = 1.094007090905047.\n",
      "Training loss in epoch 978 of generation 1: 0.1249602611220062 + 0.9683481259762805 = 1.0933083870982867.\n",
      "Evaluation loss in epoch 978 of generation 1: 0.11993222115403515 + 0.9807289164679811 = 1.1006611376220163.\n",
      "Training loss in epoch 979 of generation 1: 0.12496034448129377 + 0.9738871516013632 = 1.098847496082657.\n",
      "Evaluation loss in epoch 979 of generation 1: 0.11992860703696087 + 0.999517302667415 = 1.1194459097043759.\n",
      "Training loss in epoch 980 of generation 1: 0.12496547869732622 + 0.9721249201043459 = 1.097090398801672.\n",
      "Evaluation loss in epoch 980 of generation 1: 0.11992429695070617 + 0.9760557183499329 = 1.0959800153006392.\n",
      "Training loss in epoch 981 of generation 1: 0.12496137508852095 + 0.9670757107152634 = 1.0920370858037844.\n",
      "Evaluation loss in epoch 981 of generation 1: 0.11993288020323531 + 0.9734046271842874 = 1.0933375073875227.\n",
      "Training loss in epoch 982 of generation 1: 0.12496048218936795 + 0.968038618449849 = 1.092999100639217.\n",
      "Evaluation loss in epoch 982 of generation 1: 0.11993608764948206 + 0.9732631718616456 = 1.0931992595111277.\n",
      "Training loss in epoch 983 of generation 1: 0.12496060667746701 + 0.9666813955795411 = 1.0916420022570081.\n",
      "Evaluation loss in epoch 983 of generation 1: 0.11993418911756096 + 0.9743791882907298 = 1.0943133774082907.\n",
      "Training loss in epoch 984 of generation 1: 0.1249632257014272 + 0.9666431472537423 = 1.0916063729551695.\n",
      "Evaluation loss in epoch 984 of generation 1: 0.11992580644522316 + 0.9732817780622532 = 1.0932075845074765.\n",
      "Training loss in epoch 985 of generation 1: 0.12496019079908298 + 0.971733921713493 = 1.096694112512576.\n",
      "Evaluation loss in epoch 985 of generation 1: 0.1199308795444036 + 0.979003059845676 = 1.0989339393900797.\n",
      "Training loss in epoch 986 of generation 1: 0.12496120598300586 + 0.9754690655754951 = 1.1004302715585008.\n",
      "Evaluation loss in epoch 986 of generation 1: 0.11993276283076071 + 0.9732435311668642 = 1.093176293997625.\n",
      "Training loss in epoch 987 of generation 1: 0.12496126620733697 + 0.9700492180261329 = 1.0950104842334698.\n",
      "Evaluation loss in epoch 987 of generation 1: 0.11993073406866041 + 1.0075555663263118 = 1.1274863003949722.\n",
      "Training loss in epoch 988 of generation 1: 0.12496128218147357 + 0.9696445237410541 = 1.0946058059225277.\n",
      "Evaluation loss in epoch 988 of generation 1: 0.11992888035502382 + 0.9733678938244084 = 1.0932967741794322.\n",
      "Training loss in epoch 989 of generation 1: 0.12496112849926279 + 0.9688233385464298 = 1.0937844670456927.\n",
      "Evaluation loss in epoch 989 of generation 1: 0.11992167820364742 + 1.0236073275009179 = 1.1435290057045653.\n",
      "Training loss in epoch 990 of generation 1: 0.12496014673249925 + 0.9799908787459651 = 1.1049510254784645.\n",
      "Evaluation loss in epoch 990 of generation 1: 0.1199281142195127 + 0.9812695866189495 = 1.1011977008384621.\n",
      "Training loss in epoch 991 of generation 1: 0.12496265191778477 + 0.9699079669986177 = 1.0948706189164024.\n",
      "Evaluation loss in epoch 991 of generation 1: 0.11992893454106453 + 0.9781149348419143 = 1.0980438693829788.\n",
      "Training loss in epoch 992 of generation 1: 0.12496049577656461 + 0.9693860791029726 = 1.0943465748795371.\n",
      "Evaluation loss in epoch 992 of generation 1: 0.11993034282707837 + 0.9823613982354915 = 1.1022917410625699.\n",
      "Training loss in epoch 993 of generation 1: 0.1249607443855412 + 0.9762137218030269 = 1.101174466188568.\n",
      "Evaluation loss in epoch 993 of generation 1: 0.11993044587239646 + 0.9790166463984179 = 1.0989470922708144.\n",
      "Training loss in epoch 994 of generation 1: 0.12496041003033709 + 0.9729379206654839 = 1.097898330695821.\n",
      "Evaluation loss in epoch 994 of generation 1: 0.1199256842970297 + 0.9785799105839663 = 1.098505594880996.\n",
      "Training loss in epoch 995 of generation 1: 0.12496116871002044 + 0.9735118879186772 = 1.0984730566286975.\n",
      "Evaluation loss in epoch 995 of generation 1: 0.11992758191054334 + 1.027745974265923 = 1.1476735561764664.\n",
      "Training loss in epoch 996 of generation 1: 0.1249612654728939 + 0.9770408089821 = 1.1020020744549939.\n",
      "Evaluation loss in epoch 996 of generation 1: 0.11992385078835745 + 0.9735568579980882 = 1.0934807087864458.\n",
      "Training loss in epoch 997 of generation 1: 0.12496176434334398 + 0.9754685162120844 = 1.1004302805554285.\n",
      "Evaluation loss in epoch 997 of generation 1: 0.11992667838127477 + 0.9985726184580469 = 1.1184992968393217.\n",
      "Training loss in epoch 998 of generation 1: 0.12495985203722049 + 0.9667462189931065 = 1.091706071030327.\n",
      "Evaluation loss in epoch 998 of generation 1: 0.11992682275492898 + 0.9794549633431692 = 1.0993817860980981.\n",
      "Training loss in epoch 999 of generation 1: 0.12496051009820432 + 0.9708637330088654 = 1.0958242431070697.\n",
      "Evaluation loss in epoch 999 of generation 1: 0.11992541557100411 + 0.9920889379798172 = 1.1120143535508213.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "try:\n",
    "    for epoch in range(1000):\n",
    "        total_kl_loss = 0\n",
    "        total_mse_loss = 0\n",
    "        for states, wall_priors, step_priors, values in training_loader:\n",
    "            states = states.to(device)\n",
    "            wall_priors = wall_priors.to(device)\n",
    "            step_priors = step_priors.to(device)\n",
    "            values = values.to(device)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            wp, sp, vs = model.forward(states)\n",
    "            kl_loss, mse_loss = loss_fn(wp, sp, vs, wall_priors, step_priors, values)\n",
    "            total_kl_loss += float(kl_loss)\n",
    "            total_mse_loss += float(mse_loss)\n",
    "\n",
    "            loss = kl_loss + mse_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            del loss\n",
    "        print(\n",
    "            f\"Training loss in epoch {epoch} of generation {generation}: {total_kl_loss / len(training_data)} + {total_mse_loss / len(training_data)} = {(total_kl_loss + total_mse_loss) / len(training_data)}.\"\n",
    "        )\n",
    "        \n",
    "        model.train(False)\n",
    "        total_kl_loss = 0\n",
    "        total_mse_loss = 0\n",
    "        for states, wall_priors, step_priors, values in eval_loader:\n",
    "            states = states.to(device)\n",
    "            wall_priors = wall_priors.to(device)\n",
    "            step_priors = step_priors.to(device)\n",
    "            values = values.to(device)\n",
    "            wp, sp, vs = model.forward(states)\n",
    "            kl_loss, mse_loss = loss_fn(wp, sp, vs, wall_priors, step_priors, values)\n",
    "            total_kl_loss += float(kl_loss)\n",
    "            total_mse_loss += float(mse_loss)\n",
    "        print(\n",
    "            f\"Evaluation loss in epoch {epoch} of generation {generation}: {total_kl_loss / len(eval_data)} + {total_mse_loss / len(eval_data)} = {(total_kl_loss + total_mse_loss) / len(eval_data)}.\"\n",
    "        )\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Trainig was interrupted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ced9ae19-cfea-4b36-9f0b-52d55b3379ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = torch.unsqueeze(snapshots[0][0], 0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0da072fb-1e1b-4c9e-bb6b-9f607d1859ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.2620e-05, 1.1728e-05, 9.9632e-06, 1.1068e-05, 9.7491e-06, 1.0028e-05,\n",
       "          9.6527e-06, 1.0931e-05, 1.1280e-05, 9.7572e-06, 1.2101e-05, 1.2371e-05,\n",
       "          1.1854e-05, 1.2051e-05, 9.7584e-06, 1.1797e-05, 1.1570e-05, 1.2146e-05,\n",
       "          1.1763e-05, 9.7487e-06, 1.2140e-05, 1.0696e-05, 1.0152e-05, 1.0767e-05,\n",
       "          9.7246e-06, 1.3745e-05, 1.6174e-05, 1.6068e-05, 1.2206e-05, 1.0691e-05,\n",
       "          1.0049e-05, 1.0471e-05, 1.1226e-05, 1.1121e-05, 1.0335e-05, 1.1261e-05,\n",
       "          1.9361e-05, 1.3077e-05, 1.1814e-05, 1.1210e-05, 1.2814e-05, 1.3772e-05,\n",
       "          1.4247e-05, 1.2809e-05, 1.1578e-05, 9.7463e-06, 9.7496e-06, 9.7592e-06,\n",
       "          9.7417e-06, 9.7537e-06]], device='cuda:0', grad_fn=<SigmoidBackward0>),\n",
       " tensor([[1.0000, 1.0000, 1.0000, 0.9378]], device='cuda:0',\n",
       "        grad_fn=<SigmoidBackward0>),\n",
       " tensor([[0.1639]], device='cuda:0', grad_fn=<TanhBackward0>))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7d7d78a-a365-49db-acd5-9457152eb041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[0.0000, 0.0400, 0.0800, 0.1200, 0.1600],\n",
       "          [0.0400, 0.0800, 0.1200, 0.1600, 0.2000],\n",
       "          [0.0800, 0.1200, 0.1600, 0.2000, 0.2400],\n",
       "          [0.1200, 0.1600, 0.2000, 0.2400, 0.2800],\n",
       "          [0.1600, 0.2000, 0.2400, 0.2800, 0.3200]],\n",
       " \n",
       "         [[0.3200, 0.2800, 0.2400, 0.2000, 0.1600],\n",
       "          [0.2800, 0.2400, 0.2000, 0.1600, 0.1200],\n",
       "          [0.2400, 0.2000, 0.1600, 0.1200, 0.0800],\n",
       "          [0.2000, 0.1600, 0.1200, 0.0800, 0.0400],\n",
       "          [0.1600, 0.1200, 0.0800, 0.0400, 0.0000]],\n",
       " \n",
       "         [[0.1600, 0.1200, 0.0800, 0.0400, 0.0000],\n",
       "          [0.2000, 0.1600, 0.1200, 0.0800, 0.0400],\n",
       "          [0.2400, 0.2000, 0.1600, 0.1200, 0.0800],\n",
       "          [0.2800, 0.2400, 0.2000, 0.1600, 0.1200],\n",
       "          [0.3200, 0.2800, 0.2400, 0.2000, 0.1600]],\n",
       " \n",
       "         [[0.1600, 0.2000, 0.2400, 0.2800, 0.3200],\n",
       "          [0.1200, 0.1600, 0.2000, 0.2400, 0.2800],\n",
       "          [0.0800, 0.1200, 0.1600, 0.2000, 0.2400],\n",
       "          [0.0400, 0.0800, 0.1200, 0.1600, 0.2000],\n",
       "          [0.0000, 0.0400, 0.0800, 0.1200, 0.1600]],\n",
       " \n",
       "         [[0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 1.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 1.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       " \n",
       "         [[0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]]]),\n",
       " tensor([0.0064, 0.0076, 0.0077, 0.0075, 0.0000, 0.0078, 0.0077, 0.0077, 0.0077,\n",
       "         0.0000, 0.0076, 0.0076, 0.0076, 0.0076, 0.0000, 0.0076, 0.0077, 0.0076,\n",
       "         0.0076, 0.0000, 0.0076, 0.0076, 0.0077, 0.0077, 0.0000, 0.0065, 0.0076,\n",
       "         0.0076, 0.0078, 0.0074, 0.0076, 0.0076, 0.0076, 0.0076, 0.0078, 0.0077,\n",
       "         0.0076, 0.0077, 0.0076, 0.0076, 0.0078, 0.0076, 0.0076, 0.0078, 0.0077,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000]),\n",
       " tensor([0.3465, 0.3501, 0.0000, 0.0000]),\n",
       " tensor([0.9748])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snapshots[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc0d688b-ef5b-49dd-8c45-d957dd7b5580",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, models_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b8235c-6e6a-4dcb-831e-607a94055a08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
